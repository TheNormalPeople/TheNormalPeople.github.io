<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true,"scrollpercent":true,"b2t":true},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="1.1 统计学习 统计学习的特点  统计学习（statistical learning）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习（statistical machine learning）。 统计学习的主要特点是：  统计学习以计算机及网络为平台，是建立在计算机及网络上的。 统计学习以数据为对象，是数据驱动的学科。 统计学习的目的是对">
<meta property="og:type" content="article">
<meta property="og:title" content="第一章 统计学习及监督学习概论">
<meta property="og:url" content="http://example.com/2021/06/04/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="1.1 统计学习 统计学习的特点  统计学习（statistical learning）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习（statistical machine learning）。 统计学习的主要特点是：  统计学习以计算机及网络为平台，是建立在计算机及网络上的。 统计学习以数据为对象，是数据驱动的学科。 统计学习的目的是对">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-06-04T02:30:55.624Z">
<meta property="article:modified_time" content="2021-06-04T02:39:07.712Z">
<meta property="article:author" content="Normal People">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2021/06/04/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>第一章 统计学习及监督学习概论 | Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">by Normal People</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">6</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">14</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/04/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy libing or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          第一章 统计学习及监督学习概论
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-04 10:30:55 / 修改时间：10:39:07" itemprop="dateCreated datePublished" datetime="2021-06-04T10:30:55+08:00">2021-06-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/04/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/04/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="1-1-统计学习"><a href="#1-1-统计学习" class="headerlink" title="1.1 统计学习"></a>1.1 统计学习</h1><ol>
<li><strong>统计学习的特点</strong></li>
</ol>
<p>统计学习（statistical learning）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习（statistical machine learning）。</p>
<p>统计学习的主要特点是：</p>
<ul>
<li>统计学习以计算机及网络为平台，是建立在计算机及网络上的。</li>
<li>统计学习以数据为对象，是数据驱动的学科。</li>
<li>统计学习的目的是对数据进行预测与分析。</li>
<li>统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析</li>
<li>统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论。</li>
</ul>
<p>赫尔伯特·西蒙（Herbert A. Simon）对“学习”给出以下定义：“如果一个系统能够通过执行某个过程改进它的性能，这就是学习。”</p>
<ol start="2">
<li><strong>统计学习的对象</strong></li>
</ol>
<p>统计学习研究的对象是数据（data）。它从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。</p>
<p>统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提。</p>
<p>在统计学习中，以变量或变量组表示数据。数据分为由连续变量和离散变量表示的类型。</p>
<ol start="3">
<li><strong>统计学习的目的</strong></li>
</ol>
<p>统计学习总的目标就是考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测和分析，同时也要考虑尽可能地提高学习效率。</p>
<ol start="4">
<li><strong>统计学习的方法</strong></li>
</ol>
<p>统计学习由监督学习（supervised learning）、无监督学习（unsupervised learning）和强化学习（reinforcement learning）等组成。</p>
<p>统计学习方法可以概括如下：从给定的、有限的、用于学习的训练数据（training data）集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space）；应用某个评价标准（evaluation criterion），从假设空间中选取一个最优模型，使它对已知的训练数据及未知的测试数据（test data）在给定的评价标准下有最优的预测；最优模型的选取由算法实现。</p>
<p>统计学习方法包括模型的假设空间、模型选择的评价标准以及模型学习的算法。称其为统计学习方法的三要素，简称为模型（model）、策略（strategy）和算法（algorithm）。</p>
<p>实现统计学习方法的步骤如下：</p>
<ul>
<li>得到一个有限的训练数据集合；</li>
<li>确定包含所有可能的模型的假设空间，即学习模型的集合；</li>
<li>确定模型选择的标准，即学习的策略；</li>
<li>实现求解最优模型的算法，即学习的算法；</li>
<li>通过学习方法选择最优模型；</li>
<li>利用学习的最优模型对新数据进行预测或分析。</li>
</ul>
<ol start="5">
<li><strong>统计学习的研究</strong></li>
</ol>
<p>统计学习研究一般包括统计学习方法、统计学习理论及统计学习应用三个方面。</p>
<ol start="6">
<li><strong>统计学习的重要性</strong></li>
</ol>
<p>统计学习学科在科学技术中的重要性主要体现在以下几个方面：</p>
<ul>
<li>统计学习是处理海量数据的有效方法。</li>
<li>统计学习是计算机智能化的有效手段。</li>
<li>统计学习是计算机科学发展的一个重要组成部分。</li>
</ul>
<h1 id="1-2-统计学习的分类"><a href="#1-2-统计学习的分类" class="headerlink" title="1.2 统计学习的分类"></a>1.2 统计学习的分类</h1><p>统计学习或机器学习是一个范围宽阔、内容繁多、应用广泛的领域，并不存在（至少现在不存在）一个统一的理论体系涵盖所有内容。从几个角度对统计学习方法进行分类。</p>
<h2 id="1-2-1-基本分类"><a href="#1-2-1-基本分类" class="headerlink" title="1.2.1 基本分类"></a>1.2.1 基本分类</h2><ol>
<li><strong>监督学习</strong></li>
</ol>
<p>监督学习（supervised learning）是指从标注数据中学习预测模型的机器学习问题。标注数据表示输入输出的对应关系，预测模型对给定的输入产生相应的输出。监督学习的本质是学习输入到输出的映射的统计规律。</p>
<ol start="2">
<li><strong>无监督学习</strong></li>
</ol>
<p>无监督学习（unsupervised learning）是指从无标注数据中学习预测模型的机器学习问题。无标注数据是自然得到的数据，预测模型表示数据的类别、转换或概论。无监督学习的本质是学习数据中的统计规律或潜在结构。</p>
<ol start="3">
<li><strong>强化学习</strong></li>
</ol>
<p>强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。强化学习的本质是学习最优的序贯决策。</p>
<p>在每一步t，只能系统从环境中观测到一个状态（state）$s_t$与一个奖励（reward）$r_t$，采取一个动作（action）$a_t$。环境根据智能系统选择的动作，决定下一步$t+1$的状态$s_{t+1}$与奖励$r_{t+1}$。要学习的策略表示为给定的状态下采取的动作。智能系统的目标不是短期奖励的最大化，而是长期积累奖励的最大化。强化学习过程中，系统不断地试错（trial and error），以达到学习最优策略的目的。</p>
<ol start="4">
<li>半监督学习与主动学习</li>
</ol>
<p>半监督学习（semi-supervised learning）是指利用标注数据和未标注数据学习预测模型的机器学习问题。通常有少量标注数据、大量未标注数据。半监督学习旨在利用未标注数据中的信息，辅助标注数据，进行监督学习，以较低的成本达到较好的学习效果。</p>
<p>主动学习（active learning）是指机器不断主动给出实力让教师进行标注，然后利用标注数据学习预测模型的机器学习问题。通常的监督学习使用给定的标注数据，往往是随机得到的，可以看作是“被动学习”，主动学习的目标是找出对学习最有帮助的实力让教师标注，以较小的标注代价，达到较好的学习效果。</p>
<h2 id="1-2-2-按模型分类"><a href="#1-2-2-按模型分类" class="headerlink" title="1.2.2 按模型分类"></a>1.2.2 按模型分类</h2><ol>
<li><strong>概率模型与非概率模型</strong></li>
</ol>
<p>在监督学习中，概率模型取条件概率分布形式$P(y|x)$，非概率模型去函数形式$y=f(x)$，其中$x$是输入，$y$是输出。</p>
<p>在无监督学习中，概率模型取条件概率分布形式$P(z|x)$或$P(x|z)$，非概率模型取函数形式$z=g(x)$，其中$x$是输入，$y$是输出。</p>
<p>条件概率分布$P(y|x)$和函数$y=f(x)$可以相互转化（条件概率分布$p(z|x)$和函数$z=g(x)$同样可以）。条件概率分布最大化后得到函数，函数归一化后得到条件概率分布。</p>
<p>概率模型和非概率模型的区别不在于输入与输出之间的映射关系，而在于模型的内在结构。概率模型通常可以表示为联合概率分布的形式，其中的变量表示输入、输出、隐变量甚至参数。而非概率模型则不一定存在这样的联合概率分布。</p>
<ol start="2">
<li><strong>线性模型与非线性模型</strong></li>
</ol>
<p>如果函数$y=f(x)$或$z=g(x)$是线性函数，则称模型是线性模型，否则称模型是非线性模型。</p>
<p>深度学习（deep learning）实际是复杂神经网络的学习，也就是复杂的非线性模型的学习。</p>
<ol start="3">
<li><strong>参数化模型与非参数化模型</strong></li>
</ol>
<p>参数化模型假设模型参数的维度固定，模型可以由有限维参数完全刻画。</p>
<p>非参数化模型假定模型参数的维度不固定或者说无穷大，随着训练数据量的增加而不断增大。</p>
<h2 id="1-2-3-按算法分类"><a href="#1-2-3-按算法分类" class="headerlink" title="1.2.3 按算法分类"></a>1.2.3 按算法分类</h2><p>统计学习根据算法，可以分为在线学习（online learning）与批量学习（batch learning）。</p>
<ol>
<li><strong>在线学习</strong></li>
</ol>
<p>在线学习是指每次接受一个样本，进行预测，之后学习模型，并不断重复该操作的机器学习。</p>
<ol start="2">
<li><strong>批量学习</strong></li>
</ol>
<p>批量学习一次接受所有数据，学习模型，之后进行预测。</p>
<h2 id="1-2-4-按技巧分类"><a href="#1-2-4-按技巧分类" class="headerlink" title="1.2.4 按技巧分类"></a>1.2.4 按技巧分类</h2><ol>
<li><strong>贝叶斯学习</strong></li>
</ol>
<p>贝叶斯学习（Bayesian learning），又称为贝叶斯推理（Bayesian inference），是统计学、机器学习中重要的方法。其主要想法是，在概率模型的学习和推理中，利用贝叶斯定理，计算在给定数据条件下模型的条件概率，即后验概率，并应用这个原理进行模型的估计，以及对数据的预测。将模型、为观测要素及其参数用变量表示，使用模型的先验分布是贝叶斯学习的特点。</p>
<p>模型估计时，估计整个后验概率分布$P(\theta|D)$。如果需要给出一个模型，通常取后验概率最大的模型。</p>
<p>假设先验分布是均匀分布，取后验概率最大，就能从贝叶斯估计得到极大似然估计。<br>$$<br>极大似然估计： \hat \theta = arg\mathop{max}\limits_{\theta}P(D|\theta)<br>$$</p>
<p>$$<br>贝叶斯估计：\hat P(\theta|D) = \frac{P(\theta)P(D|\theta)}{P(D)}<br>$$</p>
<ol start="2">
<li><strong>核方法</strong></li>
</ol>
<p>核方法（kernel method）是使用核函数表示和学习非线性模型的一种机器学习方法，可以用于监督学习和无监督学习。有一些线性模型的学习方法基于相似度计算，更具体地，向量内积计算。核方法可以把它们扩展到非线性模型的学习，使其应用范围更广泛。</p>
<p>把线性模型扩展到非线性模型，直接的做法是显示地定义从输入空间（低维空间）到特征空间（高维空间）的映射，在特征空间中进行内积计算。</p>
<p>核方法的技巧在于不显示地定义这个映射，而是直接定义核函数，即映射之后在特征空间的内积。这样可以简化计算，达到同样的效果。</p>
<p>核方法直接在输入空间中定义核函数$K(x_1,x_2)$，使其满足$K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;$。</p>
<h1 id="1-3-统计学习方法三要素"><a href="#1-3-统计学习方法三要素" class="headerlink" title="1.3 统计学习方法三要素"></a>1.3 统计学习方法三要素</h1><p>统计学习方法都是由模型、策略、和算法构成的，即统计学习方法由三要素构成，可以简单地表示为：<strong>方法 = 模型 + 策略 + 算法</strong></p>
<h2 id="1-3-1-模型"><a href="#1-3-1-模型" class="headerlink" title="1.3.1 模型"></a>1.3.1 模型</h2><p>模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。</p>
<p>假设空间用$F$表示。</p>
<ul>
<li>$F$通常是由一个参数向量决定的函数族：</li>
</ul>
<p>$$<br>F = {f|Y=f_\theta(X),\theta \in R^n}<br>$$</p>
<p>参数向量$\theta$取值于$n$维欧氏空间$R^n$，称为参数空间（parameter space）。</p>
<ul>
<li>$F$通常也是由一个参数决定的条件概率分布族：<br>$$<br>F = {P|P_{\theta}(Y|X),\theta \in R^n}<br>$$</li>
</ul>
<h2 id="1-3-2-策略"><a href="#1-3-2-策略" class="headerlink" title="1.3.2 策略"></a>1.3.2 策略</h2><p>有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。</p>
<p>损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。</p>
<ol>
<li><strong>损失函数和风险函数</strong></li>
</ol>
<p>用一个<strong>损失函数（loss function）</strong>或<strong>代价函数（cost function）</strong>来度量预测错误的程度。</p>
<p>​    <strong>(1) 0-1损失函数（0-1 loss function）</strong><br>$$<br>L(Y,f(x)) = \lgroup_{0,\ \ Y=f(x)}^{1,\ \ Y \ne f(x)}<br>$$<br>​    <strong>(2)平方损失函数（quadratic loss function）</strong><br>$$<br>L(Y,f(X)) = (Y-f(X))^2<br>$$<br>​    <strong>(3)绝对损失函数（absolute loss function）</strong><br>$$<br>L(Y,f(X)) = |Y - f(X)|<br>$$<br>​    <strong>(4)对数损失函数（logarithmic loss function）</strong><br>$$<br>L(Y,f(X)) = -logP(Y|X)<br>$$<br>损失函数值越小，模型就越好。</p>
<p>损失函数的期望是<br>$$<br>R_{exp}(f) = E_p[L(Y,f(X))] = \int_{X \times Y}L(y,f(x))P(x,y)dxdy<br>$$<br>这是理论上模型$f(x)$关于联合分布$P(X,Y)$的平均意义下的损失函数，称为<strong>风险函数（risk function）</strong>或<strong>期望损失（expected loss）</strong>。</p>
<p>学习的目标就是选择期望风险最小的模型。</p>
<p>一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就成为一个<strong>病态问题（ill-formed problem）</strong></p>
<p>模型$f(X)$关于训练数据集的平均损失称为<strong>经验风险（empirical risk）</strong>或<strong>经验损失（empirical loss）</strong>，记作$R_{emp}$：<br>$$<br>R_{emp}(f) = \frac{1}{N} \sum\limits_{i=1}^N L(y_i,f(x_i))<br>$$<br>期望风险$R_{exp}(f)$是模型关于联合分布的期望损失，经验风险$R_{emp}(f)$是模型关于训练样本集的平均损失。根据大数定律，当样本容量$N$趋于无穷时，经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$。</p>
<ol start="2">
<li><strong>经验风险最小化与结构风险最小化</strong></li>
</ol>
<ul>
<li><strong>经验风险最小化（empirical risk minimization， ERM）</strong>的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：</li>
</ul>
<p>$$<br>\mathop{min}<em>{f \in F} \frac{1}{N} \sum\limits</em>{i=1}^{N} L(y_i,f(x_i))<br>$$</p>
<p>​        <strong>极大似然估计（maximum likelihood estimation）</strong>就是经验风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。</p>
<ul>
<li><p><strong>结构风险最小化（structural risk minimization，SRM）</strong>是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term）。结构风险的定义是：<br>$$<br>R_{srm}(f) = \frac{1}{N}\sum\limits_{i=1}^N L(y_i,f(x_i))\ + \ \lambda J(f)<br>$$</p>
<p>其中$J(f)$为模型的复杂度，是定义在假设空间$F$上的泛函。模型$f$越复杂，复杂度$J(f)$就越大；反之，模型$f$越简单，复杂度$J(f)$就越小。复杂度表示了对复杂模型的惩罚。$\lambda\ge 0$是系数，用以权衡经验风险和模型复杂度。</p>
<p>结构风险最小化的策略认为结构风险最小的模型是最优的模型。所以求最优模型，就是求解最优化问题：<br>$$<br>\mathop{min}<em>{f \in F} \frac{1}{N} \sum\limits</em>{i=1}^{N} L(y_i,f(x_i)) \ + \ \lambda J(f)<br>$$</p>
</li>
</ul>
<h2 id="1-3-3-算法"><a href="#1-3-3-算法" class="headerlink" title="1.3.3 算法"></a>1.3.3 算法</h2><p>算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。</p>
<p>统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。</p>
<p>统计学习方法之间的不同，主要来自其模型、策略、算法的不同。确定了模型、策略、算法，统计学习的方法也就确定了。</p>
<h1 id="1-4-模型评估与模型选择"><a href="#1-4-模型评估与模型选择" class="headerlink" title="1.4 模型评估与模型选择"></a>1.4 模型评估与模型选择</h1><h2 id="1-4-1-训练误差与测试误差"><a href="#1-4-1-训练误差与测试误差" class="headerlink" title="1.4.1 训练误差与测试误差"></a>1.4.1 训练误差与测试误差</h2><p>当损失函数给定时，基于损失函数的模型的<strong>训练误差（training error）</strong>和模型的<strong>测试误差（test error）</strong>就自然成为学习方法评估的标准。统计学习方法具体采用的损失函数未必是评估时使用的损失函数。当然，让两者一致是比较理想的。</p>
<p><strong>误差率（error rate，e）</strong>与<strong>准确率（accuracy，r）</strong>的关系为：<br>$$<br>r + e = 1<br>$$<br>将学习方法对未知数据的预测能力称为<strong>泛化能力（generalization ability）</strong>。</p>
<h2 id="1-4-2-过拟合与模型选择"><a href="#1-4-2-过拟合与模型选择" class="headerlink" title="1.4.2 过拟合与模型选择"></a>1.4.2 过拟合与模型选择</h2><p>当假设空间含有不同复杂度（例如，不同的参数个数）的模型时，就要面临<strong>模型选择（model selection）</strong>的问题。</p>
<p>如果在假设空间存在“真”模型，那么所选择的模型应该逼近真模型。具体地，所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。</p>
<p>如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高。这种现象称为<strong>过拟合（over-fitting）</strong>。</p>
<p>过拟合是指选择的模型所包含的参数过多，以致出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。可以说模型选择旨在避免过拟合并提供高模型的预测能力。</p>
<p>设$M$次多项式为<br>$$<br>f_M(x,w)=w_0 + w_1x + w_2x^2 + … + w_Mx^M = \sum\limits_{j = 0} ^ M w_j x^j<br>$$<br>式中$x$是单变量输入，$w_0,w_1,…,w_M$是$M+1$个参数。</p>
<p>首先确定模型的复杂度，即确定多项式的次数；然后在给定的模型复杂度下，按照经验风险最小化的策略，求解参数，即多项式的系数。具体地，求以下经验风险最小化：<br>$$<br>L(w)=\frac{1}{2}\sum\limits_{i=1}^N (f(x_i,w) - y_i)^2<br>$$<br>这是，损失函数为平方函数，系数$\frac{1}{2}$是为了计算方便。</p>
<p>模型选择时，不仅要考虑对已知数据的预测能力，而且还要考虑对未知数据的预测能力。</p>
<h1 id="1-5-正则化与交叉验证"><a href="#1-5-正则化与交叉验证" class="headerlink" title="1.5 正则化与交叉验证"></a>1.5 正则化与交叉验证</h1><h2 id="1-5-1-正则化"><a href="#1-5-1-正则化" class="headerlink" title="1.5.1 正则化"></a>1.5.1 正则化</h2><p>模型选择的典型方法时<strong>正则化（regularization）</strong>。正则化时结构风险最小化策略的实现，是在经验风险上加一个<strong>正则化项（regularizer）</strong>或<strong>罚项（penalty term）</strong>。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。</p>
<p>正则化一般具有如下形式：<br>$$<br>\mathop{min}<em>{f \in F} \frac{1}{N} \sum\limits</em>{i=1}^{N} L(y_i,f(x_i)) \ + \ \lambda J(f)<br>$$<br>正则化的作用是选择经验风险与模型复杂度同时较小的模型。</p>
<p>正则化符合<strong>奥卡姆剃刀（Occam`s razor）</strong>原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。</p>
<h2 id="1-5-2-交叉验证"><a href="#1-5-2-交叉验证" class="headerlink" title="1.5.2 交叉验证"></a>1.5.2 交叉验证</h2><p>如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为<strong>训练集（training set）</strong>、<strong>验证集（validation set）</strong>和<strong>测试集（test set）</strong>。</p>
<p>训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。</p>
<p>在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。由于验证集有足够多的数据，用它对模型进行选择也是有效的。</p>
<ol>
<li><strong>简单交叉验证</strong></li>
</ol>
<p>首先随即地将已给数据分为两部分，一部分作为训练集，另一部分作为测试集（例如，70%的数据为训练集，30%的数据为测试机）；然后用训练集在各种条件下（例如，不同的参数个数）训练模型，从而得到不同的模型；在测试集上评估各个模型的测试误差，选出测试误差最小的模型。</p>
<ol start="2">
<li><strong>$S$折交叉验证</strong></li>
</ol>
<p>首先随机地将已给数据切分为$S$个互不相交、大小相同的子集；然后利用$S-1$个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的$S$种选择重复进行；最后选出$S$次评测中平均测试误差最小的模型。</p>
<ol start="3">
<li><strong>留一交叉验证</strong></li>
</ol>
<p>$S$折交叉验证的特殊情形是$S = N$，称为留一交叉验证（leave-one-out cross validation），往往在数据缺乏的情况下使用。</p>
<h1 id="1-6-泛化能力"><a href="#1-6-泛化能力" class="headerlink" title="1.6 泛化能力"></a>1.6 泛化能力</h1><h2 id="1-6-1-泛化误差"><a href="#1-6-1-泛化误差" class="headerlink" title="1.6.1 泛化误差"></a>1.6.1 泛化误差</h2><p>学习方法的<strong>泛化能力（generalization ability）</strong>是指该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。</p>
<p>如果学到的模型是$\hat f$，那么用这个模型对未知数据预测的误差即为<strong>泛化误差（generalization error）</strong>：<br>$$<br>R_{exp}(\hat f) = E_p[L(Y,\hat f(X))] = \int_{X \times Y}L(y,\hat f(x))P(x,y)dxdy<br>$$<br>事实上，泛化误差就是所学习到的模型的期望风险。</p>
<h2 id="1-6-2-泛化误差上界"><a href="#1-6-2-泛化误差上界" class="headerlink" title="1.6.2 泛化误差上界"></a>1.6.2 泛化误差上界</h2><p>学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称为<strong>泛化误差上界（generalization error bound）</strong>。</p>
<p>具体来说，就是通过比较两种学习方法的泛化误差上界的大小来比较它们的优劣。</p>
<p>泛化误差上界通常具有以下性质：</p>
<ul>
<li>它是样本容量的函数，当样本容量增加时，泛化上界趋于0；</li>
<li>它是假设空间容量（capacity）的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。</li>
</ul>
<p><strong>定理1.1（泛化误差上界） 对二分类问题，当假设空间是有限个函数的集合$F={f_1,f_2,…,f_d}$时，对任意一个函数$f \in F$，至少以概率$1-\delta,0&lt;\delta&lt;1$，以下不等式成立：</strong><br>$$<br>R(f) \leq \hat R(f) + \epsilon(d,N,\delta)<br>$$<br><strong>其中$R(f)$为期望风险，$\hat R(f)$为经验风险，$\epsilon(d,N,\delta)=\sqrt{\frac{1}{2N}(\log d + \log \frac{1}{\delta})}$</strong></p>
<p>左端$R(f)$是泛化误差，右端即为泛化误差上界。在泛化误差上界中，第1项是训练误差，训练误差越小，泛化误差也越小。第2项$\epsilon(d,N,\delta)$是$N$的单调递减函数，当$N$趋于无穷时趋于0；同时它也是$\sqrt{\log d}$阶的函数，假设空间$F$包含的函数越多，其值越大。</p>
<h1 id="1-7-生成模型与判别模型"><a href="#1-7-生成模型与判别模型" class="headerlink" title="1.7 生成模型与判别模型"></a>1.7 生成模型与判别模型</h1><p>监督学习方法又可以分为<strong>生成方法（generative approach）</strong>和<strong>判别方法（discriminative approach）</strong>。所学到的模型分别称为<strong>生成模型（generative model）</strong>和<strong>判别模型（discriminative model）</strong>。</p>
<ul>
<li>生成方法原理上由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型：</li>
</ul>
<p>$$<br>P(Y|X) = \frac{P(X,Y)}{P(X)}<br>$$</p>
<p>​        这样的方法之所以称为生成方法，是因为模型表示了给定输入$X$产生输出$Y$的生成关系。</p>
<ul>
<li>判别方法由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。判别方法关心的是对给定的输入$X$，应该预测什么样的输出$Y$。</li>
</ul>
<p>生成方法的特点：生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。</p>
<p>判别方法的特点：判别方法直接学习的时条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率更高；由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</p>
<h1 id="1-8-监督学习应用"><a href="#1-8-监督学习应用" class="headerlink" title="1.8 监督学习应用"></a>1.8 监督学习应用</h1><h2 id="1-8-1-分类问题"><a href="#1-8-1-分类问题" class="headerlink" title="1.8.1 分类问题"></a>1.8.1 分类问题</h2><p>监督学习从数据中学习一个分类模型或分类决策函数，称为<strong>分类器（classifier）</strong>。分类器对新的输入进行输出的预测，称为<strong>分类（classification）</strong>。可能的输出称为<strong>类（class）</strong>。</p>
<p>评价分类器性能的指标一般是<strong>分类准确率（accuracy）</strong>，其定义是：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也就是损失函数是0-1损失时测试数据集上的准确率。</p>
<p>对于二分类问题常用的评价指标是<strong>精确率（precision）</strong>和<strong>召回率（recall）</strong>。</p>
<p>4种情况出现的总数分别记作：</p>
<ul>
<li><p>$TP$   ————将正类预测为正类数；</p>
</li>
<li><p>$FN$  ————将正类预测为负类数；</p>
</li>
<li><p>$FP$  ————将负类预测为正类数；</p>
</li>
<li><p>$TN$  ————将负类预测为负类数；</p>
</li>
</ul>
<p>精确率定义为：<br>$$<br>P = \frac{TP}{TP+FP}<br>$$<br>召回率定义为：<br>$$<br>R = \frac{TP}{TP+FN}<br>$$<br>此外，还有$F_1$值，是精确率和召回率的调和均值，即<br>$$<br>\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}<br>$$</p>
<p>$$<br>F_1 = \frac{2TP}{2TP + FP + FN}<br>$$</p>
<h2 id="1-8-2-标注问题"><a href="#1-8-2-标注问题" class="headerlink" title="1.8.2 标注问题"></a>1.8.2 标注问题</h2><p><strong>标注（tagging）</strong>也是一个监督学习问题。</p>
<p>评价标准模型的指标与评价分类模型的指标一样，常用的有标注准确率、精确率和召回率。</p>
<p>标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。</p>
<p>自然语言处理中的<strong>词性标注（part of speech tagging）</strong>就是一个典型的标注问题：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对每一个单词序列预测其对应的词性标记序列。</p>
<h2 id="1-8-3-回归问题"><a href="#1-8-3-回归问题" class="headerlink" title="1.8.3 回归问题"></a>1.8.3 回归问题</h2><p>回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化。</p>
<p>回归模型正是表示从输入变量到输出变量之间映射的函数。回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。</p>
<p>回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型即模型的类型，分为线性回归和非线性回归。</p>
<p>回归学习常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的<strong>最小二乘法（least squares）</strong>求解。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/06/03/Python_Notes/%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%88%97%E8%A1%A8%E7%AE%80%E4%BB%8B/" rel="prev" title="第三章 列表简介">
      <i class="fa fa-chevron-left"></i> 第三章 列表简介
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/06/04/Python_Notes/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E6%93%8D%E4%BD%9C%E5%88%97%E8%A1%A8/" rel="next" title="第四章 操作列表">
      第四章 操作列表 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-1-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">1.1 统计学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-2-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB"><span class="nav-number">2.</span> <span class="nav-text">1.2 统计学习的分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-1-%E5%9F%BA%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="nav-number">2.1.</span> <span class="nav-text">1.2.1 基本分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-2-%E6%8C%89%E6%A8%A1%E5%9E%8B%E5%88%86%E7%B1%BB"><span class="nav-number">2.2.</span> <span class="nav-text">1.2.2 按模型分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-3-%E6%8C%89%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB"><span class="nav-number">2.3.</span> <span class="nav-text">1.2.3 按算法分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-4-%E6%8C%89%E6%8A%80%E5%B7%A7%E5%88%86%E7%B1%BB"><span class="nav-number">2.4.</span> <span class="nav-text">1.2.4 按技巧分类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-3-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B8%89%E8%A6%81%E7%B4%A0"><span class="nav-number">3.</span> <span class="nav-text">1.3 统计学习方法三要素</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-1-%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.1.</span> <span class="nav-text">1.3.1 模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-2-%E7%AD%96%E7%95%A5"><span class="nav-number">3.2.</span> <span class="nav-text">1.3.2 策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-3-%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.</span> <span class="nav-text">1.3.3 算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-4-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">4.</span> <span class="nav-text">1.4 模型评估与模型选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-1-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE"><span class="nav-number">4.1.</span> <span class="nav-text">1.4.1 训练误差与测试误差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-2-%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">4.2.</span> <span class="nav-text">1.4.2 过拟合与模型选择</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-5-%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">5.</span> <span class="nav-text">1.5 正则化与交叉验证</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-1-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">5.1.</span> <span class="nav-text">1.5.1 正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-2-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">5.2.</span> <span class="nav-text">1.5.2 交叉验证</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-6-%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="nav-number">6.</span> <span class="nav-text">1.6 泛化能力</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-1-%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="nav-number">6.1.</span> <span class="nav-text">1.6.1 泛化误差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-2-%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%E4%B8%8A%E7%95%8C"><span class="nav-number">6.2.</span> <span class="nav-text">1.6.2 泛化误差上界</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-7-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.</span> <span class="nav-text">1.7 生成模型与判别模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-8-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8"><span class="nav-number">8.</span> <span class="nav-text">1.8 监督学习应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-8-1-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-number">8.1.</span> <span class="nav-text">1.8.1 分类问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-8-2-%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98"><span class="nav-number">8.2.</span> <span class="nav-text">1.8.2 标注问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-8-3-%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="nav-number">8.3.</span> <span class="nav-text">1.8.3 回归问题</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Normal People"
      src="/images/posthead.jpg">
  <p class="site-author-name" itemprop="name">Normal People</p>
  <div class="site-description" itemprop="description">Get busy libing or get busy dying</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/TheNormalPeople" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;TheNormalPeople" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1272481411@qq.com" title="E-Mail → mailto:1272481411@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/5938927274" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;5938927274" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/cy19970506" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;cy19970506" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Normal People</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'y8XFURQNCoQsprRTou9DiEJu-gzGzoHsz',
      appKey     : 'QfVpjKDtJQdJrnJNnWUbVvjH',
      placeholder: "留下邮箱,有空时间回复您！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
