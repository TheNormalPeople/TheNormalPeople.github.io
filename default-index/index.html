<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true,"scrollpercent":true,"b2t":true},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Get busy living or get busy dying">
<meta property="og:type" content="website">
<meta property="og:title" content="Blog">
<meta property="og:url" content="http://example.com/default-index/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="Get busy living or get busy dying">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Normal People">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/default-index/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">by Normal People</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">6</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">30</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/24/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/24/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/" class="post-title-link" itemprop="url">第十五章 奇异值分解</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-24 15:01:33 / 修改时间：15:24:35" itemprop="dateCreated datePublished" datetime="2021-06-24T15:01:33+08:00">2021-06-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/24/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/24/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>奇异值分解（singular value decomposition，SVD）</strong>是一种矩阵因子分解方法，是线性代数的概念，但在统计学习中被广泛使用，成为其重要工具。</p>
<p>任意一个$m \times n$矩阵，都可以表示为三个矩阵的乘积（因子分解）形式，分别是$m$阶正交矩阵、由降序排列的非负的对角线元素组成的$m \times n$矩形对角矩阵和$n$阶正交矩阵，称为该矩阵的奇异值分解。</p>
<p>矩阵的奇异值分解一定存在，但不唯一。</p>
<p>奇异值分解可以看作是矩阵数据压缩的一种方法，即用因子分解的方式近似地表示原始矩阵，这种近似是在平方损失意义下的最优近似。</p>
<h1 id="15-1-奇异值分解的定义与性质"><a href="#15-1-奇异值分解的定义与性质" class="headerlink" title="15.1 奇异值分解的定义与性质"></a>15.1 奇异值分解的定义与性质</h1><h2 id="15-1-1-定义与定理"><a href="#15-1-1-定义与定理" class="headerlink" title="15.1.1 定义与定理"></a>15.1.1 定义与定理</h2><p><strong>定义15.1（奇异值分解）</strong>  矩阵的奇异值分解是指，将一个非零的$m \times n$实矩阵$A, A \in R^{m \times n}$，表示为一下三个实矩阵乘积形式的运算，即进行矩阵的因子分解：<br>$$<br>A = U \Sigma V^T \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.1)<br>$$<br>其中$U$是$m$阶<strong>正交矩阵（orthogonal matrix）</strong>，$V$是$n$阶正交矩阵，$\Sigma$是由降序排列的非负的对角线元素组成的$m \times n$<strong>矩形对角矩阵（rectangular diagonal matrix）</strong>，满足<br>$$<br>UU^T = I<br>$$</p>
<p>$$<br>VV^T = I<br>$$</p>
<p>$$<br>\Sigma = diag(\sigma_1,\sigma_2,…,\sigma_p)<br>$$</p>
<p>$$<br>\sigma_1 \geq \sigma_2 \geq… \geq \sigma_p \geq 0<br>$$</p>
<p>$$<br>p = \min(m,n)<br>$$</p>
<p>$U \Sigma V^T$称为矩阵$A$的<strong>奇异值分解（singular value decomposition，SVD）</strong>，$\sigma_i$称为矩阵$A$的<strong>奇异值（singular value）</strong>，$U$的列向量称为<strong>左奇异向量（left singular value）</strong>，$V$的列向量称为<strong>右奇异向量（right singular value）</strong>。</p>
<hr>
<p><strong>定理 15.1（奇异值分解基本定理）</strong>  若$A$为一$m \times n$实矩阵，$A \in R^{m\times n}$，则$A$的奇异值分解存在<br>$$<br>A = U \Sigma V^T \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.2)<br>$$<br>其中$U$是$m$阶正交矩阵，$V$是$n$阶正交矩阵，$\Sigma$是$m \times n$矩形对角矩阵，其对角线元素非负 且按降序排列。</p>
<h2 id="15-1-2-紧奇异值分解与截断奇异值分解"><a href="#15-1-2-紧奇异值分解与截断奇异值分解" class="headerlink" title="15.1.2 紧奇异值分解与截断奇异值分解"></a>15.1.2 紧奇异值分解与截断奇异值分解</h2><p>定理15.1给出的奇异值分解<br>$$<br>A = U \Sigma V^T<br>$$<br>又称为矩阵的<strong>完全奇异值分解（full singular value decomposition）</strong>。实际常用的是奇异值分解的紧凑形式和截断形式。紧奇异值分解是与原始矩阵等秩的奇异值分解，截断奇异值分解是比原始矩阵低秩的奇异值分解。</p>
<ol>
<li><strong>紧奇异值分解</strong></li>
</ol>
<p> <strong>定义 15.2</strong>  设有$m \times n$实矩阵$A$，其秩为$rank(A) = r,r \leq min(m,n)$，则称$U_r\Sigma_r V_r^T$为$A$的紧奇异值分解（compact singular value decomposition），即<br>$$<br>A =U_r\Sigma_r V_r^T \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.18)<br>$$<br>其中$U_r$是$m \times r$矩阵，$V_r$是$n \times r$矩阵，$\Sigma_r$是$r$阶对角矩阵；矩阵$U_r$由完全奇异值分解中$U$的前$r$列、矩阵$V_r$由$V$的前$r$列、矩阵$\Sigma_r$由$\Sigma$的前$r$个对角线元素得到。紧奇异值分解的对角矩阵$\Sigma_r$的秩与原始矩阵$A$的秩相等。</p>
<hr>
<ol start="2">
<li><strong>截断奇异值分解</strong></li>
</ol>
<p>再聚真的奇异值分解中，只取最大的$k$个奇异值（$k&lt;r$，$r$为矩阵的秩）对应的部分，就得到矩阵的截断奇异值分解。实际应用中提到矩阵的奇异值分解时，通常指截断奇异值分解。</p>
<p>定义 15.3  设$A$为$m \times n$实矩阵，其秩$rank(A) = r$，且$ 0 &lt; k &lt; r$，则称$U_k\Sigma_k V_k^T$为矩阵$A$的截断奇异值分解（truncated singular value decomposition）<br>$$<br>A \approx U_k\Sigma_k V_k^T\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.19)<br>$$<br>其中$U_k$是$m \times k$矩阵，$V_k$是$n \times k$矩阵，$\Sigma_k$是$k$阶对角矩阵；矩阵$U_k$由完全奇异值分解中$U$的前$k$列、矩阵$V_k$由$V$的前$k$列、矩阵$\Sigma_k$由$\Sigma$的前$r$个对角线元素得到。对角矩阵$\Sigma_k$的秩比原始矩阵$A$的秩低。</p>
<p>在实际应用中，常常需要对矩阵的数据进行压缩，将其近似表示，奇异值分解提供了一种方法。紧奇异值分解对应着无损压缩，截断奇异值分解对应着有损压缩。</p>
<h2 id="15-1-3-几何解释"><a href="#15-1-3-几何解释" class="headerlink" title="15.1.3 几何解释"></a>15.1.3 几何解释</h2><p>从线性变换的角度理解奇异值分解，$m \times n$矩阵$A$表示从$n$维空间$R^n$到$m$维空间$R^m$的一个线性变换，<br>$$<br>T : x \rightarrow Ax<br>$$<br>$x \in R^n,Ax \in R^m$，$x$和$Ax$分别是各自空间的向量。</p>
<p>线性变换可以分解为三个简单的变换：</p>
<ul>
<li>一个坐标系的旋转或反射变换；</li>
<li>一个坐标轴的缩放变换；</li>
<li>另一个坐标系的旋转或反射变换。</li>
</ul>
<h2 id="15-1-4-主要性质"><a href="#15-1-4-主要性质" class="headerlink" title="15.1.4 主要性质"></a>15.1.4 主要性质</h2><p>（1）设矩阵$A$的奇异值分解为$A = U \Sigma V^T$，则以下关系成立：<br>$$<br>A^TA = (U \Sigma V^T)^T(U \Sigma V^T) = V(\Sigma^T \Sigma)V^T\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.20)<br>$$</p>
<p>$$<br>AA^T =(U \Sigma V^T)(U \Sigma V^T)^T = U(\Sigma \Sigma^T)U^T\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.21)<br>$$</p>
<p>也就是说，矩阵$A^TA$和矩阵$AA^T$的特征分解存在，且可以由矩阵$A$的奇异值分解的矩阵表示。$V$的列向量是$A^TA$的特征向量，$U$的列向量是$AA^T$的特征向量，$\Sigma$的奇异值是$A^TA$和$AA^T$的特征值的平方根。</p>
<p>（2）在矩阵$A$的奇异值分解中，奇异值、左奇异向量和右奇异向量之间存在对应关系。</p>
<p>由$A = U \Sigma V^T$易知<br>$$<br>AV = U\Sigma<br>$$<br>比较这一等式两端的第$j$列，得到<br>$$<br>Av_j = \sigma_j u_j\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.22)<br>$$<br>这是矩阵$A$的右奇异向量和奇异值、左奇异向量的关系。</p>
<p>类似地，得到<br>$$<br>A^T u_j = \sigma_jv_j, \ \ \ j = 1,2,…,n\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.23)<br>$$</p>
<p>$$<br>A^Tu_j = 0,\ \ \ j = n+1,n+2,…,m<br>$$</p>
<p>这是矩阵$A$的左奇异向量和奇异值、右奇异向量的关系。</p>
<p>（3）矩阵$A$的奇异值分解中，奇异值$\sigma_1,\sigma_2,…,\sigma_n$是唯一的，而矩阵$U$和$V$不是唯一的。</p>
<p>（4）矩阵$A$和$\Sigma$的秩相等，等于正奇异值$\sigma_i$的个数$r$（包含重复的奇异值）。</p>
<p>（5）</p>
<ul>
<li>矩阵$A$的$r$个右奇异向量$v_1,v_2,…,v_r$构成$A^T$的值域$R(A^T)$的一组标准正交基。</li>
<li>矩阵$A$的$n-r$个右奇异向量$v_{r+1},v_{r+2},…,v_n$构成$A$的零空间$N(A)$的一组标准正交基。</li>
<li>矩阵$A$的$r$个左奇异向量$u_1,u_2,…,u_r$构成值域$R(A)$的一组标准正交基。</li>
<li>矩阵$A$的$m-r$个左奇异向量$u_{r+1},u_{r+2},…,u_m$构成$A^T$的零空间$N(A^T)$的一组标准正交基。</li>
</ul>
<h1 id="15-2-奇异值分解的计算"><a href="#15-2-奇异值分解的计算" class="headerlink" title="15.2 奇异值分解的计算"></a>15.2 奇异值分解的计算</h1><p>矩阵$A$的奇异值分解可以通过求对称矩阵$A^TA$的特征值和特征向量得到。$A^TA$的特征向量构成正交矩阵$V$的列；$A^TA$的特征值$\lambda_j$的平方根为奇异值$\sigma_j$，即<br>$$<br>\sigma_j = \sqrt{\lambda_j} \ \ \ j = 1,2,…,n<br>$$<br>对其由大到小排列作为对角线元素，构成对角矩阵$\Sigma$；求正奇异值对应的左奇异向量，再求扩充的$A^T$的标准正交基，构成正交矩阵$U$的列。从而得到$A$的奇异值分解$A = U \Sigma V^T$。</p>
<p>（1）首先求$A^TA$的特征值和特征向量。</p>
<p>计算对称矩阵$W = A^TA$。</p>
<p>求解特征方程<br>$$<br>(W-\lambda I)x = 0<br>$$<br>得到特征值$\lambda_i$，并将特征值由大到小排列<br>$$<br>\lambda_1  \geq \lambda_2 \geq … \geq \lambda_n \geq 0<br>$$<br>将特征值$\lambda_i(i= 1,2,…,n)$代入特征方程求得对应的特征向量。</p>
<p>（2）求$n$阶正交矩阵$V$</p>
<p>将特征向量单位化，得到单位特征向量$v_1,v_2,…,v_n$，构成$n$阶正交矩阵$V$；<br>$$<br>V = [v_1,v_2…v_n]<br>$$<br>（3）求$m \times n$对角矩阵$\Sigma$</p>
<p>计算$A$的奇异值<br>$$<br>\sigma_j = \sqrt{\lambda_j} \ \ \ j = 1,2,…,n<br>$$<br>构造$m \times n$矩形对角矩阵$\Sigma$，主对角元素是奇异值，其余元素是零，<br>$$<br>\Sigma = diag(\sigma_1,\sigma_2,…,\sigma_n)<br>$$<br>（4）求$m$阶正交矩阵$U$</p>
<p>对$A$的前$r$个正奇异值，令<br>$$<br>u_j = \frac{1}{\sigma_j}Av_j, \ \ \ j = 1,2,…,r<br>$$<br>得到<br>$$<br>U_1 = [u_1,u_2,…,u_r]<br>$$<br>求$A^T$的零空间的一组标准正交基$u_{r+1},r_{r+2},…,u_m$，令<br>$$<br>U_2 = [u_{r+1},r_{r+2},…,u_m]<br>$$<br>并令<br>$$<br>U = [U_1,U_2]<br>$$<br>（5）得到奇异值分解<br>$$<br>A = U \Sigma V^T<br>$$<br>实际应用的奇异值分解算法是通过求$A^TA$的特征值进行，但不直接计算$A^TA$。</p>
<h1 id="15-3-奇异值分解与矩阵近似"><a href="#15-3-奇异值分解与矩阵近似" class="headerlink" title="15.3 奇异值分解与矩阵近似"></a>15.3 奇异值分解与矩阵近似</h1><h2 id="15-3-1-弗罗贝尼乌斯范数"><a href="#15-3-1-弗罗贝尼乌斯范数" class="headerlink" title="15.3.1 弗罗贝尼乌斯范数"></a>15.3.1 弗罗贝尼乌斯范数</h2><p>奇异值分解也是一种矩阵近似的方法，这个近似是在弗罗贝尼乌斯范数（Frobenius norm）意义下的近似。矩阵的弗罗贝尼乌斯范数是向量的$L_2$范数的直接推广，对应着机器学习中的平方损失函数。</p>
<p><strong>定义 15.4（弗罗贝尼乌斯范数）</strong>  设矩阵$A \in R^{m \times n}$， $A = \left[ a_{ij} \right]<em>{m \times n}$，定义矩阵$A$的弗罗贝尼乌斯范数为<br>$$<br>||A||</em>{F} = (\sum\limits_{i = 1}^{m} \sum\limits_{j = 1}^{n} ( a_{ij})^2 )^{ \frac{1}{2} }\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.25)<br>$$<br><strong>引理 15.1</strong>  设矩阵$A \in R^{m \times n}$，$A$的奇异值分解为$U \Sigma V^T$，其中$\Sigma = diag(\sigma_1,\sigma_2,…,\sigma_n)$，则<br>$$<br>||A||_{F} = (\sigma_1^2,\sigma_2^2,…,\sigma_n^2)^{\frac{1}{2} }\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.26)<br>$$</p>
<h2 id="15-3-2-矩阵的最优近似"><a href="#15-3-2-矩阵的最优近似" class="headerlink" title="15.3.2 矩阵的最优近似"></a>15.3.2 矩阵的最优近似</h2><p>奇异值分解是在平方损失（弗罗贝尼乌斯范数）意义下对矩阵的最优近似，即数据压缩。</p>
<p><strong>定理 15.2</strong>  设矩阵$A \in R^{m \times n}$，矩阵的秩$rank(A) = r$，并设$\mathcal{M}$为$R^{m \times n}$中所有秩不超过$k$的矩阵集合，$ 0 &lt; k &lt;r$，则存在一个秩为$k$的矩阵$X \in \mathcal{M}$，使得<br>$$<br>||A - X||<em>{F} = \min\limits</em>{ S \in \mathcal{M} } ||A -S||_{F} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.31)<br>$$<br>称矩阵$X$为矩阵$A$在弗罗贝尼乌斯范数意义下的最优近似。</p>
<p><strong>定理 15.3</strong>  设矩阵$A \in R^{m \times n}$，矩阵的秩$rank(A) = r$，有奇异值分解$A = U \Sigma V^T$，并设$\mathcal{M}$为$R^{m\times n}$中所有秩不超过$k$的矩阵的集合，$0&lt;k&lt;r$，若秩为$k$的矩阵$X \in \mathcal{M}$满足<br>$$<br>||A - X||<em>{F} = \min\limits</em>{S \in \mathcal{M} }||A -S||<em>{F}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.32)<br>$$<br>则<br>$$<br>||A - X||</em>{F} =( \sigma_{k+1}^2, \sigma_{k+2}^2,…, \sigma_n^2)^{ \frac{1}{2} }\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.33)<br>$$<br>特别地，若$A^{‘} = U \Sigma^{‘} V^T$，其中<br>$$<br>\left[ \begin{matrix}<br>\sigma_1 &amp; &amp; &amp; &amp; &amp; \<br> &amp; …&amp; &amp; &amp; 0 &amp; \<br> &amp; &amp; &amp;\sigma_k &amp; &amp; \<br>  &amp; &amp; &amp; 0 &amp; &amp; \<br>  &amp; 0 &amp; &amp; &amp; …&amp; \<br>  &amp; &amp; &amp; &amp; &amp; 0 \<br>\end{matrix} \right] </p>
<p>= \left[ \begin{matrix}<br>\Sigma_k &amp;0 \<br>0&amp;0<br>\end{matrix} \right]<br>$$<br>则<br>$$<br>||A-A^{‘}||<em>F =(\sigma</em>{k+1}^2,\sigma_{k+2}^2,…,\sigma_n^2)^{\frac{1}{2}} =\min\limits_{S \in \mathcal{M}}||A -S||_F\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.34)<br>$$<br>定理 15.3 表明，在秩不超过$k$的$m\times n$矩阵的集合中，存在矩阵$A$的弗罗贝尼乌斯范数意义下的最优近似矩阵$X$。$A^{‘} = U \Sigma^{‘} V^T$是达到最优值的一个矩阵。</p>
<h1 id="15-3-3-矩阵的外积展开式"><a href="#15-3-3-矩阵的外积展开式" class="headerlink" title="15.3.3 矩阵的外积展开式"></a>15.3.3 矩阵的外积展开式</h1><p>矩阵$A$的奇异值分解$U\Sigma V^T$也可以由外积形式表示。事实上，若将$A$的奇异值分解看成矩阵$U\Sigma$和$V^T$的乘积，将$U\Sigma$按列向量分块，将$V^T$按行向量分块，即得<br>$$<br>U\Sigma = [\sigma_1u_1 \ \ \sigma_1u_1 \ \ … \ \ \ \sigma_1u_1]<br>$$</p>
<p>$$<br>V^T  =\left[ \begin{matrix}<br>v_1^T\<br>v_2^T\<br>…\<br>v_N^T<br>\end{matrix} \right]<br>$$</p>
<p>则<br>$$<br>A = \sigma_1u_1v_1^T + \sigma_2u_2v_2^T+…+\sigma_nu_nv_n^T\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.45)<br>$$<br>式$(15.45)$称为矩阵$A$的外积展开式，其中$u_kv_k^T$为$m\times n$矩阵，是列向量$u_k$和行向量$v_k^T$的外积，其第$i$行第$j$列元素为$u_k$的第$i$个元素与$v_k^T$的第$j$个元素的乘积。</p>
<p>$A$的外积展开式也可以写成下面的形式<br>$$<br>A = \sum\limits_{k=1}^nA_k = \sum\limits_{k=1}^n \sigma_ku_kv_k^T\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.46)<br>$$<br>其中$A_k = \sigma_ku_kv_k^T$是$m \times n$矩阵。式$(15.46)$将矩阵$A$分解为矩阵的有序加权和。</p>
<p>由矩阵$A$的外积展开式知，若$A$的秩为$n$，则<br>$$<br>A =\sigma_1u_1v_1^T + \sigma_2u_2v_2^T+…+\sigma_nu_nv_n^T\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (15.47)<br>$$<br>一般地，设矩阵<br>$$<br>A_k =\sigma_1u_1v_1^T + \sigma_2u_2v_2^T+…+\sigma_ku_kv_k^T<br>$$<br>则$A_k$的秩为$k$，并且$A_k$是秩为$k$的矩阵中在弗罗贝尼乌斯范数意义下$A$的最优近似矩阵。矩阵$A_k$就是$A$的截断奇异值分解。</p>
<p>由于通常奇异值$\sigma_i$递减很快，所以$k$取很小值时，$A_k$也可以对$A$有很好的近似。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/22/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/22/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">第十四章 聚类方法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-22 15:28:05" itemprop="dateCreated datePublished" datetime="2021-06-22T15:28:05+08:00">2021-06-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-23 21:25:08" itemprop="dateModified" datetime="2021-06-23T21:25:08+08:00">2021-06-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/22/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/22/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>聚类时针对给定的样本，依据它们特征的相似度或距离，将其归并到若干个“类”或“簇”的数据分析问题。一个类是给定样本集合的一个子集。</p>
<p>直观上，相似的样本聚集在相同的类，不相似的样本分散在不同的类。</p>
<h1 id="14-1-聚类的基本概念"><a href="#14-1-聚类的基本概念" class="headerlink" title="14.1 聚类的基本概念"></a>14.1 聚类的基本概念</h1><p>包括样本之间的距离或相似度，类或簇，类与类之间的距离。</p>
<h2 id="14-1-1-相似度或距离"><a href="#14-1-1-相似度或距离" class="headerlink" title="14.1.1 相似度或距离"></a>14.1.1 相似度或距离</h2><p>聚类的核心概念是<strong>相似度（similarity）</strong>或<strong>距离（distance）</strong>，有多种相似度或距离的定义。因为相似度直接影响聚类的结果，所以其选择是聚类的根本问题。</p>
<p>矩阵的第$j$列表示第$j$个样本，$j=1,2,…n$；第$i$行表示第$i$个属性，$i = 1,2,…,m$；</p>
<p>矩阵元素$x_{ij}$表示第$j$个样本的第$i$个属性值，$i = 1,2,…,m;j = 1,2,…,n$。</p>
<hr>
<ol>
<li><strong>闵可夫斯基距离</strong></li>
</ol>
<p>在聚类中，可将样本集合看作是向量空间中点的集合，以该空间的距离表示样本之间的相似度。常用的距离有闵可夫斯基距离，特别是欧氏距离。闵可夫斯基距离越大相似度越小，距离越小相似度越大。</p>
<p><strong>定义 14.1</strong>  给定样本集合$X$，$X$是$m$维实数向量空间$R^m$中点的集合，其中$x_i,x_j \in X, x_i=(x_{1i},x_{2i},…,x_{mi})^T,x_i=(x_{1j},x_{2j},…,x_{mj})^T$，样本$x_i$与样本$x_j$的<strong>闵可夫斯基距离（Minkowski distance）</strong>定义为<br>$$<br>d_{ij} = (\sum\limits_{k=1}^{m}|x_{ki} - x_{kj}|^p)^{\frac{1}{p}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.2)<br>$$<br>这里$p \geq 1$。当$p=2$时称为<strong>欧氏距离（Euclidean distance）</strong>，即<br>$$<br>d_{ij} = (\sum\limits_{k=1}^{m}|x_{ki} - x_{kj}|^2)^{\frac{1}{2}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.3)<br>$$<br>当$p = 1$时称为<strong>曼哈顿距离（Manhattan distance）</strong>，即<br>$$<br>d_{ij} = (\sum\limits_{k=1}^{m}|x_{ki} - x_{kj}|)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.4)<br>$$<br>当$p = \infty$时称为<strong>切比雪夫距离（Chebyshev distance）</strong>，取各个坐标数值差的绝对值的最大值，即<br>$$<br>d_{ij} = \max\limits_{k}|x_{ki} - x_{kj}|\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.5)<br>$$</p>
<hr>
<ol start="2">
<li><strong>马哈拉诺比斯距离</strong></li>
</ol>
<p><strong>马哈拉诺比斯距离（Mahalanobis distance）</strong>，简称马氏距离，也是另一种常用的相似度，考虑各个分量（特征）之间的相关性并与各个分量尺度无关。马哈拉诺比斯距离越大相似度越小，距离越小相似度越大。</p>
<p><strong>定义 14.2</strong>   给定一个样本集合$X,X= [x{ij}]{m \times n}$，其中协方差矩阵记作$S$。样本$x_i$与样本$x_j$之间的马哈拉诺比斯距离$d_{ij}$定义为<br>$$<br>d_{ij} = [(x_i-x_j)^T S^{-1}(x_i-x_j)]^{\frac{1}{2}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.6)<br>$$<br>其中<br>$$<br>x_i=(x_{1i},x_{2i},…,x_{mi})^T,x_j=(x_{1j},x_{2j},…,x_{mj})^T\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.7)<br>$$<br>当$S$为单位矩阵时，即样本数据的各个分量互相独立且各个分量的方差为1时，由式$(14.6)$知马氏距离就是欧氏距离，所以马氏距离是欧式距离的推广。</p>
<ol start="3">
<li><strong>相关系数</strong></li>
</ol>
<p>样本之间的相似度也可以用<strong>相关系数（correlation coefficient）</strong>来表示。相关系数的绝对值越接近于1，表示样本越相似；越接近于0，表示样本越不相似。</p>
<p><strong>定义 14.3</strong>  样本$x_i$与样本$x_j$之间的相关系数定义为<br>$$<br>r_{ij} = \frac{\sum\limits_{k=1}^{m}(x_{ki} - \overline x_i)(x_{kj} - \overline x_j)}{[\sum\limits_{k=1}^{m}(x_{ki} - \overline x_i)^2 \sum\limits_{k=1}^{m}(x_{kj} - \overline x_j)^2]^{\frac{1}{2}}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.8)<br>$$</p>
<hr>
<p>其中<br>$$<br>\overline x_i = \frac{1}{m}\sum\limits_{k=1}^{m}x_{ki}, \ \    \overline x_j = \frac{1}{m}\sum\limits_{k=1}^{m}x_{kj}<br>$$</p>
<ol start="4">
<li><strong>夹角余弦</strong></li>
</ol>
<p>样本之间的相似度也可以用<strong>夹角余弦（cosine）</strong>来表示。夹角余弦越接近于1，表示样本越相似；越接近于0，表示样本越不相似。</p>
<p><strong>定义 14.4</strong>  样本$x_i$与样本$x_j$之间的夹角余弦定义为<br>$$<br>s_{ij} = \frac{\sum\limits_{k=1}^{m}x_{ki}x_{kj}}{[\sum\limits_{k=1}^{m}x_{ki}^2\sum\limits_{k=1}^{m}x_{kj} ^2]^{\frac{1}{2}}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.9)<br>$$</p>
<p>用距离度量相似度时，距离越小样本越相似；用相关系数时，相关系数越大样本越相似。注意不同相似度度量得到的结果并不一定一致。</p>
<h2 id="14-1-2-类或簇"><a href="#14-1-2-类或簇" class="headerlink" title="14.1.2 类或簇"></a>14.1.2 类或簇</h2><p>通过聚类得到的类或簇，本质是样本的子集。如果一个聚类方法假定一个样本只能属于一个类，或类的交集为空集，那么该方法称为<strong>硬聚类（hard clustering）</strong>方法。否则，如果一个样本可以属于多个类，或者类的交集不为空集，那么该方法称为<strong>软聚类（soft clustering）</strong>方法。</p>
<p>用$G$表示类或<strong>簇（cluster）</strong>，用$x_i,x_j$表示类中的样本，用$n_G$表示$G$中样本的个数，用$d_{ij}$表示样本$x_i$与样本$x_j$之间的距离。</p>
<hr>
<p><strong>定义 14.5</strong>  设$T$为给定的正数，若集合$G$中任意两个样本$x_i,x_j$，有<br>$$<br>d_{ij} \leq T<br>$$<br>则称$G$为一个类或簇。</p>
<hr>
<p><strong>定义 14.6</strong>  设$T$为给定的正数，若对集合$G$的任意样本$x_i$，一定存在$G$中的另一个样本$x_j$，使得<br>$$<br>d_{ij} \leq T<br>$$<br>则称$G$为一个类或簇。</p>
<hr>
<p><strong>定义 14.7</strong>  设$T$为给定的正数，若对集合$G$的任意一个样本$x_i$，$G$中的另一个样本$x_j$满足<br>$$<br>\frac{1}{n_G-1} \sum\limits_{x_j \in G} d_{ij} \leq T<br>$$<br>其中$n_G$为$G$中样本的个数，则称$G$为一个类或簇。</p>
<hr>
<p><strong>定义 14.8</strong>  设$T$和$V$为给定的两个正数，如果集合$G$中任意两个样本$x_i,x_j$的距离$d_{ij}$满足<br>$$<br>\frac{1}{n_G(n_G-1)} \sum\limits_{x_i \in G}\sum\limits_{x_j \in G} d_{ij} \leq T<br>$$</p>
<p>$$<br>d_{ij} \leq V<br>$$</p>
<p>则称$G$为一个类或簇。</p>
<hr>
<p>类的特征可以通过不同角度来刻画，常用的特征有下面三种：</p>
<p>（1）类的均值$\overline x_G$，又称为类的中心<br>$$<br>\overline x_G =\frac{1}{n_G} \sum\limits_{a = 1}^{n_G} x_a\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.10)<br>$$<br>（2）类的<strong>直径（diameter）</strong>$D_G$</p>
<p>类的直径$D_G$是类中任意两个样本之间的最大距离，即<br>$$<br>D_G = \max\limits_{x_i,x_j \in G}d_{ij}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.11)<br>$$<br>（3）类的样本<strong>散布矩阵（scatter matrix）</strong>$A_G$与样本<strong>协方差矩阵（covariance matrix）</strong>$S_G$</p>
<p>类样本散布矩阵$A_G$为<br>$$<br>A_G =\sum\limits_{i = 1}^{n_G}(x_i-\bar{x}_G)(x_i-\bar{x}_G)^T\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.12)<br>$$<br>样本协方差矩阵$S_G$为<br>$$<br>S_G =\frac{1}{n_G - 1}A_G<br>$$</p>
<p>$$<br>=\frac{1}{n_G-1}\sum\limits_{i = 1}^{n_G}(x_i-\bar{x}_G)(x_i-\bar{x}_G)^T\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.13)<br>$$</p>
<h2 id="14-1-3-类与类之间的距离"><a href="#14-1-3-类与类之间的距离" class="headerlink" title="14.1.3 类与类之间的距离"></a>14.1.3 类与类之间的距离</h2><p>类$G_p$与类$G_q$之间的距离$D(p,q)$，也称为连接（linkage）。类与类之间的距离也有多种定义。</p>
<p>设类$G_p$包含$n_p$个样本，$G_q$包含$n_q$个样本，分别用$\bar{x}_p$和$\bar{x}_q$表示$G_p$和$G_q$的均值，即类的中心。</p>
<hr>
<p><strong>（1）最短距离或单链接（single linkage）</strong></p>
<p>定义类$G_p$的样本与类$G_q$的样本之间的最短距离为两类之间的距离<br>$$<br>D_{pq} = \min{d_{ij}|x_i \in G_p,x_j \in G_q}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.14)<br>$$<br><strong>（2）最长距离或完全连接（complete linkage）</strong></p>
<p>定义类$G_p$的样本与类$G_q$的样本之间的最长距离为两类之间的距离<br>$$<br>D_{pq} = \max{d_{ij}|x_i \in G_p,x_j \in G_q}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.15)<br>$$<br><strong>（3）中心距离</strong></p>
<p>定义类$G_p$与类$G_q$的中心之间的距离为两类之间的距离<br>$$<br>D_{pq} = d_{\overline x_p\overline x_q}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.16)<br>$$<br><strong>（4）平均距离</strong></p>
<p>定义类$G_p$与类$G_q$任意两个样本之间距离的平均值为两类之间的距离<br>$$<br>D_{pq} = \frac{1}{n_p n_q}\sum\limits_{x_i \in G_p}\sum\limits_{x_j \in G_q}d_{ij}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.17)<br>$$</p>
<h1 id="14-2-层次聚类"><a href="#14-2-层次聚类" class="headerlink" title="14.2 层次聚类"></a>14.2 层次聚类</h1><p>层次聚类假设类别之间存在层次结构，将样本聚到层次化的类中。</p>
<p>层次聚类又有<strong>聚合（agglomerative）</strong>或<strong>自下而上（bottom-up）聚类</strong>、<strong>分裂（divisive）</strong>或<strong>自上而下（top-down）聚类</strong>两种方法。</p>
<ul>
<li><p>聚合聚类开始将每个样本各自分到一个类；之后将相邻距离最近的两类合并，建立一个新的类，重复此操作直到满足停止条件；得到层次化的类别。</p>
</li>
<li><p>分裂聚类开始将所有样本分到一个类；之后将已有类中相距最远的样本分到两个新的类，重复此操作直到满足停止条件；得到层次化的类别。</p>
</li>
</ul>
<p>聚合聚类的具体过程如下：对于给定的样本集合，开始将每个样本分到一个类；然后按照一定规则，例如类间距离最小，将最满足规则条件的两个类进行合并；如此反复进行，每次减少一个类，直到满足停止条件，如所有样本聚为一类。</p>
<p>由此可知，聚合聚类需要预先确定下面三个要素：</p>
<ul>
<li>距离或相似度</li>
<li>合并规则</li>
<li>停止条件</li>
</ul>
<p><strong>算法 14.1（聚合聚类算法）</strong></p>
<p><strong>输入</strong>：$n$个样本组成的样本集合及样本之间的距离；</p>
<p><strong>输出</strong>：对样本集合的一个层次化聚类。</p>
<p>（1）计算$n$个样本两两之间的欧式距离${d_{ij}}$，记作矩阵$D = [d_{ij}]_{n \times n}$。</p>
<p>（2）构造$n$个类，每个类只包含一个样本。</p>
<p>（3）合并类间距离最小的两个类，其中最短距离为类间距离，构建一个新类。</p>
<p>（4）计算新类与当前各类的距离。若类的个数为1，终止计算，否则回到步（3）。</p>
<h1 id="14-3-k-均值聚类"><a href="#14-3-k-均值聚类" class="headerlink" title="14.3 $k$均值聚类"></a>14.3 $k$均值聚类</h1><p>$k$均值聚类是基于样本集合划分的聚类算法。$k$均值聚类将样本集合划分为$k$个子集，构成$k$个类，将$n$个样本划分到$k$个类中，每个样本到其所属类的中心的距离最小。</p>
<h2 id="14-3-1-模型"><a href="#14-3-1-模型" class="headerlink" title="14.3.1 模型"></a>14.3.1 模型</h2><p>给定$n$个样本的集合$X = {x_1,x_2,…,x_n}$，每个样本由一个特征向量表示，特征向量的维数是$m$。$k$均值聚类的目标是将$n$个样本分到$k$个不同的类或簇中，这里假设$k &lt; n$。$k$个类$G_1,G_2,…,G_k$形成对样本集合$X$的划分，其中$G_i \bigcap G_j = \emptyset,\bigcup\limits_{i=1}^{k}G_i = X$。用$C$表示划分，一个划分对应着一个聚类结果。</p>
<p>划分$C$是一个多对一的函数。如果把每个样本用一个正数$i\in{1,2,…,n}$表示，每个类也用一个正数$l \in {1,2,…,k}$表示，那么划分或者聚类可以用函数$l=C(i)$表示。所以$k$均值聚类的模型是一个从样本到类的函数。</p>
<h2 id="14-3-2-策略"><a href="#14-3-2-策略" class="headerlink" title="14.3.2 策略"></a>14.3.2 策略</h2><p>$k$均值聚类归结为样本集合$X$的划分，或者从样本到类的函数的选择问题。$k$均值聚类的策略是通过损失函数的最小化选取最优的划分或函数$C^*$。</p>
<p>首先，采用<strong>欧氏距离平方（squared Euclidean distance）</strong>作为样本之间的距离$d(x_i,x_j)$<br>$$<br>d(x_i,x_j) = \sum\limits_{k=1}^{m}(x_{ki} - x_{kj})^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.18)<br>$$<br>然后，定义样本与其所属类的中心之间的距离的总和为损失函数，即<br>$$<br>W(C) =\sum\limits_{l=1}^{k}\sum\limits_{C(i) =l}||x_i-\bar{x}_l||^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.19)<br>$$<br>函数$W(C)$也称为能量，表示相同类中的样本相似的程度。</p>
<p>$k$均值聚类就是求解最优化问题：<br>$$<br>C^* = \arg\min\limits_{C}W(C)<br>$$</p>
<p>$$<br>=\arg\min\limits_{C}\sum\limits_{l=1}^{k}\sum\limits_{C(i) =l}||x_i-\bar{x}_l||^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (14.20)<br>$$</p>
<p>相似的样本被聚到同类时，损失函数值最小，这个目标函数的优化能达到聚类的目的。</p>
<p>$k$均值聚类的最优解求解问题是NP困难问题。现实中采用迭代的方法求解。</p>
<h2 id="14-3-3-算法"><a href="#14-3-3-算法" class="headerlink" title="14.3.3 算法"></a>14.3.3 算法</h2><p>$k$均值聚类的算法是一个迭代的过程，每次迭代包括两个步骤。</p>
<ul>
<li>选择$k$个类的中心，将样本逐个指派到与其最近的中心的类中，得到一个聚类结果；</li>
<li>更新每个类的样本的均值，作为类的新的中心；</li>
</ul>
<p>重复以上步骤，直到收敛为止。</p>
<p><strong>算法 14.2（</strong>$k$<strong>均值聚类算法）</strong></p>
<p><strong>输入</strong>：$n$个样本的集合$X$；</p>
<p><strong>输出</strong>：样本集合的聚类$C^\cdot$。</p>
<p>​    （1）初始化。令$t=0$，随机选择$k$个样本点作为初始聚类中心$m^{(0)} = (m_1^{(0)},..,m_l^{(0)},…,m_k^{(0)})$。</p>
<p>​    （2）对样本进行聚类。对固定的类中心$m^{(t)} = (m_1^{(t)},..,m_l^{(t)},…,m_k^{(t)})$，其中$m_l^{(t)}$为类$G_t$的中心，计算每个样本到类中心的距离，将每个样本指派到与其最近的中心的类中，构成聚类结果$C^{(t)}$。</p>
<p>​    （3）计算新的类中心。对聚类结果$C^{(t)}$，计算当前各个类中的样本的均值，作为新的类中心$m^{(t+1)} = (m_1^{(t+1)},..,m_l^{(t+1)},…,m_k^{(t+1)})$。</p>
<p>​    （4）如果迭代收敛或符合停止条件，输出$C^* = C^{(t)}$。否则，令$t= t+1$，返回步（2）。</p>
<h2 id="14-3-4-算法特性"><a href="#14-3-4-算法特性" class="headerlink" title="14.3.4 算法特性"></a>14.3.4 算法特性</h2><ol>
<li><strong>总体特点</strong></li>
</ol>
<p>$k$均值聚类有以下特点：</p>
<ul>
<li>基本划分的聚类方法；</li>
<li>类别数$k$事先指定；</li>
<li>以欧式距离平方表示样本之间的距离，以中心或样本的均值表示类别；</li>
<li>以样本和其所属类的中心之间的距离的总和为最优化的目标函数；</li>
<li>得到的类别是平坦的、非层次化的；</li>
<li>算法是迭代算法，不能保证得到全局最优。</li>
</ul>
<ol start="2">
<li><strong>收敛性</strong></li>
</ol>
<p>$k$均值聚类属于启发式方法，不能保证收敛到全局最优，初始中心的选择会直接影响聚类结果。</p>
<ol start="3">
<li><strong>初始类的选择</strong></li>
</ol>
<p>选择不同的初始中心，会得到不同的聚类结果。</p>
<p>初始中心的选择，比如可以用层次聚类对样本进行聚类，得到$k$个类时停止。然后从每个类中选取一个与中心距离最近的点。</p>
<ol start="4">
<li><strong>类别数$k$的选择</strong></li>
</ol>
<p>$k$均值聚类中的类别数$k$值需要预先指定，而在实际应用中最优的$k$值是不知道的。解决这个问题的一个方法是尝试用不同的$k$值聚类，检验各自得到聚类结果的质量，推测最优的$k$值。聚类结果的质量可以用类的平均直径来衡量。一般地，类别数变小时，平均直径会增加；类别数变大超过某个值以后，平均直径会不变；而这个值正是最优的$k$值。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/21/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/21/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" class="post-title-link" itemprop="url">第十三章 无监督学习概论</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-21 20:01:55 / 修改时间：20:01:33" itemprop="dateCreated datePublished" datetime="2021-06-21T20:01:55+08:00">2021-06-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/21/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/21/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="13-1-无监督学习基本原理"><a href="#13-1-无监督学习基本原理" class="headerlink" title="13.1 无监督学习基本原理"></a>13.1 无监督学习基本原理</h1><p>无监督学习是从无标注的数据中学习数据的统计规律或者说内在结构的机器学习，主要包括聚类、降维、概率估计。</p>
<p>假设训练数据集由$N$个样本组成，每个样本是一个$M$维向量。训练数据可以由一个矩阵表示，每一行对应一个特征，每一列对应一个样本。</p>
<p>无监督学习的基本想法是对给定数据（矩阵数据）进行某种“压缩”，从而找到数据的潜在结构。</p>
<ul>
<li>可以考虑发掘数据的纵向结构，把相似的样本聚到同类，即对数据进行聚类。</li>
<li>还可以考虑发掘数据的横向结构，把高维空间的向量转换为低维空间的向量，即对数据进行降维。</li>
<li>也可以同时考虑发掘数据的纵向与横向结构，假设数据由含有隐式结构的概率模型生成得到，从数据中学习该概率模型。</li>
</ul>
<h1 id="13-2-基本问题"><a href="#13-2-基本问题" class="headerlink" title="13.2 基本问题"></a>13.2 基本问题</h1><ol>
<li><strong>聚类</strong></li>
</ol>
<p><strong>聚类（clustering）</strong>是将样本集合中相似的样本（实例）分配到相同的类，不相似的样本分配到不同的类。聚类时，样本通常是欧式空间中的向量，类别不是事先给定，而是从数据中自动发现，但类别的个数通常是事先给定。样本之间的相似度或距离由应用决定。</p>
<ul>
<li>如果一个样本只能属于一个类，则称为<strong>硬聚类（hard clustering）</strong>；</li>
<li>如果一个样本可以属于多个类，则称为<strong>软聚类（soft clustering）</strong>。</li>
</ul>
<p>假设输入空间是欧式空间$X \subseteq R^d$，输出空间是类别集合$Z = {1,2,…,k}$。聚类的模型是函数$z = g_\theta(x)$或者条件概率分布$P_\theta(z|x)$，其中$x \in X$是样本的向量，$z \in Z$是样本的类别，$\theta$是参数。前者的函数是硬聚类模型，后者的条件概率分布是软聚类模型。</p>
<p>聚类的过程就是学习聚类模型的过程：    </p>
<ul>
<li>硬聚类时，每一个样本属于某一类$z_i = g_\theta(x_i),i=1,2,…,N$；</li>
<li>软聚类时，每一个样本依概率属于每一个类$P_\theta(z_i|x_i),i=1,2,…,N$。</li>
</ul>
<hr>
<ol start="2">
<li><strong>降维</strong></li>
</ol>
<p><strong>降维（dimensionality reduction）</strong>是将训练数据中的样本（实例）从高维空间转换到低维空间。</p>
<p>假设样本原本存在于低维空间，或者近似地存在于低维空间，通过降维则可以更好地表示样本数据的结构，即更好地表示样本之间的关系。</p>
<p>高维空间通常是高维的欧式空间，而低维空间是低维的欧式空间或者<strong>流形（manifold）</strong>。</p>
<p>低维空间不是事先给定，而是从数据中自动发现，其维数通常是事先给定的。</p>
<p>从高维到低维的降维中，要保证样本中的信息损失最小。</p>
<p>降维有线性的降维和非线性的降维。</p>
<p>降维的过程就是学习降维模型的过程。降维时，每一个样本从高维向量转换为低维向量$z_i = g_\theta(x_i),i=1,2,…,N$。</p>
<hr>
<ol start="3">
<li><strong>概率模型估计</strong></li>
</ol>
<p><strong>概率模型估计（probability model estimation）</strong>，简称概率估计，假设训练数据由一个概率模型生成，由训练数据学习概率模型的结构和参数。</p>
<p>概率模型的结构类型，或者说概率模型的集合事先给定，而模型的具体结构与参数从数据中自动学习。</p>
<p>学习的目标是找到最有可能生成数据的结构和参数。</p>
<p>概率模型包括混合模型、概率图模型等。概率图模型又包括有向图模型和无向图模型。</p>
<p>概率模型表示为条件概率分布$P_\theta(x|z)$，其中随机变量$x$表示观测数据，可以是连续变量也可以是离散变量；随机变量$z$表示隐式结构，是离散变量；随机变量$\theta$表示参数。</p>
<p>模型是混合模型时，$z$表示成分的个数；</p>
<p>模型是概率图模型时，$z$表示图的结构。</p>
<p>概率模型的一种特殊情况是隐式结构不存在，即满足$P_\theta(x|z) = P_\theta(x)$。这时条件概率分布估计变成概率分布估计，只要估计分布$P_\theta(x)$的参数即可。</p>
<h1 id="13-3-机器学习三要素"><a href="#13-3-机器学习三要素" class="headerlink" title="13.3 机器学习三要素"></a>13.3 机器学习三要素</h1><p>同监督学习一样，无监督学习也有三要素：模型、策略、算法。</p>
<ul>
<li><p>模型就是函数$z=g_\theta(x)$，条件概率分布$P_\theta(x|z)$，或条件概率分布$P_\theta(z|x)$，在聚类、降维、概率模型估计中拥有不同的形式。</p>
</li>
<li><p>策略在不同的问题中有不同的形式，但都可以表示为目标函数的优化。</p>
</li>
<li><p>算法通常是迭代算法，通过迭代达到目标函数的最优化，比如，梯度下降法。</p>
</li>
</ul>
<h1 id="13-4-无监督学习方法"><a href="#13-4-无监督学习方法" class="headerlink" title="13.4 无监督学习方法"></a>13.4 无监督学习方法</h1><ol>
<li>聚类</li>
</ol>
<p>聚类主要用于数据分析，也可以用于监督学习的前处理。</p>
<ol start="2">
<li>降维</li>
</ol>
<p>降维主要用于数据分析，也可以用于监督学习的前处理。降维可以帮助发现高维数据中的统计规律。数据是连续变量表示的。</p>
<ol start="3">
<li>话题分析</li>
</ol>
<p>略。</p>
<ol start="4">
<li>图分析</li>
</ol>
<p><strong>图分析（graph analytics）</strong>的目的是发掘隐藏在图中的统计规律或潜在结构。</p>
<p><strong>链接分析（link analysis）</strong>是图分析的一种，包括PageRank算法，主要是发现有向图中的重要节点。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/18/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B9%9D%E7%AB%A0-EM%E7%AE%97%E6%B3%95%E5%8F%8A%E6%8E%A8%E5%B9%BF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/18/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B9%9D%E7%AB%A0-EM%E7%AE%97%E6%B3%95%E5%8F%8A%E6%8E%A8%E5%B9%BF/" class="post-title-link" itemprop="url">第九章 EM算法及推广</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-18 16:36:10 / 修改时间：16:35:44" itemprop="dateCreated datePublished" datetime="2021-06-18T16:36:10+08:00">2021-06-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/18/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B9%9D%E7%AB%A0-EM%E7%AE%97%E6%B3%95%E5%8F%8A%E6%8E%A8%E5%B9%BF/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/18/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B9%9D%E7%AB%A0-EM%E7%AE%97%E6%B3%95%E5%8F%8A%E6%8E%A8%E5%B9%BF/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>EM算法是一种迭代算法，用于含有<strong>隐变量（hidden variable）</strong>的概率模型参数的极大似然估计，或极大后验概率估计。</p>
<p>EM算法的每次迭代由两步组成：    </p>
<ul>
<li>E步，求<strong>期望（expectation）</strong>；</li>
<li>M步，求<strong>极大（maximization）</strong>。</li>
</ul>
<p>所以这一算法称为<strong>期望极大算法（expectation maximization algorithm）</strong>，简称EM算法。</p>
<h1 id="9-1-EM算法的引入"><a href="#9-1-EM算法的引入" class="headerlink" title="9.1 EM算法的引入"></a>9.1 EM算法的引入</h1><p>概率模型有时既含有<strong>观测变量（observable variable）</strong>，又含有隐变量或<strong>潜在变量（latent variable）</strong>。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计发，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。</p>
<h2 id="9-1-1-EM算法"><a href="#9-1-1-EM算法" class="headerlink" title="9.1.1 EM算法"></a>9.1.1 EM算法</h2><p>EM算法与初值的选择有关，选择不同的初值可能得到不同的参数估计值。</p>
<p>一般地，用$Y$表示观测随机变量的数据，$Z$表示隐随机变量的数据。$Y$和$Z$连在一起称为<strong>完全数据（complete-data）</strong>，观测数据$Y$又称为<strong>不完全数据（incomplete-data）</strong>。假设给定观测数据$Y$，其概率分布是$P(Y|\theta)$，其中$\theta$是需要估计的模型参数，那么不完全数据$Y$的似然函数是$P(Y|\theta)$，对数似然函数$L(\theta) = \log P(Y|\theta)$；假设$Y$和$Z$的联合概率分布是$P(Y,Z|\theta)$，那么完全数据的对数似然函数是$\log P(Y,Z|\theta)$。</p>
<p>EM算法通过迭代求$L(\theta) = \log P(Y|\theta)$的极大似然估计。每次迭代都包含两步：E步，求期望；M步，求极大化。</p>
<p><strong>算法 9.1（EM算法）</strong></p>
<p>输入：观测变量数据$Y$，隐变量数据$Z$，联合分布$P(Y,Z|\theta)$，条件分布$P(Z|Y,\theta)$；</p>
<p><strong>输出</strong>：模型参数$\theta$。</p>
<p>​    （1）选择参数的初值$\theta^{(0)}$，开始迭代；</p>
<p>​    （2）E步：记$\theta^{(i)}$为第$i$次迭代参数$\theta$的估计值，在第$i+1$次迭代的E步，计算<br>$$<br>Q(\theta,\theta^{(i)}) = E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]<br>$$</p>
<p>$$<br>\ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ =\sum\limits_{Z} \log P(Y,Z|\theta)P(Z|Y,\theta)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.9)<br>$$</p>
<p>这里，$P(Z|Y,\theta)$是在给定观测数据$Y$和当前的参数估计$\theta^{(i)}$下隐变量数据$Z$的条件概率分布；</p>
<p>​    （3）M步：求使$Q(\theta,\theta^{(i)})$极大化的$\theta$，确定第$i+1$次迭代的参数的估计值$\theta^{(i+1)}$<br>$$<br>\theta^{(i+1)} = \arg\max\limits_{\theta}Q(\theta,\theta^{(i)})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.10)<br>$$<br>​    （4）重复第（2）步和第（3）步，直到收敛。</p>
<p>式$(9.9)$的函数$Q(\theta,\theta^{(i)})$是EM算法的核心，称为$Q$<strong>函数（Q function）</strong>。</p>
<p><strong>定义 9.1（</strong>$Q$<strong>函数）</strong>  完全数据的对数似然函数$\log P(Y,Z|\theta)$关于在给定观测数据$Y$和当前参数$\theta^{(i)}$下对未观测数据$Z$的条件概率分布$P(Z|Y,\theta^{(i)})$的期望称为$Q$函数，即<br>$$<br>Q(\theta,\theta^{(i)}) = E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.11)<br>$$<br>关于EM算法的几点说明：</p>
<ul>
<li><p><strong>步骤（1）</strong> 参数的初值可以任意选择，但需要注意EM算法对初值是敏感的。</p>
</li>
<li><p><strong>步骤（2）</strong> E步求$Q(\theta,\theta^{(i)})$。$Q$函数式中$Z$是未观测数据，$Y$是观测数据。注意，$Q(\theta,\theta^{(i)})$的第一个变元表示要极大化的参数，第二个变元表示参数的当前估计值。每次迭代实际在求$Q$函数及其极大。</p>
</li>
<li><p><strong>步骤（3）</strong> M步求$Q(\theta,\theta^{(i)})$的极大化，得到$\theta^{(i+1)}$，完成一次迭代$\theta^{(i)} \longrightarrow \theta^{(i+1)}$。</p>
</li>
<li><p><strong>步骤（4）</strong>给出停止迭代的条件，一般是对较小的正数$\epsilon_1,\epsilon_2$，若满足</p>
</li>
</ul>
<p>$$<br>||\theta^{(i+1)} - \theta^{(i)}|| &lt; \epsilon_1\ \ \  或 \ \ \ ||Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})|| &lt; \epsilon_2<br>$$</p>
<p>则停止迭代。</p>
<h2 id="9-1-2-EM算法的导出"><a href="#9-1-2-EM算法的导出" class="headerlink" title="9.1.2 EM算法的导出"></a>9.1.2 EM算法的导出</h2><p>对一个含有隐变量的概率模型，目标是极大化观测数据（不完全数据）$Y$关于参数$\theta$的对数似然函数，即极大化<br>$$<br>L(\theta) = \log P(Y|\theta) = \log\sum\limits_{Z}P(Y,Z|\theta)<br>$$</p>
<p>$$<br>\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = \log(\sum\limits_{Z} P(Y|Z,\theta)P(Z|\theta))\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.12)<br>$$</p>
<p>这一极大化的主要困难是式$(9.12)$中有未观测数据并有包含和（或积分）的对数。</p>
<p>事实上，EM算法是通过迭代逐步近似极大化$L(\theta)$的。假设在第$i$次迭代后$\theta$的估计值是$\theta^{(i)}$。我们希望新估计值$\theta$能使$L(\theta)$增加，即$L(\theta) &gt; L(\theta^{(i)})$，并逐步达到极大值。为此，考虑两者的差：<br>$$<br>L(\theta) - L(\theta^{(i)}) = \log(\sum\limits_{Z} P(Y|Z,\theta)P(Z|\theta)) - \log P(Y|\theta^{(i)})<br>$$<br>利用$Jensen$不等式（Jensen inequality）得到其下界：<br>$$<br>L(\theta) - L(\theta^{(i)}) = \log(\sum\limits_{Z} P(Y|Z,\theta)P(Z|\theta)) - \log P(Y|\theta^{(i)})<br>$$</p>
<p>$$<br>\geq \sum\limits_{Z} P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}- \log P(Y|\theta^{(i)})<br>$$</p>
<p>$$<br>= \sum\limits_{Z} P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}<br>$$</p>
<p>令<br>$$<br>B(\theta,\theta^{(i)}) \hat= L(\theta^{(i)}) +\sum\limits_{Z} P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.13)<br>$$<br>则<br>$$<br>L(\theta) \geq B(\theta,\theta^{(i)})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.14)<br>$$<br>即函数$B(\theta,\theta^{(i)})$是$L(\theta)$的一个下界，而且由式$(9.13)$可知，<br>$$<br>L(\theta^{(i)}) = B(\theta^{(i)},\theta^{(i)})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.15)<br>$$<br>因此，任何可以使$B(\theta,\theta^{(i)})$增大的$\theta$，也可以使$L(\theta)$增大。为了使$L(\theta)$有尽可能大的增长，选择$\theta^{(i+1)}$使$B(\theta,\theta^{(i)})$达到极大，即<br>$$<br>\theta^{(i+1)} = \arg\max\limits_{\theta}B(\theta,\theta^{(i)})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.16)<br>$$<br>现在求$\theta^{(i+1)}$的表达式。省去对$\theta$的极大化而言是常数的项，由式$(9.16)$、式$(9.13)$即式$(9.10)$，有<br>$$<br>\theta^{(i+1)} = \arg\max\limits_{\theta}(L(\theta^{(i)}) +\sum\limits_{Z} P(Z|Y,\theta^{(i)})\log \frac{c}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})})<br>$$</p>
<p>$$<br>=\arg\max\limits_{\theta}(\sum\limits_{Z} P(Z|Y,\theta^{(i)})\log P(Y|Z,\theta)P(Z|\theta))<br>$$</p>
<p>$$<br>=\arg\max\limits_{\theta} (\sum\limits_{Z} P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta))<br>$$</p>
<p>$$<br>= \arg\max\limits_{\theta}Q(\theta,\theta^{(i)})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.17)<br>$$</p>
<p>式$(9.17)$等价于EM算法的一次迭代，即求$Q$函数及其极大化。EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法。</p>
<p>EM算法不能保证找到全局最优值。</p>
<h2 id="9-1-3-EM算法在无监督学习中的应用"><a href="#9-1-3-EM算法在无监督学习中的应用" class="headerlink" title="9.1.3 EM算法在无监督学习中的应用"></a>9.1.3 EM算法在无监督学习中的应用</h2><p>EM算法可以用于生成模型的无监督学习。生成模型由联合概率分布$P(X,Y)$表示，可以认为无监督学习训练数据是联合概率分布产生的数据。$X$为观测数据，$Y$为未观测数据。</p>
<h1 id="9-2-EM算法的收敛性"><a href="#9-2-EM算法的收敛性" class="headerlink" title="9.2 EM算法的收敛性"></a>9.2 EM算法的收敛性</h1><p><strong>定理 9.1</strong>  设$P(Y|\theta)$为观察数据的似然函数，$\theta^{(i)}(i=1,2,…)$为EM算法得到的参数估计序列，$P(Y|\theta^{(i)})(i=1,2,…)$为对应的似然函数序列，则$P(Y|\theta^{(i)})$是单调递增的，即<br>$$<br>P(Y|\theta^{(i+1)}) \geq P(Y|\theta^{(i)})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.18)<br>$$<br><strong>定理 9.2</strong>  设$L(\theta) = \log P(Y|\theta)$为观测数据的对数似然函数，$\theta^{(i)}(i=1,2,…)$为EM算法得到的参数估计序列，$L(\theta^{(i)})(i=1,2,…)$为对应的对数似然函数序列。    </p>
<p>​    （1）如果$P(Y|\theta)$有上界，则$L(\theta^{(i)}) = \log P(Y|\theta^{(i)})$收敛到某一值$L^*$；</p>
<p>​    （2）在函数$Q(\theta,\theta^{‘})$与$L(\theta)$满足一定条件下，由EM算法得到的参数估计序列$\theta^{(i)}$的收敛值$\theta^{*}$是$L(\theta)$的稳定点。</p>
<p>EM算法的收敛性包含关于对数似然函数序列$L(\theta^{(i)})$的收敛性和关于参数估计序列$\theta^{(i)}$的收敛性两层意思，前者并不蕴含后者。此外，定理只能保证参数估计序列收敛到对数似然函数序列的稳定点，不能保证收敛到极大值点。所以在应用中，初值的选择变得非常重要，常用的办法是选取几个不同的初值进行迭代，然后对得到的各个估计值加以比较，从中选择最好的。</p>
<h1 id="9-3-EM算法在高斯混合模型学习中的应用"><a href="#9-3-EM算法在高斯混合模型学习中的应用" class="headerlink" title="9.3 EM算法在高斯混合模型学习中的应用"></a>9.3 EM算法在高斯混合模型学习中的应用</h1><p>EM算法是学习<strong>高斯混合模型（Gaussian mixture model）</strong>的有效方法。</p>
<h2 id="9-3-1-高斯混合模型"><a href="#9-3-1-高斯混合模型" class="headerlink" title="9.3.1 高斯混合模型"></a>9.3.1 高斯混合模型</h2><p><strong>定义 9.2（高斯混合模型）</strong>   高斯混合模型是指具有如下形式的概率分布模型：<br>$$<br>P(y|\theta) = \sum\limits_{k=1}^{K}\alpha_{k}\phi(y|\theta_k)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.24)<br>$$<br>其中，$\alpha_k$是系数，$\alpha_k \geq 0$，$\sum\limits_{k=1}^{K}\alpha_{k} = 1$；$\phi(y|\theta_k)$是高斯分布密度，$\theta_k = (\mu_k,\sigma_k^2)$，<br>$$<br>\phi(y|\theta_k) = \frac{1}{\sqrt{2\pi}\sigma_k} \exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.25)<br>$$<br>称为第$k$个分模型。</p>
<p>一般混合模型可以由任意概率分布密度代替式$(9.25)$中的高斯分布密度。</p>
<h2 id="9-3-2-高斯混合模型参数估计的EM算法"><a href="#9-3-2-高斯混合模型参数估计的EM算法" class="headerlink" title="9.3.2 高斯混合模型参数估计的EM算法"></a>9.3.2 高斯混合模型参数估计的EM算法</h2><p><strong>算法 9.2（高斯混合模型参数估计的EM算法）</strong></p>
<p><strong>输入</strong>：观测数据$y_1,y_2,…,y_N$，高斯混合模型；</p>
<p><strong>输出</strong>：高斯混合模型参数。</p>
<p>​    （1）取参数的初始值开始迭代；</p>
<p>​    （2）E步：依据当前模型参数，计算分模型$k$对观测数据$y_j$的响应度<br>$$<br>\hat \gamma_{jk} = \frac{\alpha_{k}\phi(y_j|\theta_k)}{\sum\limits_{k=1}^{K}\alpha_{k}\phi(y_j|\theta_k)},\ \ \ j=1,2,..,N;\ \ \ k=1,2,…,K<br>$$<br>​    （3）M步：计算新一轮迭代的模型参数<br>$$<br>\hat \mu_k = \frac{\sum\limits_{j=1}^{N}\hat \gamma_{jk}y_j}{\sum\limits_{j=1}^{N}\hat \gamma_{jk}},\ \ \ k=1,2,..,K<br>$$</p>
<p>$$<br>\hat \sigma_k^2 = \frac{\sum\limits_{j=1}^{N}\hat \gamma_{jk}(y_i - \mu_k)^2}{\sum\limits_{j=1}^{N}\hat \gamma_{jk}},\ \ \ k=1,2,..,K<br>$$</p>
<p>$$<br>\hat \alpha_k = \frac{\sum\limits_{j=1}^{N}\hat \gamma_{jk}}{N},\ \ \ k=1,2,..,K<br>$$</p>
<p>​    （4）重复第（2）步和第（3）步，直到收敛。</p>
<h1 id="9-4-EM算法的推广"><a href="#9-4-EM算法的推广" class="headerlink" title="9.4 EM算法的推广"></a>9.4 EM算法的推广</h1><p>EM算法还可以解释为$F$<strong>函数（F function）</strong>的<strong>极大-极大算法（maximization-maximization algorithm）</strong>，基于这个解释有若干变形与推广，如<strong>广义期望极大（generalized expectation maximization，GEM）</strong>算法。</p>
<h2 id="9-4-1-F-函数的极大-极大算法"><a href="#9-4-1-F-函数的极大-极大算法" class="headerlink" title="9.4.1 $F$函数的极大-极大算法"></a>9.4.1 $F$函数的极大-极大算法</h2><p><strong>定义 9.3（</strong>$F$<strong>函数）</strong>  假设隐变量数据$Z$的概率分布为$\tilde{P}(Z)$，定义分布$\tilde{P}$与参数$\theta$的函数$F(\tilde{P},\theta)$如下：<br>$$<br>F(\tilde{P},\theta) = E_{\tilde{P}}[\log P(Y,Z|\theta)] + H(\tilde{P})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.33)<br>$$<br>称为$F$函数。式中$H(\tilde{P}) = - E_{\tilde{P}}\log \tilde{P}(Z)$是分布$\tilde{P}(Z)$的熵。</p>
<hr>
<p><strong>引理 9.1</strong>  对于固定的$\theta$，存在唯一的分布$\tilde{P}_\theta$极大化$F(\tilde{P},\theta)$，这时$\tilde{P}_\theta$由下式给出：<br>$$<br>\tilde{P}_\theta(Z) = P(Z|Y,\theta)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.34)<br>$$<br>并且$\tilde{P}_\theta$随$\theta$连续变化。</p>
<hr>
<p><strong>引理 9.2</strong>  若$\tilde{P}_\theta(Z) = P(Z|Y,\theta)$，则<br>$$<br>F(\tilde{P},\theta) = \log P(Y|\theta)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9.36)<br>$$</p>
<hr>
<p><strong>定理 9.3</strong>  设$L(\theta) = \log P(Y|\theta)$为观测数据的对数似然函数，$\theta^{(i)},i=1,2,…$，为EM算法得到的参数估计序列，函数$F(\tilde{P},\theta)$由式$(9.33)$定义。如果$F(\tilde{P},\theta)$在$\tilde{P}^*$和$\theta^*$有局部极大值，那么$L(\theta)$也在$\theta^*$有局部极大值。类似地，如果$F(\tilde{P},\theta)$在$\tilde{P}^*$和$\theta^*$达到全局最大值，那么$L(\theta)$也在$\theta^*$达到全局最大值。</p>
<hr>
<p><strong>定理 9.4</strong>  EM算法的一次迭代可由$F$函数的极大-极大算法实现。</p>
<p>设$\theta^{(i)}$为第$i$次迭代参数$\theta$的估计，$\tilde{P}^{(i)}$为第$i$次迭代函数$\tilde{P}$的估计。在第$i+1$次迭代的两步为：</p>
<p>​    （1）对固定的$\theta^{(i)}$，求$\tilde{P}^{(i+1)}$使$F(\tilde{P},\theta^{(i)})$极大化；</p>
<p>​    （2）对固定的$\tilde{P}^{(i+1)}$,求$\theta^{(i+1)}$使$F(\tilde{P}^{(i+1)},\theta)$极大化。</p>
<h2 id="9-4-2-GEM算法"><a href="#9-4-2-GEM算法" class="headerlink" title="9.4.2 GEM算法"></a>9.4.2 GEM算法</h2><p><strong>算法 9.3（GEM算法1）</strong></p>
<p><strong>输入</strong>：观测数据，$F$函数；</p>
<p><strong>输出</strong>：模型参数。</p>
<p>​    （1）初始化参数$\theta^{(0)}$，开始迭代。</p>
<p>​    （2）第$i+1$次迭代，第一步：记$\theta^{(i)}$为参数$\theta$的估计值，$\tilde{P}^{(i)}$为函数$\tilde{P}$的估计，求$\tilde{P}^{(i+1)}$使$\tilde{P}$极大化$F(\tilde{P},\theta^{(i)})$；</p>
<p>​    （3）第二步：求$\theta^{(i+1)}$使$F(\tilde{P}^{(i+1)},\theta)$极大化；</p>
<p>​    （4）重复（2）和（3），直到收敛。</p>
<p>在GEM算法1中，有时求$Q(\theta,\theta^{(i)})$的极大化是很困难的。下面介绍GEM算法2和GEM算法3.</p>
<hr>
<p><strong>算法 9.4（GEM算法2）</strong></p>
<p><strong>输入</strong>：观测数据，$Q$函数；</p>
<p><strong>输出</strong>：模型参数。</p>
<p>​    （1）初始化参数$\theta^{(0)}$，开始迭代。</p>
<p>​    （2）第$i+1$次迭代，第一步：记$\theta^{(i)}$为参数$\theta$的估计值，计算<br>$$<br>Q(\theta,\theta^{(i)}) = E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]<br>$$</p>
<p>$$<br>\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\sum\limits_{Z} \log P(Y,Z|\theta)P(Z|Y,\theta)<br>$$</p>
<p>​    （3）第二步：求$\theta^{(i+1)}$使<br>$$<br>Q(\theta^{(i+1)},\theta^{(i)}) &gt; Q(\theta^{(i)},\theta^{(i)})<br>$$<br>​    （4）重复（2）和（3），直到收敛。</p>
<hr>
<p>当参数的维数为时，可采用一种特殊的GEM算法，它将EM算法的M步分解为次条件极大化，每次只改变参数向量的一个分量，其余分量不改变。</p>
<p><strong>算法 9.5（GEM算法3）</strong></p>
<p><strong>输入</strong>：观测数据，$Q$函数；</p>
<p><strong>输出</strong>：模型参数。</p>
<p>​    （1）初始化参数$\theta^{(0)} = {\theta^{(0)}_1,\theta^{(0)}_2,…,\theta^{(0)}_d}$，开始迭代。</p>
<p>​    （2）第$i+1$次迭代，第一步：记$\theta^{(i)} = {\theta^{(i)}_1,\theta^{(i)}_2,…,\theta^{(i)}_d}$为参数$\theta = {\theta_1,\theta_2,…,\theta_d}$的估计值，计算<br>$$<br>Q(\theta,\theta^{(i)}) = E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]<br>$$</p>
<p>$$<br>\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\sum\limits_{Z} \log P(Y,Z|\theta)P(Z|Y,\theta)<br>$$</p>
<p>​    （3）第二步：进行$d$次条件极大化：</p>
<p>首先，在$\theta^{(i)}_2,…,\theta^{(i)}_d$保持不变的条件下求使$Q(\theta,\theta^{(i)}) $达到极大的$\theta_1^{(i+1)}$；然后，在$\theta_1 = \theta_1^{(i+1)},\theta_j = \theta_j^{(i)},j=3,4,..,d$的条件下求使$Q(\theta,\theta^{(i)})$达到极大的$\theta_2^{(i+1)}$；如此继续，经过$d$次条件极大化，得到$\theta^{(i+1)} = {\theta^{(i+1)}_1,\theta^{(i+1)}_2,…,\theta^{(i+1)}_d}$使得<br>$$<br>Q(\theta^{(i+1)},\theta^{(i)}) &gt; Q(\theta^{(i)},\theta^{(i)})<br>$$<br>​    （4）重复（2）和（3），直到收敛。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/17/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%85%AB%E7%AB%A0-%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/17/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%85%AB%E7%AB%A0-%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">第八章 提升方法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-17 21:49:30 / 修改时间：21:48:53" itemprop="dateCreated datePublished" datetime="2021-06-17T21:49:30+08:00">2021-06-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/17/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%85%AB%E7%AB%A0-%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/17/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%85%AB%E7%AB%A0-%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>提升（boosting）方法</strong>是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。</p>
<h1 id="8-1-提升方法AdaBoost算法"><a href="#8-1-提升方法AdaBoost算法" class="headerlink" title="8.1 提升方法AdaBoost算法"></a>8.1 提升方法AdaBoost算法</h1><h2 id="8-1-1-提升方法的基本思路"><a href="#8-1-1-提升方法的基本思路" class="headerlink" title="8.1.1 提升方法的基本思路"></a>8.1.1 提升方法的基本思路</h2><p>提升方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。</p>
<p>在<strong>概率近似正确（probably approximately correct，PAC）</strong>学习框架中，</p>
<ul>
<li><p>一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；</p>
</li>
<li><p>一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。</p>
</li>
</ul>
<p>在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。</p>
<p>提升方法就是从弱学习算法，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</p>
<hr>
<p>对于提升方法来说，有两个问题需要回答：</p>
<ul>
<li>在每一轮如何改变训练数据的权值或概率分布；</li>
<li>如何将弱分类器组合成一个强分类器。</li>
</ul>
<p>关于第一个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。</p>
<p>至于第二个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</p>
<h2 id="8-1-2-AdaBoost算法"><a href="#8-1-2-AdaBoost算法" class="headerlink" title="8.1.2 AdaBoost算法"></a>8.1.2 AdaBoost算法</h2><p><strong>算法 8.1（AdaBoost）</strong></p>
<p><strong>输入</strong>：训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中，$x_i \in \chi = R^n, y \in Y = {+1, -1}, i=1,2,…,N$；弱学习算法；</p>
<p><strong>输出</strong>：最终分类器$G(x)$。</p>
<p>​    （1）初始化训练数据的权值分布<br>$$<br>D_1 = (w_{11},…,w_{1i},…,w_{1N})\ \ \ w_{1i} = \frac{1}{N},\ \ \ i= 1,2,…,N<br>$$<br>​    （2）对$m=1,2,…,M$</p>
<p>​        （a）使用具有权值分布$D_m$的训练数据集学习，得到基本分类器。<br>$$<br>G_m(x) : \mathcal{X} \longrightarrow {-1, +1}<br>$$<br>​        （b）计算$G_m(x)$在训练数据集上的分类误差率<br>$$<br>e_m = \sum\limits_{i = 1}^{N}P(G_m(x_i) \neq y_i) = \sum\limits_{i = 1}^{N}w_{mi}I(G_m(x_i) \neq y_i)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.1)<br>$$<br>​        （c）计算$G_m(x)$的系数<br>$$<br>\alpha_m = \frac{1}{2}\log\frac{1 - e_m}{e_m}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.2)<br>$$<br>这里的对数是自然对数。</p>
<p>​        （d）更新训练数据集的权值分布<br>$$<br>D_{m+1} = (w_{m+1,1},…,w_{m+1,i},…,w_{m+1,N})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.3)<br>$$</p>
<p>$$<br>w_{m+1,i} = \frac{w_{mi}}{Z_m}\exp(-\alpha_my_iG_m(x_i)), \ \ \ i= 1,2,…,N \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.4)<br>$$</p>
<p>这里，$Z_m$是规范化因子<br>$$<br>Z_m = \sum\limits_{i = 1}^{N} w_{mi}\exp(-\alpha_my_iG_m(x_i))\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.5)<br>$$<br>它使$D_{m+1}$成为一个概率分布。</p>
<p>​    （3）构建基本分类器的线性组合（所有$\alpha_m$之和并不为1）<br>$$<br>f(x) =\sum\limits_{i = 1}^{N} \alpha_m G_m(x)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.6)<br>$$<br>得到最终分类器<br>$$<br>G(x) = sign(f(x)) = sign(\sum\limits_{i = 1}^{N} \alpha_m G_m(x))\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.7)<br>$$<br>AdaBoost的特点：</p>
<ul>
<li>不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用；</li>
<li>利用基本分类器的线性组合构建最终分类器。</li>
</ul>
<h1 id="8-2-AdaBoost算法的训练误差分析"><a href="#8-2-AdaBoost算法的训练误差分析" class="headerlink" title="8.2 AdaBoost算法的训练误差分析"></a>8.2 AdaBoost算法的训练误差分析</h1><p>AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。</p>
<p><strong>定理 8.1（AdaBoost的训练误差界）</strong>  AdaBoost算法最终分类器的训练误差界为<br>$$<br>\frac{1}{N}\sum\limits_{i = 1}^{N}I(G(x_i) \neq y_i) \leq \frac{1}{N}\sum\limits_{i}\exp(-y_i f(x_i)) = \prod\limits_{m} Z_m\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.9)<br>$$<br>这里，$G(x),f(x)$和$Z_m$分别由式$(8.7)$、式$(8.6)$和式$(8.5)$给出。</p>
<p>这一定理说明，可以在每一轮选取适当的$G_m$使得$Z_m$最小，从而使训练误差下降最快。</p>
<hr>
<p><strong>定理 8.2（二类分类问题AdaBoost的训练误差界）</strong><br>$$<br>\prod\limits_{m=1}^{M} Z_m = \prod\limits_{m=1}^{M} [2\sqrt{e_m(1-e_m)} ]<br>$$</p>
<p>$$<br>= \prod\limits_{m=1}^{M} \sqrt{(1-4\gamma^2_m)}<br>$$</p>
<p>$$<br>\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \leq \exp(-1\sum\limits_{m=1}^{M} \gamma_m^2)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.10)<br>$$</p>
<p>这里，$\gamma_m = \frac{1}{2} - e_m$。</p>
<hr>
<p><strong>推论 8.1</strong>  如果存在$\gamma &gt; 0$，对所有$m$有$\gamma_m \geq \gamma$，则<br>$$<br>\frac{1}{N}\sum\limits_{i=1}^{N}I(G(x_i) \neq y_i) \leq \exp(-2M\gamma^2)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.12)<br>$$<br>这表明在此条件下AdaBoost的训练误差是以指数速率下降的。</p>
<p>AdaBoost具有适应性，即它能适应弱分类器各自的训练误差率。</p>
<h1 id="8-3-AdaBoost算法的解释"><a href="#8-3-AdaBoost算法的解释" class="headerlink" title="8.3 AdaBoost算法的解释"></a>8.3 AdaBoost算法的解释</h1><p>AdaBoost算法还有另一个解释，即可以认为AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法。</p>
<h2 id="8-3-1-前向分步算法"><a href="#8-3-1-前向分步算法" class="headerlink" title="8.3.1 前向分步算法"></a>8.3.1 前向分步算法</h2><p>考虑<strong>加法模型（additive model）</strong><br>$$<br>f(x) = \sum\limits_{m=1}^{M}\beta_m b(x;\gamma_m)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.13)<br>$$<br>其中，$b(x;\gamma_m)$为基函数，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数。</p>
<p>在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即损失函数极小化问题：<br>$$<br>\min\limits_{\beta_m,\gamma_m} \sum\limits_{i=1}^{N} L(y_i,\sum\limits_{m=1}^{M}\beta_m b(x_i;\gamma_m))\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.14)<br>$$<br><strong>前向分步算法（forward stagewise algorithm）</strong>求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数$(8.14)$，那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数：<br>$$<br>\min\limits_{\beta,\gamma} \sum\limits_{i=1}^{N} L(y_i,\beta b(x_i;\gamma))\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.15)<br>$$</p>
<hr>
<p><strong>算法 8.2（前向分步算法）</strong></p>
<p><strong>输入</strong>：训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$；损失函数$L(y,f(x))$；基函数集${b(x;\gamma)}$；</p>
<p><strong>输出</strong>：加法模型$f(x)$。</p>
<p>​    （1）初始化$f_0(x) = 0$；</p>
<p>​    （2）对$m = 1,2,..,M$</p>
<p>​        （a）极小化损失函数<br>$$<br>(\beta_m,\gamma_m) = \arg\min\limits_{\beta,\gamma}\sum\limits_{i=1}^{N} L(y_i,f_{m-1}(x_i)  + \beta b(x_i;\gamma))\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.16)<br>$$<br>​    得到参数$\beta_m,\gamma_m$。</p>
<p>​        （b）更新<br>$$<br>f_m(x) = f_{m-1}(x) + \beta_m b(x;\gamma_m)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.17)<br>$$<br>​    （3）得到加法模型<br>$$<br>f(x) = f_M(x) = \sum\limits_{m=1}^{M}\beta_m b(x;\gamma_m)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.18)<br>$$<br>这样，前向分步算法将同时求解从$m = 1$到$M$所有参数$\beta_m,\gamma_m$的优化问题简化为逐次求解各个$\beta_m,\gamma_m$的优化问题。</p>
<h2 id="8-3-2-前向分步算法与AdaBoost"><a href="#8-3-2-前向分步算法与AdaBoost" class="headerlink" title="8.3.2 前向分步算法与AdaBoost"></a>8.3.2 前向分步算法与AdaBoost</h2><p><strong>定理 8.3</strong> AdaBoost算法是前向分步加法算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。</p>
<h1 id="8-4-提升树"><a href="#8-4-提升树" class="headerlink" title="8.4 提升树"></a>8.4 提升树</h1><p>提升树是以分类树或回归树为基本分类器的提升方法。</p>
<h2 id="8-4-1-提升树模型"><a href="#8-4-1-提升树模型" class="headerlink" title="8.4.1 提升树模型"></a>8.4.1 提升树模型</h2><p>以决策树为基函数的提升方法称为<strong>提升树（booting tree）</strong>。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。</p>
<p>提升树模型可以表示为决策树的加法模型：<br>$$<br>f_M(x) = \sum\limits_{m=1}^{M}T(x;\Theta_m)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.24)<br>$$<br>其中，$T(x;\Theta_m)$表示决策树，$\Theta_m$为决策树的参数，$M$为树的个数。</p>
<h2 id="8-4-2-提升树算法"><a href="#8-4-2-提升树算法" class="headerlink" title="8.4.2 提升树算法"></a>8.4.2 提升树算法</h2><p>提升树算法采用前向分步算法。首先确定初始提升树$f_0(x) = 0$，第$m$步的模型是<br>$$<br>f_m(x) = f_{m-1}(x) + T(x;\Theta_m)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.25)<br>$$<br>其中，$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$：<br>$$<br>\hat \Theta_m  = \arg\min\limits_{i=1}^{N}L(y_i,f_{m-1}(x_i) + T(x_i;\Theta_m))\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.26)<br>$$<br>针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。</p>
<p>对于二分类问题，提升树算法只需将AdaBoost算法8.1中的基本分类器限制为二类分类树即可，可以说这时的提升树算法是AdaBoost算法的特殊情况。</p>
<hr>
<p>已知一个训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中，$x_i \in \chi = R^n$，$\mathcal{X}$为输入空间，$ y \in Y \subseteq R$，$\mathcal{Y}$为输出空间。如果将输入空间$\mathcal{X}$划分为$J$个互不相交的区域$R_1,R_2,…,R_J$，并且在每个区域上确定输出的常量$c_j$，那么树可表示为<br>$$<br>T(x;\Theta) = \sum\limits_{j=1}^{J}c_jI(x \in R_j)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.27)<br>$$<br>其中，参数$\Theta = {(R_1,c_1),(R_2,c_2),…,(R_J,c_J)}$表示树的区域划分和各区域上的常数。$J$是回归树的复杂度即叶结点个数。</p>
<p>回归问题提升树使用以下前向分步算法：<br>$$<br>f_0(x) = 0<br>$$</p>
<p>$$<br>f_m(x) = f_{m-1}(x) + T(x;\Theta_m), \ \ \ m = 1,2,…,M<br>$$</p>
<p>$$<br>f_M(x) = \sum\limits_{m=1}^{M}T(x;\Theta_m)<br>$$</p>
<p>在前向分步算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解<br>$$<br>\hat \Theta_m  = \arg\min\limits_{i=1}^{N}L(y_i,f_{m-1}(x_i) + T(x_i;\Theta_m))<br>$$<br>得到$\hat \Theta_m$，即第$m$棵树的参数。</p>
<p>当采用平方误差损失函数时，<br>$$<br>L(y,f(x)) = (y-f(x))^2<br>$$<br>其损失变为<br>$$<br>L(y,f_{m-1}(x) + T(x;\Theta_m)) = [y- f_{m-1}(x)-T(x;\Theta_m)]^2<br>$$</p>
<p>$$<br>= [r-T(x;\Theta_m)]^2<br>$$</p>
<p>这里，<br>$$<br>r = y- f_{m-1}(x)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.28)<br>$$<br>是更强模型拟合数据的<strong>残差（residual）</strong>。所以，对回归问题的提升树算法来说，只需简单地拟合当前模型的残差。</p>
<p><strong>算法 8.3（回归问题的提升树算法）</strong></p>
<p><strong>输入</strong>：训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，$x_i \in \chi = R^n$，$ y \in Y \subseteq R$；</p>
<p><strong>输出</strong>：提升树$f_M(x)$。</p>
<p>​    （1）初始化$f_0(x) = 0$。</p>
<p>​    （2）对$m = 1,2,…,M$。</p>
<p>​        （a）按式$(8.28)$计算残差：<br>$$<br>r_{mi} = y_i- f_{m-1}(x_i), \ \ \ i=1,2,…,N<br>$$<br>​        （b）拟合残差$r_{mi}$学习一个回归树，得到$T(x;\Theta_m)$。</p>
<p>​        （c）更新$f_m(x) = f_{m-1}(x) + T(x;\Theta_m)$。</p>
<p>​    （3）得到回归问题提升树<br>$$<br>f_M(x) = \sum\limits_{m=1}^{M}T(x;\Theta_m)<br>$$</p>
<h2 id="8-4-3-梯度提升"><a href="#8-4-3-梯度提升" class="headerlink" title="8.4.3 梯度提升"></a>8.4.3 梯度提升</h2><p>对于一般损失函数而言，往往每一步优化并不那么容易。针对这一问题，Freidman提出了<strong>梯度提升（gradient boosting）</strong>算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值<br>$$<br>-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]<em>{f(x)=f</em>{m-1}(x)}<br>$$<br>作为回归问题提升树算法中的残差近似值，拟合一个回归树。</p>
<p><strong>算法 8.4（梯度提升算法）</strong></p>
<p><strong>输入</strong>：训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，$x_i \in \chi = R^n$，$ y \in Y \subseteq R$；</p>
<p><strong>输出</strong>：提升树$\hat f(x)$。</p>
<p>​    （1）初始化<br>$$<br>f_0(x) = \arg\min\limits_{c}\sum\limits_{i=1}^{N}L(y_i,c)<br>$$<br>​    （2）对$m = 1,2,…,M$</p>
<p>​        （a）对$i=1,2,…,N$，计算<br>$$<br>r_{mi} = -[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]<em>{f(x)=f</em>{m-1}(x)}<br>$$<br>​        （b）对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj},j=1,2,…,J$。</p>
<p>​        （c）对$j=1,2,…,J$，计算<br>$$<br>c_{mj} =\arg\min\limits_{c}\sum\limits_{x_i \in R_{mj}}L(y_i,f_{m-1}(x_i) + c)<br>$$<br>​        （d）更新$f_m(x) = f_{m-1}(x) + \sum\limits_{j=1}^{J}c_{mj}I(x \in R_{mj})$</p>
<p>​    （3）得到回归树<br>$$<br>\hat f(x) = f_M(x) = \sum\limits_{m=1}^{M}\sum\limits_{j=1}^{J}I(x \in R_{mj})<br>$$</p>
<ul>
<li><p>算法第1步初始化，估计使损失函数极小化的常数值，它是只有一个根结点的树。</p>
</li>
<li><p>第2(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。</p>
<ul>
<li>对于平方损失函数，它就是通常所说的残差；</li>
<li>对于一般损失函数他就是残差的近似值。</li>
</ul>
</li>
<li><p>第2(b)步估计回归树叶结点区域，以拟合残差的近似值。</p>
</li>
<li><p>第2(c)步利用线性搜索估计叶结点区域的值，是损失函数极小化。</p>
</li>
<li><p>第2(d)步更新回归树。</p>
</li>
<li><p>第3步得到输出的最终模型$\hat f(x)$</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/16/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B8%83%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/16/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B8%83%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="post-title-link" itemprop="url">第七章 支持向量机</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-16 20:46:39 / 修改时间：20:46:21" itemprop="dateCreated datePublished" datetime="2021-06-16T20:46:39+08:00">2021-06-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/16/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B8%83%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/16/Statistical_Learning_Methods_Notes/%E7%AC%AC%E4%B8%83%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li><p><strong>支持向量机（support vector machines，SVM）</strong>是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。</p>
</li>
<li><p>支持向量机的学习策略就是间隔最大化，可形式化为一个求解<strong>凸二次规划（convex quadratic programming）</strong>的问题，也等价于正则化的合页损失函数的最小化问题。</p>
</li>
<li><p>支持向量机的学习算法是求解凸二次规划的最优化算法。</p>
</li>
</ul>
<p>支持向量机学习方法包含构建由简至繁的模型：</p>
<ul>
<li><p><strong>线性可分支持向量机（linear support vector machine in linearly separable case）</strong></p>
<p>当训练数据线性可分时，通过<strong>硬间隔最大化（hard margin maximization）</strong>，学习一个线性分类器，即线性可分支持向量机，又称为硬间隔支持向量机。</p>
</li>
<li><p><strong>线性支持向量机（linear support vector machine）</strong></p>
<p>当训练数据近似线性可分时，通过<strong>软间隔最大化（soft margin maximization）</strong>，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机。</p>
</li>
<li><p><strong>非线性支持向量机（non-linear support vector machine）</strong></p>
<p>当训练数据线性不可分时，通过使用<strong>核技巧（kernel trick）</strong>及软间隔最大化，学习非线性支持向量机。</p>
</li>
</ul>
<p>当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，<strong>核函数（kernel function）</strong>表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。</p>
<h1 id="7-1-线性可分支持向量机与硬间隔最大化"><a href="#7-1-线性可分支持向量机与硬间隔最大化" class="headerlink" title="7.1 线性可分支持向量机与硬间隔最大化"></a>7.1 线性可分支持向量机与硬间隔最大化</h1><h1 id="7-1-1-线性可分支持向量机"><a href="#7-1-1-线性可分支持向量机" class="headerlink" title="7.1.1 线性可分支持向量机"></a>7.1.1 线性可分支持向量机</h1><p>假设输入空间与特征空间为两个不同的空间。输入空间为欧式空间或离散集合，特征空间为欧式空间或希尔伯特空间。线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量。非线性支持向量机利用一个从输入到特征空间的非线性映射将输入映射为特征向量。所以，输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的。</p>
<p>假设给定一个特征空间上的训练数据集<br>$$<br>T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}<br>$$<br>其中，$x_i \in \chi = R^n, y \in \mathcal{Y} = {+1, -1}, i=1,2,…,N$；在假设训练数据集是线性可分的。</p>
<p>学习的目标是：在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程$\omega · x + b = 0$，它由法向量$\omega$和截距$b$决定，可用$(\omega,b)$来表示。分离超平面将特征空间划分为两部分，一部分是正类，一部分是负类。法向量指向一侧为正类，另一侧为负类。</p>
<p>一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。</p>
<p><strong>定义 7.1（线性可分支持向量机）</strong>  给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为<br>$$<br>\omega^* · x + b^* = 0 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.1)<br>$$<br>以及相应的分类决策函数<br>$$<br>f(x) = sign(\omega^* · x + b^*)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.2)<br>$$<br>称为线性可分支持向量机。</p>
<h2 id="7-1-2-函数间隔和几何间隔"><a href="#7-1-2-函数间隔和几何间隔" class="headerlink" title="7.1.2 函数间隔和几何间隔"></a>7.1.2 函数间隔和几何间隔</h2><ul>
<li><p>一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$\omega · x + b = 0$确定的情况下，$|\omega · x + b|$能够相对地表示点$x$距离超平面的远近。而$\omega · x + b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可用量$y(\omega · x + b)$来表示分类的正确性及确信度，这就是<strong>函数间隔（functional margin）</strong>的概念。</p>
<p><strong>定义 7.2（函数间隔）</strong>  对于给定的训练数据集$T$和超平面$(\omega,b)$，定义超平面$(\omega,b)$关于样本点$(x_i,y_i)$的函数间隔为<br>$$<br>\hat \gamma_i = y_i(\omega·x_i+b)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.3)<br>$$<br>定义超平面$(\omega,b)$关于训练数据集$T$的函数间隔为超平面$(\omega,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，即<br>$$<br>\hat \gamma= \min\limits_{i=1,2,…,N}\hat\gamma_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.4)<br>$$<br>函数间隔可以表示分类预测的正确性及确信度。</p>
</li>
<li><p>但是选择分离超平面时，只有函数间隔还不够。因为只要成比例地改变$\omega$和$b$，超平面并没有改变，但函数间隔却成为原来的2倍。可以对分离超平面的法向量$\omega$加某些约束，如规范化，$||\omega|| = 1$，这使得间隔是确定的。这时函数间隔成为<strong>几何间隔（geometric margin）</strong>。</p>
<p><strong>定义 7.3（几何间隔）</strong>  对于给定的训练数据集$T$和超平面$(\omega,b)$，定义超平面$(\omega,b)$关于样本点$(x_i,y_i)$的几何间隔为<br>$$<br>\gamma_i = y_i(\frac{\omega}{||\omega||}·x_i + \frac{b}{||\omega||})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.5)<br>$$<br>定义超平面$(\omega,b)$关于训练数据集$T$的几何间隔为超平面$(\omega,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即<br>$$<br>\gamma= \min\limits_{i=1,2,…,N}\gamma_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.6)<br>$$<br>超平面$(\omega,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的<strong>带符号的距离（signed distance）</strong>，当样本点被超平面正确分类时就是实例点到超平面的距离。</p>
</li>
</ul>
<hr>
<p>从函数间隔和几何间隔的定义可知，函数间隔和几何间隔有下面的关系：<br>$$<br>\gamma_i = \frac{\hat\gamma_i}{||\omega||}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.7)<br>$$</p>
<p>$$<br>\gamma = \frac{\hat\gamma}{||\omega||}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.8)<br>$$</p>
<p>如果$||\omega|| = 1$，那么函数间隔和几何间隔相等。如果超平面参数$\omega$和$b$成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。</p>
<h2 id="7-1-3-间隔最大化"><a href="#7-1-3-间隔最大化" class="headerlink" title="7.1.3 间隔最大化"></a>7.1.3 间隔最大化</h2><p>支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。</p>
<p>间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。</p>
<ol>
<li><strong>最大间隔分离超平面</strong></li>
</ol>
<p>考虑如何求得一个几何间隔最大的分离超平面，即最大间隔分离超平面。具体地，这个问题可以表示为下面的约束最优化问题：<br>$$<br>\max\limits_{\omega,b} \gamma\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.9)<br>$$</p>
<p>$$<br>s.t.\ \ \ y_i(\frac{\omega}{||\omega||}·x_i + \frac{b}{||\omega||}) \geq \gamma, \ \ \ i=1,2,…,N\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.10)<br>$$</p>
<p>即我们希望最大化超平面$(\omega,b)$关于训练数据集的几何间隔$\gamma$，约束条件表示的是超平面$(\omega,b)$关于每个训练样本点的几何间隔至少是$\gamma$。</p>
<p>考虑几何间隔和函数间隔的关系式，可将这个问题改写为<br>$$<br>\max\limits_{\omega,b} \frac{\hat\gamma}{||\omega||}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.11)<br>$$</p>
<p>$$<br>s.t.\ \ \ y_i(\omega·x_i+b) \geq \hat\gamma, \ \ \ i=1,2,…,N\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.12)<br>$$</p>
<p>函数间隔$\hat\gamma$的取值并不影响最优化问题的解。事实上，假设将$\omega$和$b$按比例改变为$\lambda\omega$和$\lambda b$，这时函数间隔成为$\lambda \hat\gamma$。函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，它产生一个等价的最优化问题。这样，就可以取$\hat\gamma = 1$。将$\hat\gamma = 1$代入上面的最优化问题，注意到最大化$\frac{1}{||\omega||}$和最小化$\frac{1}{2}||\omega||^2$是等价的，于是就得到下面的线性可分支持向量机学习的最优化问题：<br>$$<br>\min\limits_{\omega,b}\frac{1}{2}||\omega||^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.13)<br>$$</p>
<p>$$<br>s.t.\ \ \ y_i(\omega·x_i+b) - 1 \geq 0, \ \ \ i=1,2,…,N\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.14)<br>$$</p>
<p>这是一个凸二次规划（convex quadratic programming）问题。</p>
<p>凸优化问题是指约束最优化问题<br>$$<br>\min f(\omega)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.15)<br>$$</p>
<p>$$<br>s.t.\ \ \ g_i(\omega) \leq 0, \ \ \ i=1,2,…,k\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.16)<br>$$</p>
<p>$$<br>h_i(\omega) = 0, \ \ \ i=1,2,…,l\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.17)<br>$$</p>
<p>其中，目标函数$f(\omega)$和约束函数$g_i(\omega)$都是$R^n$上的连续可微的凸函数，约束函数$h_i(\omega)$是$R^n$上的仿射函数。</p>
<p>注：$f(x)$称为仿射函数，如果它满足$f(x) = a·x +b,a\in R^n,b \in R^n,x\in R^n$。</p>
<p>当目标函数$f(\omega)$是二次函数且约束函数$g_i(\omega)$是仿射函数时，上述凸最优化问题称为凸二次规划问题。</p>
<p><strong>算法 7.1（线性可分支持向量机学习算法——最大间隔法）</strong></p>
<p><strong>输入</strong>：线性可分训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中，$x_i \in \chi = R^n, y \in \mathcal{Y} = {+1, -1}, i=1,2,…,N$；</p>
<p><strong>输出</strong>：最大间隔分离超平面和分类决策函数。</p>
<p>​    （1）构造并求解约束最优化问题：<br>$$<br>\min\limits_{\omega,b}\frac{1}{2}||\omega||^2<br>$$</p>
<p>$$<br>s.t.\ \ \ y_i(\omega·x_i+b) - 1 \geq 0, \ \ \ i=1,2,…,N<br>$$</p>
<p>求得最优解$\omega^*,b^*$。</p>
<p>​    （2）由此得到分离超平面：<br>$$<br>\omega ^* ·x + b^* = 0<br>$$<br>分类决策函数<br>$$<br>f(x) = sign(\omega ^* ·x + b^*)<br>$$</p>
<ol start="2">
<li><strong>最大间隔分离超平面的存在唯一性</strong></li>
</ol>
<p><strong>定理 7.1（最大间隔分离超平面的存在唯一性）</strong>  若训练数据集$T$线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</p>
<ol start="3">
<li><strong>支持向量和间隔边界</strong></li>
</ol>
<p>在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。支持向量是使约束条件式（7.14）等号成立的点，即<br>$$<br>y_i(\omega·x_i+ b) -1 = 0<br>$$<br>对$y_i = +1$的正例点，支持向量在超平面<br>$$<br>H_1 ：\omega ·x +b = 1<br>$$<br>上，对$y_i = -1$的正例点，支持向量在超平面<br>$$<br>H_1 ：\omega ·x +b = -1<br>$$<br>上。</p>
<p>c，并且没有实例点落在它们中间。在$H_1$与$H_2$之间形成一条长带，分离超平面与它们平行且位于它们中央。长带的宽度，即$H_1$与$H_2$之间的距离称为<strong>间隔（margin）</strong>。间隔依赖于分离超平面的法向量$\omega$，等于$\frac{2}{||\omega||}$。$H_1$和$H_2$称为间隔边界。</p>
<p>在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。</p>
<p>支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定。</p>
<h2 id="7-1-4-学习的对偶算法"><a href="#7-1-4-学习的对偶算法" class="headerlink" title="7.1.4 学习的对偶算法"></a>7.1.4 学习的对偶算法</h2><p>通过求解<strong>对偶问题（dual problem）</strong>得到<strong>原始问题（primal problem）</strong>的最优解，这就是线性可分支持向量机的<strong>对偶算法（dual algorithm）</strong>。</p>
<p>这样做的优点：</p>
<ul>
<li>对偶问题往往更容易求解；</li>
<li>自然引入核函数，进而推广到非线性分类问题。</li>
</ul>
<p>首先构建拉格朗日函数（Lagrange function）。为此，对每一个不等式约束$(7.14)$引进拉格朗日乘子（Lagrange multiplier）$\alpha_i \geq 0,\ \ \ i = 1,2,…,N$，定义拉格朗日函数：<br>$$<br>L(\omega,b,\alpha) = \frac{1}{2}||\omega||^2 - \sum\limits_{i=1}^{N}\alpha_i y_i (\omega·x_i + b) + \sum\limits_{i=1}^{N}\alpha_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.18)<br>$$<br>其中，$\alpha = (\alpha_1,\alpha_2,…,\alpha_N)^T$为拉格朗日乘子向量。</p>
<p>根据拉格朗日对偶性，原始为题的对偶问题是极大极小问题：<br>$$<br>\max\limits_{\alpha} \min\limits_{\omega,b}L(\omega,b,\alpha)<br>$$<br>所以，为了得到对偶问题的解，需要先求$L(\omega,b,\alpha)$对$\omega,b$的极小，再求对$\alpha$的极大。</p>
<p>对偶最优化问题：<br>$$<br>\min\limits_{\alpha} \ \ \ \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i \alpha_j y_i y_j (x_i·x_j)\ - \ \sum\limits_{i=1}^{N}\alpha_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.22)<br>$$</p>
<p>$$<br>s.t. \ \ \ \sum\limits_{i=1}^{N}\alpha_i y_i = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.23)<br>$$</p>
<p>$$<br>\alpha_i \geq 0, \ \ \ i = 1,2,…,N\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.24)<br>$$</p>
<p><strong>定理 7.2</strong>  设$\alpha^* = (\alpha^*_1,\alpha^*_2,…,\alpha^*<em>l)^T$是对偶最优化问题$(7.22)$ ~ $(7.24)$的解，则存在下标$j$，使得$\alpha_j^* &gt; 0$，并可按下式求得原始最优化问题$(7.13)$ ~ $(7.14)$的解$\omega^*,b^*$<br>$$<br>\omega^* =\sum\limits</em>{i=1}^{N}\alpha_i y_i x_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.25)<br>$$</p>
<p>$$<br>b^* = y_j - \sum\limits_{i=1}^{N}\alpha_i y_i (x_i·x_j)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.26)<br>$$</p>
<p>由此定理可知，分离超平面可以写成<br>$$<br>\sum\limits_{i=1}^{N}\alpha_i^* y_i (x·x_i) + b^* = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.29)<br>$$<br>分类决策函数可以写成<br>$$<br>f(x) = sign(\sum\limits_{i=1}^{N}\alpha_i^* y_i (x·x_i) + b^*)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.30)<br>$$<br>这就是说，分类决策函数只依赖于输入$x$和训练样本输入的内积。式$(7.30)$称为线性可分支持向量机的对偶形式。</p>
<p>综上所述，对于给定的线性可分训练数据集，可以首先求对偶问题$(7.22)$ ~ $(7.24)$的解$\alpha^*$；再利用式$(7.25)$ ~ $(7.26)$求得原始问题的解$\omega^*,b^*$；从而得到分离超平面及分类决策函数。这种算法称为线性可分支持向量机的对偶学习算法，是线性可分支持向量机学习的基本算法。</p>
<p><strong>算法 7.2（线性可分支持向量机学习算法）</strong></p>
<p><strong>输入</strong>：线性可分训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中，$x_i \in \chi = R^n, y \in \mathcal{Y} = {+1, -1}, i=1,2,…,N$；</p>
<p><strong>输出</strong>：分离超平面和分类决策函数。</p>
<p>​    （1）构造并求约束最优化问题<br>$$<br>\min\limits_{\alpha} \ \ \ \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i \alpha_j y_i y_j (x_i·x_j)\ + \ \sum\limits_{i=1}^{N}\alpha_i<br>$$</p>
<p>$$<br>s.t. \ \ \ \sum\limits_{i=1}^{N}\alpha_i y_i = 0<br>$$</p>
<p>$$<br>\alpha_i \geq 0, \ \ \ i = 1,2,…,N<br>$$</p>
<p>求得最优解$\alpha^* = (\alpha^*_1,\alpha^*_2,…,\alpha^*_N)^T$。</p>
<p>​    （2）计算<br>$$<br>\omega^* =\sum\limits_{i=1}^{N}\alpha_i y_i x_i<br>$$<br>并选择$\alpha^*$的一个正分量$\alpha_j^* &gt; 0 $，计算<br>$$<br>b^* = y_j - \sum\limits_{i=1}^{N}\alpha_i y_i (x_i·x_j)<br>$$<br>​    （3）求得分离超平面<br>$$<br>\omega^<em>·x +b^</em> = 0<br>$$<br>分类决策函数：<br>$$<br>f(x) = sign(\omega^<em>·x +b^</em>)<br>$$<br>由式$(7.25)$、式$(7.26)$可知，$\omega^*$和$b^*$只依赖于训练数据中对应于$\alpha_i^* &gt; 0$的样本点$(x_i,y_i)$，而其他样本点对$\omega^*$和$b^*$没有影响。我们将训练数据中对应于$\alpha_i^* &gt; 0$的实例点$x_i \in R^n$称为支持向量。</p>
<p><strong>定义 7.4（支持向量）</strong>  考虑原始最优化问题$(7.13)$ ~ $(7.14)$及对偶最优化问题$(7.22)$ ~ $(7.24)$，将训练数据集中对应于$\alpha_i^* &gt; 0$的样本点$(x_i,y_i)$的实例$x_i \in R^n$称为支持向量。</p>
<h1 id="7-2-线性支持向量机与软间隔最大化"><a href="#7-2-线性支持向量机与软间隔最大化" class="headerlink" title="7.2 线性支持向量机与软间隔最大化"></a>7.2 线性支持向量机与软间隔最大化</h1><h2 id="7-2-1-线性支持向量机"><a href="#7-2-1-线性支持向量机" class="headerlink" title="7.2.1 线性支持向量机"></a>7.2.1 线性支持向量机</h2><p>假设给定一个特征空间上的训练数据集<br>$$<br>T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}<br>$$<br>其中，$x_i \in \chi = R^n, y \in Y = {+1, -1}, i=1,2,…,N$，$x_i$为第$i$个特征向量，$y_i$为$x_i$的类标记。再假设训练数据集不是线性可分的。通常情况是，训练数据中有一些<strong>特异点（outlier）</strong>，将这些特异点出去后，剩下大部分的样本点组成的集合是线性可分的。</p>
<p>线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件$(7.14)$。为了解决这个问题，可以对每个样本点$(x_i,y_i)$引进一个松弛变量$\xi_i \geq 0$，使函数间隔加上松弛变量大于等于1。这样，约束条件变为<br>$$<br>y_i(\omega·x_i+b) \geq 1 - \xi_i<br>$$<br>同时，对每个松弛变量$\xi_i$，支付一个代价$\xi_i$。目标函数由原来的$\frac{1}{2}||\omega||^2$变成<br>$$<br>\frac{1}{2}||\omega||^2 \ + \ C\sum\limits_{i=1}^N \xi_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.31)<br>$$<br>这里，$C&gt;0$称为惩罚参数，一般由应用问题决定，$C$值大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。最小化目标函数$(7.31)$包含两层含义：使$\frac{1}{2}||\omega||^2 $尽量小即间隔尽量大，同时使误分类点的个数尽量小，$C$是调和二者的系数。</p>
<p>线性不可分的支持向量机的学习问题变成如下凸二次规划（convex quadratic programming）问题（原始问题）：<br>$$<br>\min\limits_{\omega,b,\xi} \ \ \ \frac{1}{2}||\omega||^2 \ + \ C\sum\limits_{i=1}^N \xi_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.32)<br>$$</p>
<p>$$<br>s.t.\ \ \ y_i(\omega·x_i+b) \geq 1 - \xi_i \ \ \ i = 1,2,…,N\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.33)<br>$$</p>
<p>$$<br>\xi_i \geq 0,\ \ \ i = 1,2,…,N\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.34)<br>$$</p>
<p>原始问题$(7.32)$ ~ $(7.34)$是一个凸二次规划问题，因而关于$(\omega,b,\xi)$的解是存在的。可以证明$\omega$的解是唯一的，但$b$的解可能不唯一，而是存在于一个区间。</p>
<p><strong>定义 7.5（线性支持向量机）</strong>  对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题$(7.32)$ ~ $(7.34)$，得到的分离超平面为<br>$$<br>\omega^* · x + b^* =0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.35)<br>$$<br>以及相应的分类决策函数<br>$$<br>f(x) = sign(\omega^* · x + b^*)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.36)<br>$$<br>称为线性支持向量机。</p>
<p>显然，线性支持向量机包含线性可分支持向量机。</p>
<h2 id="7-2-2-学习的对偶算法"><a href="#7-2-2-学习的对偶算法" class="headerlink" title="7.2.2 学习的对偶算法"></a>7.2.2 学习的对偶算法</h2><p>原始问题$(7.32)$ ~ $(7.34)$的对偶问题是<br>$$<br>\min\limits_{\alpha} \ \ \ \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i \alpha_j y_i y_j (x_i·x_j)\ - \ \sum\limits_{i=1}^{N}\alpha_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.37)<br>$$</p>
<p>$$<br>s.t. \ \ \ \sum\limits_{i=1}^{N}\alpha_i y_i = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.38)<br>$$</p>
<p>$$<br>0 \leq \alpha_i \leq C, \ \ \ i = 1,2,…,N\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.39)<br>$$</p>
<p><strong>定理 7.3</strong>  设$\alpha^* = (\alpha^*_1,\alpha^*_2,…,\alpha^*<em>N)^T$是对偶问题$(7.37)$<del>$(7.39)$的一个解，若存在$\alpha^*$的一个分量$\alpha_j^*$，$0&lt; \alpha_j^*&lt;C$ ，则原始问题$(7.32)$</del>$(7.34)$的解$\omega^*,b^*$可按下式求得：<br>$$<br>\omega^* =\sum\limits</em>{i=1}^{N}\alpha_i^* y_i x_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.50)<br>$$</p>
<p>$$<br>b^* = y_j - \sum\limits_{i=1}^{N}\alpha_i^* y_i (x_i·x_j)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.51)<br>$$</p>
<p>由此定理可知，分离超平面可以写成<br>$$<br>\sum\limits_{i=1}^{N}\alpha_i^* y_i (x·x_j) + b^* = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.55)<br>$$<br>分类决策函数可以写成<br>$$<br>f(x)= sign(\sum\limits_{i=1}^{N}\alpha_i^* y_i (x·x_j) + b^*)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.56)<br>$$<br><strong>算法 7.3（线性支持向量机学习算法）</strong></p>
<p><strong>输入</strong>：线性可分训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中，$x_i \in \chi = R^n, y \in \mathcal{Y} = {+1, -1}, i=1,2,…,N$；</p>
<p><strong>输出</strong>：分离超平面和分类决策函数。</p>
<p>​    （1）选择惩罚参数$C&gt;0$，构造并求解凸二次规划问题<br>$$<br>\min\limits_{\alpha} \ \ \ \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i \alpha_j y_i y_j (x_i·x_j)\ - \ \sum\limits_{i=1}^{N}\alpha_i<br>$$</p>
<p>$$<br>s.t. \ \ \ \sum\limits_{i=1}^{N}\alpha_i y_i = 0<br>$$</p>
<p>$$<br>0 \leq \alpha_i \leq C, \ \ \ i = 1,2,…,N<br>$$</p>
<p>求得最优解$\alpha^* = (\alpha^*_1,\alpha^*_2,…,\alpha^*_N)^T$。</p>
<p>​    （2）计算$\omega^* = \sum\limits_{i=1}^{N}\alpha_i^* y_i x_i$</p>
<p>选择$\alpha^*$的一个分量$\alpha_j^*$适合条件$0&lt;\alpha_j^* &lt;C$，计算<br>$$<br>b^* = y_j - \sum\limits_{i=1}^{N}\alpha_i^* y_i (x_i·x_j)<br>$$<br>​    （3）求得分离超平面<br>$$<br>\omega^<em>·x+b^</em> = 0<br>$$<br>分类决策函数<br>$$<br>f(x)=sign(\omega^<em>·x+b^</em>)<br>$$</p>
<h2 id="7-2-3-支持向量"><a href="#7-2-3-支持向量" class="headerlink" title="7.2.3 支持向量"></a>7.2.3 支持向量</h2><p>在线性不可分的情况下，将对偶问题$(7.37)$~$(7.39)$的解$\alpha^* = (\alpha^*_1,\alpha^*_2,…,\alpha^*_N)^T$中对应于$\alpha^*_i &gt; 0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量（软间隔的支持向量）。</p>
<p>软间隔的支持向量$x_i$或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。</p>
<ul>
<li>若$\alpha_i^* &lt;C$，则$\xi_i = 0$，支持向量$x_i$恰好落在间隔边界上；</li>
<li>若$\alpha_i^* = C$，$0&lt;\xi_i &lt;1$，则分类正确，$x_i$在间隔边界与分离超平面之间；</li>
<li>若$\alpha_i^* = C$，$\xi_i = 1$，则$x_i$在分离超平面上；</li>
<li>若$\alpha_i^* = C$，$\xi_i &gt; 1$，则$x_i$位于分离超平面误分类一侧。</li>
</ul>
<h2 id="7-2-4-合页损失函数"><a href="#7-2-4-合页损失函数" class="headerlink" title="7.2.4 合页损失函数"></a>7.2.4 合页损失函数</h2><p>线性支持向量机学习还有另外一种解释，就是最小化以下目标函数：<br>$$<br>\sum\limits_{i=1}^{N}[1-y_i(\omega·x +b)]_{+} + \lambda||\omega||^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.57)<br>$$</p>
<ul>
<li><p>目标函数的第1项是经验损失或经验风险，函数<br>$$<br>L(y(\omega·x + b)) = [1-y(\omega·x+b)]<em>{+}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.58)<br>$$<br>称为<strong>合页损失函数（hinge loss function）</strong>。下标“+”表示以下去正值的函数。<br>$$<br>[z]</em>{+} = \lbrace_{0,\ \ \ z \leq 0}^{z, \ \ \ z &gt;0}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.59)<br>$$<br>这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔（确信度）$y_i(\omega·x_i+b)$大于1时，损失是0，否则损失是$1-y_i(\omega·x_i+b)$。</p>
</li>
<li><p>目标函数的第2项是系数为$\lambda$的$\omega$的$L_2$范数，是正则化项。</p>
</li>
</ul>
<p><strong>定理 7.4</strong>  线性支持向量机原始最优化问题：<br>$$<br>\min\limits_{\omega,b,\xi} \ \ \ \frac{1}{2}||\omega||^2 \ + \ C\sum\limits_{i=1}^N \xi_i<br>$$</p>
<p>$$<br>s.t.\ \ \ y_i(\omega·x_i+b) \geq 1 - \xi_i \ \ \ i = 1,2,…,N<br>$$</p>
<p>$$<br>\xi_i \geq 0,\ \ \ i = 1,2,…,N<br>$$</p>
<p>等价于最优化问题<br>$$<br>\min\limits_{\omega,b}\ \ \ \sum\limits_{i=1}^{N}[1-y_i(\omega·x +b)]_{+} + \lambda||\omega||^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.63)<br>$$<br>合页损失函数横轴是函数间隔$y(\omega·x +b)$，纵轴是损失。由于函数形状像一个合页，故名合页损失函数。</p>
<p>0-1损失函数，可以认为它是二分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化由其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为<strong>代理损失函数（surrogate loss function）</strong>。</p>
<p>合页损失函数不仅要分类正确，而且确信度足够高时损失才是0。也就是说，合页损失函数对学习有更高的要求。</p>
<h1 id="7-3-非线性支持向量机与核函数"><a href="#7-3-非线性支持向量机与核函数" class="headerlink" title="7.3 非线性支持向量机与核函数"></a>7.3 非线性支持向量机与核函数</h1><p>非线性支持向量机主要特点是利用核技巧（kernel trick）。</p>
<h2 id="7-3-1-核技巧"><a href="#7-3-1-核技巧" class="headerlink" title="7.3.1 核技巧"></a>7.3.1 核技巧</h2><ol>
<li><strong>非线性分类问题</strong></li>
</ol>
<p>一般来说，对给定的一个训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中，$x_i \in \chi = R^n, y \in Y = {+1, -1}, i=1,2,…,N$。如果能用$R^n$中的一个超曲面将正负例正确分开，则称这个问题为非线性可分问题。</p>
<p>非线性问题往往不好求解，所以希望能用解线性分类问题的方法解决这个问题。所采取的方法是进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题。</p>
<p>用线性分类方法求解非线性分类问题分为两步：</p>
<ul>
<li>使用一个变换将原空间的数据映射到新空间；</li>
<li>在新空间里用线性分类学习方法从训练数据中学习分类模型。</li>
</ul>
<hr>
<ol start="2">
<li><strong>核函数的定义</strong></li>
</ol>
<p><strong>定义 7.6 （核函数）</strong>  设$\chi$是输入空间（欧式空间$R^n$的子集或离散集合），又设$\mathcal{H}$为特征空间（希尔伯特空间），如果存在一个从$\mathcal{X}$到$\mathcal{H}$的映射<br>$$<br>\phi(x) : \mathcal{X} \longrightarrow \mathcal{H}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.65)<br>$$<br>使得对所有$x,z \in \mathcal{X}$，函数$K(x,z)$满足条件<br>$$<br>K(x,z) = \phi(x)·\phi(z)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.66)<br>$$<br>则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)·\phi(z)$为$\phi(x)$和$\phi(z)$的内积。</p>
<hr>
<p>核技巧想法是，在学习与预测中只定义核函数$K(x,z)$，而不显示地定义映射函数$\phi$。通常，直接计算$K(x,z)$比较容易，而通过$\phi(x)$和$\phi(z)$计算$K(x,z)$并不容易。</p>
<p>对于给定的核$K(x,z)$，特征空间$\mathcal{H}$和映射函数$\phi$的取法并不唯一，可以取不同的特征空间，即便是在同一特征空间里也可以取不同的映射。</p>
<hr>
<ol start="3">
<li><strong>核技巧在支持向量机中的应用</strong></li>
</ol>
<p>在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数$(7.37)$中内积$x_i·x_j$可以用核函数$K(x_i,x_j) = \phi(x_i)·\phi(x_j)$来代替。此时对偶问题的目标函数成为<br>$$<br>W(\alpha) = \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i \alpha_j y_i y_j K(x_i,x_j)\ - \ \sum\limits_{i=1}^{N}\alpha_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.67)<br>$$<br>同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为<br>$$<br>f(x)= sign(\sum\limits_{i=1}^{N}\alpha_i^* y_i \phi(x)·\phi(x_j) + b^*)<br>$$</p>
<p>$$<br>\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =sign(\sum\limits_{i=1}^{N}\alpha_i^* y_i K(x_i,x) + b^*)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.68)<br>$$</p>
<p>这等价于经过映射函数$\phi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积$x_i·x_j$变换为特征空间中的内积$\phi(x_i)·\phi(x_j)$，在新的特征空间里从训练样本中学习线性支持向量机。当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型。</p>
<p>在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法来求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。</p>
<h2 id="7-3-2-正定核"><a href="#7-3-2-正定核" class="headerlink" title="7.3.2 正定核"></a>7.3.2 正定核</h2><p>通常所说的核函数就是<strong>正定核函数（positive definite kernel function）</strong>。</p>
<p>假设$K(x,z)$是定义在$\mathcal{X} \times \mathcal{X}$上的对称函数，并且对任意的$x_i,x_2,…,x_m \in \mathcal{X}$，$K(x,z)$关于$x_i,x_2,…,x_m$的$Gram$矩阵是半正定的。可以依据函数$K(x,z)$，构成一个希尔伯特空间(Hilbert space)，其步骤是：首先定义映射$\phi$并构成向量空间$\mathcal{S}$；然后在$\mathcal{S}$上定义内积构成内积空间；最后将$\mathcal{S}$完备化构成希尔伯特空间。</p>
<ol>
<li><strong>定义映射，构成向量空间$\mathcal{S}$</strong></li>
</ol>
<p>先定义映射<br>$$<br>\phi : x \longrightarrow K(·,x)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.69)<br>$$<br>根据这一映射，对任意$x_i \in \mathcal{X},\alpha_i \in R,i=1,2,…,m$，定义线性组合<br>$$<br>f(·)=\sum\limits_{i=1}^{m}\alpha_i K(·,x_i)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.70)<br>$$<br>考虑由线性组合为元素的集合$\mathcal{S}$。由于集合$\mathcal{S}$对加法和数乘运算是封闭的，所以$\mathcal{S}$构成一个向量空间。</p>
<ol start="2">
<li><strong>在$\mathcal{S}$上定义内积，使其成为内积空间</strong></li>
</ol>
<p>在$\mathcal{S}$上定义一个运算$*$：对任意$f,g \in \mathcal{S}$,<br>$$<br>f(·)=\sum\limits_{i=1}^{m}\alpha_i K(·,x_i)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.71)<br>$$</p>
<p>$$<br>g(·)=\sum\limits_{j=1}^{l}\beta_j K(·,z_j)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.72)<br>$$</p>
<p>定义运算$<em>$<br>$$<br>f * g = \sum\limits_{i=1}^{m}\sum\limits_{j=1}^{l}\alpha_i\beta_j K(x_i,z_j)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.73)<br>$$<br>赋予内积的向量空间为内积空间。因此$\mathcal{S}$是一个内积空间。既然$</em>$为$\mathcal{S}$的内积运算，那么仍然用$·$表示，即<br>$$<br>f · g = \sum\limits_{i=1}^{m}\sum\limits_{j=1}^{l}\alpha_i\beta_j K(x_i,z_j)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.74)<br>$$</p>
<ol start="3">
<li><strong>将内积空间</strong>$\mathcal{S}$<strong>完备化为希尔伯特空间</strong></li>
</ol>
<p>现将内积空间$\mathcal{S}$完备化。由式$(7.81)$定义的内积可以得到范数<br>$$<br>||f|| = \sqrt{f·f}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.82)<br>$$<br>因此，$\mathcal{S}$是一个赋范向量空间。根据泛函分析理论，对于不完备的赋范向量空间$\mathcal{S}$，一定可以使之完备化，得到完备的赋范向量空间$\mathcal{H}$。一个内积空间，当作为一个赋范向量空间是完备的时候，就是希尔伯特空间。这样，就得到了希尔伯特空间$\mathcal{H}$。</p>
<p>这一希尔伯特空间$\mathcal{H}$称为<strong>再生核希尔伯特空间（reproducing kernel Hilbert space，RKHS）</strong>。这是由于核$K$具有再生性，即满足<br>$$<br>K(·,x)·f = f(x)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.83)<br>$$<br>及<br>$$<br>K(·,x) · K(·,z) = K(x,z)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.84)<br>$$<br>称为再生核。</p>
<ol start="4">
<li><strong>正定核的充要条件</strong></li>
</ol>
<p><strong>定理 7.5（正定核的充要条件）</strong>  设$K:\mathcal{X} \times \mathcal{X} \longrightarrow R$是对称函数，则$K(x,z)$为正定核函数的充要条件是对任意$x_i \in \mathcal{X},i=1,2,…,m$，$K(x,z)$对应的$Gram$矩阵：<br>$$<br>K = [K(x_i,x_j)]_{m \times m}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.85)<br>$$<br>是半正定矩阵。</p>
<p><strong>定义 7.7（正定核的等价定义）</strong>  设$\mathcal{X} \subset R^n$，$K(x,z)$是定义在$\mathcal{X} \times \mathcal{X}$上的对称函数，如果对任意$x_i \in \mathcal{X},i=1,2,…,m$，$K(x,z)$对应的$Gram$矩阵<br>$$<br>K = [K(x_i,x_j)]_{m \times m}<br>$$<br>是半正定矩阵，则称$K(x,z)$是正定核。</p>
<h2 id="7-3-3-常用核函数"><a href="#7-3-3-常用核函数" class="headerlink" title="7.3.3 常用核函数"></a>7.3.3 常用核函数</h2><ol>
<li><strong>多项式核函数（polynomial kernel function）</strong></li>
</ol>
<p>$$<br>K(x,z) = (x·z + 1 )^p\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.88)<br>$$</p>
<p>对应的支持向量机是一个$p$次多项式分类器。在此情形下，分类决策函数成为<br>$$<br>f(x) = sign(\sum\limits_{i=1}^{N_\mathcal{S}}a_i^* y_i(x_i·x + 1)^p + b^*)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.89)<br>$$</p>
<ol start="2">
<li><strong>高斯核函数（Gaussian kernel function）</strong></li>
</ol>
<p>$$<br>K(x,z) = exp(-\frac{||x-z||^2}{2\sigma^2})\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.90)<br>$$</p>
<p>对应的支持向量机是<strong>高斯径向基函数（radial basis function）</strong>分类器。在此情形下，分类决策函数成为<br>$$<br>f(x) = sign(\sum\limits_{i=1}^{N_\mathcal{S}}a_i^* y_iexp(-\frac{||x-x_i||^2}{2\sigma^2}) + b^*)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.91)<br>$$</p>
<ol start="3">
<li><strong>字符串核函数（string kernel function）</strong></li>
</ol>
<p>核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上。</p>
<p>两个字符串$s$和$t$上的字符串核函数是基于映射$\phi$的特征空间中的内积：<br>$$<br>k_n(s,t) = \sum\limits_{u\in \sum^n}[\phi_n(s)]_u [\phi_n(t)]_u<br>$$</p>
<p>$$<br>= \sum\limits_{u\in \sum^n}\sum\limits_{(i,j):s(i)=t(j)=u}\lambda^{l(i)}\lambda^{l(j)}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.93)<br>$$</p>
<p>字符串核函数$k_n(s,t)$给出了字符串$s$和$t$中长度等于$n$的所有子串组成的特征向量的<strong>余弦相似度（cosine similarity）</strong>。直观上，两个字符串相同的子串越多，他们就越相似，字符串核函数的值就越大。</p>
<h2 id="7-3-4-非线性支持向量分类机"><a href="#7-3-4-非线性支持向量分类机" class="headerlink" title="7.3.4 非线性支持向量分类机"></a>7.3.4 非线性支持向量分类机</h2><p>将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数。</p>
<p><strong>定义 7.8（非线性支持向量机）</strong>  从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划$(7.95)$~$(7.97)$，学习得到的分类决策函数<br>$$<br>f(x) = sign(\sum\limits_{i=1}^{N}\alpha_i^* y_i K(x,x_i) + b^*)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.94)<br>$$<br>称为非线性支持向量机，$K(x,z)$是正定核函数。</p>
<hr>
<p>算法 7.4 （非线性支持向量机学习算法）</p>
<p><strong>输入</strong>：训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中，$x_i \in \chi = R^n, y \in \mathcal{Y} = {+1, -1}, i=1,2,…,N$；</p>
<p><strong>输出</strong>：分类决策函数。</p>
<p>​    （1）选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题<br>$$<br>\min\limits_{\alpha}\ \ \  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i \alpha_j y_i y_j K(x_i,x_j)\ - \ \sum\limits_{i=1}^{N}\alpha_i\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.95)<br>$$</p>
<p>$$<br>s.t. \ \ \ \ \sum\limits_{i=1}^{N}\alpha_iy_i = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.96)<br>$$</p>
<p>$$<br>0 \leq \alpha_i \leq C,\ \ \ i = 1,2,…,N\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (7.97)<br>$$</p>
<p>求得最优解$\alpha^* = (\alpha^*_1,\alpha^*_2,…,\alpha^*_N)^T$。</p>
<p>​    （2）选择$\alpha^*$的一个正分量$0&lt;\alpha_j^* &lt; C$，计算<br>$$<br>b^* = y_j - \sum\limits_{i=1}^{N}\alpha_i^* y_i K(x_i·x_j)<br>$$<br>​    （3）构造决策函数：<br>$$<br>f(x) = sign(\sum\limits_{i=1}^{N}\alpha_i^* y_i K(x,x_i) + b^*)<br>$$<br>当$K(x,z)$是正定核函数时，问题$(7.95)$~$(7.97)$是凸二次规划问题，解是存在的。</p>
<h1 id="7-4-序列最小最优化算法"><a href="#7-4-序列最小最优化算法" class="headerlink" title="7.4 序列最小最优化算法"></a>7.4 序列最小最优化算法</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/11/Python_Notes/%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/11/Python_Notes/%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E7%B1%BB/" class="post-title-link" itemprop="url">第九章 类</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-11 16:17:10 / 修改时间：16:16:32" itemprop="dateCreated datePublished" datetime="2021-06-11T16:17:10+08:00">2021-06-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">Python基础</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/11/Python_Notes/%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E7%B1%BB/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/11/Python_Notes/%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E7%B1%BB/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="9-1-创建和使用类"><a href="#9-1-创建和使用类" class="headerlink" title="9.1 创建和使用类"></a>9.1 创建和使用类</h1><p>使用类几乎可以模拟任何东西。</p>
<h2 id="9-1-1-创建一个类"><a href="#9-1-1-创建一个类" class="headerlink" title="9.1.1 创建一个类"></a>9.1.1 创建一个类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个类来模拟小狗&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sit</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时蹲下&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> is now sitting.&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roll_over</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时打滚&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> rolled over!&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>类中的函数称为<strong>方法</strong>。</li>
<li>通过实例访问的变量称为<strong>属性</strong>。</li>
</ul>
<p>**方法__init__()**：方法__init__()是一个特殊方法，每当根据类创建一个新实例时，Python都会自动运行它。在这个方法的定义中，形参self必不可少，而且必须位于其他形参的前面。每个与实例相关联的方法调用都会自动传递实参self，它是一个指向实例本身的引用，让实例能够访问类中的属性和方法。</p>
<p>注意：方法__init__() 前后有两个_。</p>
<h2 id="9-1-2-根据类创建实例"><a href="#9-1-2-根据类创建实例" class="headerlink" title="9.1.2 根据类创建实例"></a>9.1.2 根据类创建实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个类来模拟小狗&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sit</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时蹲下&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> is now sitting.&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roll_over</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时打滚&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> rolled over!&quot;</span>)</span><br><span class="line"> </span><br><span class="line">my_dog = Dog(<span class="string">&#x27;willie&#x27;</span>, <span class="number">6</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog`s name is <span class="subst">&#123;my_dog.name&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;My og is <span class="subst">&#123;my_dog.age&#125;</span> years old&quot;</span>)        </span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">My dog&#96;s name is willie</span><br><span class="line">My dog is 6 years old</span><br></pre></td></tr></table></figure>

<ol>
<li>访问属性</li>
</ol>
<p>要访问实例的属性，可使用句点表示法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my_dog.name</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>调用方法</li>
</ol>
<p>要调用方法，可指定实例的名称和要调用的方法，并用句点分隔。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_dog.sit()</span><br><span class="line">my_dog.roll_over()</span><br></pre></td></tr></table></figure>



<h1 id="9-2-使用类和实例"><a href="#9-2-使用类和实例" class="headerlink" title="9.2 使用类和实例"></a>9.2 使用类和实例</h1><p>可使用类来模拟现实世界中的很多情景。类编写好后，大部分时间将花在根据类创建的实例上。可以直接修改实例的属性，也可以编写方法以特定的方式进行修改。</p>
<h2 id="9-2-1-修改属性的值"><a href="#9-2-1-修改属性的值" class="headerlink" title="9.2.1 修改属性的值"></a>9.2.1 修改属性的值</h2><ol>
<li><p>直接修改属性的值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个类来模拟小狗&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sit</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时蹲下&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> is now sitting.&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roll_over</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时打滚&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> rolled over!&quot;</span>)</span><br><span class="line"> </span><br><span class="line">my_dog = Dog(<span class="string">&#x27;willie&#x27;</span>, <span class="number">6</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog`s name is <span class="subst">&#123;my_dog.name&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog is <span class="subst">&#123;my_dog.age&#125;</span> years old&quot;</span>)    </span><br><span class="line">my_dog.age = <span class="number">7</span>    <span class="comment">#直接修改属性的值</span></span><br><span class="line">print(<span class="string">f&quot;Now, My dog is <span class="subst">&#123;my_dog.age&#125;</span> years old&quot;</span>)   </span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">My dog&#96;s name is willie</span><br><span class="line">My dog is 6 years old</span><br><span class="line">Now, My dog is 7 years old</span><br></pre></td></tr></table></figure></li>
<li><p>通过方法修改属性的值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个类来模拟小狗&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sit</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时蹲下&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> is now sitting.&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roll_over</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时打滚&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> rolled over!&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_age</span>(<span class="params">self, add_value</span>):</span></span><br><span class="line">        self.age = self.age + add_value</span><br><span class="line">        </span><br><span class="line">my_dog = Dog(<span class="string">&#x27;willie&#x27;</span>, <span class="number">6</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog`s name is <span class="subst">&#123;my_dog.name&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog is <span class="subst">&#123;my_dog.age&#125;</span> years old&quot;</span>)    </span><br><span class="line">my_dog.add_age(<span class="number">2</span>) </span><br><span class="line">print(<span class="string">f&quot;Now, My dog is <span class="subst">&#123;my_dog.age&#125;</span> years old&quot;</span>)   </span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">My dog&#96;s name is willie</span><br><span class="line">My dog is 6 years old</span><br><span class="line">Now, My dog is 8 years old</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="9-3-继承"><a href="#9-3-继承" class="headerlink" title="9.3 继承"></a>9.3 继承</h1><p>编写类时，并非总是要从空白开始。如果要编写的类是另一个现成类的特殊版本，可使用继承。一个类继承另一个类时，将自动获得另一个类的所有属性和方法。原有的类称为父类，而新类称为子类。子类继承了父类的所有属性和方法，同时还可以定义自己的属性和方法。</p>
<h2 id="9-3-1-子类的方法-init"><a href="#9-3-1-子类的方法-init" class="headerlink" title="9.3.1 子类的方法__init__()"></a>9.3.1 子类的方法__init__()</h2><p>在既有类的基础上编写新类时，通常要调用父类的方法__init__()。这将初始化在父类__init__()方法中定义的所有属性。从而让子类包含这些属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个类来模拟小狗&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sit</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时蹲下&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> is now sitting.&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roll_over</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时打滚&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> rolled over!&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_age</span>(<span class="params">self, add_value</span>):</span></span><br><span class="line">        self.age = self.age + add_value</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeaDog</span>(<span class="params">Dog</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(name, age)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">my_sea_dog = SeaDog(<span class="string">&#x27;sea_willie&#x27;</span>, <span class="number">10</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog`s name is <span class="subst">&#123;my_sea_dog.name&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog is <span class="subst">&#123;my_sea_dog.age&#125;</span> years old&quot;</span>)    </span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">My dog&#96;s name is sea_willie</span><br><span class="line">My dog is 10 years old</span><br></pre></td></tr></table></figure>

<p>super()是一个特殊函数，能够调用父类的方法。父类也称为<strong>超类</strong>（superclass），名称super由此而来。</p>
<h2 id="9-3-2-给子类定义属性和方法"><a href="#9-3-2-给子类定义属性和方法" class="headerlink" title="9.3.2 给子类定义属性和方法"></a>9.3.2 给子类定义属性和方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个类来模拟小狗&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sit</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时蹲下&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> is now sitting.&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roll_over</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时打滚&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> rolled over!&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_age</span>(<span class="params">self, add_value</span>):</span></span><br><span class="line">        self.age = self.age + add_value</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeaDog</span>(<span class="params">Dog</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(name, age)</span><br><span class="line">        self.skill = <span class="string">&#x27;swim&#x27;</span> <span class="comment">#定义子类自己的属性</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">describe_sea_dog</span>(<span class="params">self</span>):</span> <span class="comment">#定义子类自己的方法</span></span><br><span class="line">        print(<span class="string">f&quot;sea dog can <span class="subst">&#123;self.skill&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">my_sea_dog = SeaDog(<span class="string">&#x27;sea_willie&#x27;</span>, <span class="number">10</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog`s name is <span class="subst">&#123;my_sea_dog.name&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog is <span class="subst">&#123;my_sea_dog.age&#125;</span> years old&quot;</span>)   </span><br><span class="line">my_sea_dog.describe_sea_dog() </span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">My dog&#96;s name is sea_willie</span><br><span class="line">My dog is 10 years old</span><br><span class="line">sea dog can swim</span><br></pre></td></tr></table></figure>

<h2 id="9-3-3-重写父类的方法"><a href="#9-3-3-重写父类的方法" class="headerlink" title="9.3.3 重写父类的方法"></a>9.3.3 重写父类的方法</h2><p>对于父类的方法，只要它不符合子类模拟的实物行为，都可以进行重写。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个类来模拟小狗&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sit</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时蹲下&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> is now sitting.&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roll_over</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;模拟小狗收到命令时打滚&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">f&quot;<span class="subst">&#123;self.name&#125;</span> rolled over!&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_age</span>(<span class="params">self, add_value</span>):</span></span><br><span class="line">        self.age = self.age + add_value</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeaDog</span>(<span class="params">Dog</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(name, age)</span><br><span class="line">        self.skill = <span class="string">&#x27;swim&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">describe_sea_dog</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">f&quot;sea dog can <span class="subst">&#123;self.skill&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roll_over</span>(<span class="params">self</span>):</span><span class="comment"># 重写父类的打滚方法</span></span><br><span class="line">        print(<span class="string">&quot;sea dog can&#x27;t roll over!&quot;</span>)</span><br><span class="line">        </span><br><span class="line">my_sea_dog = SeaDog(<span class="string">&#x27;sea_willie&#x27;</span>, <span class="number">10</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog`s name is <span class="subst">&#123;my_sea_dog.name&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;My dog is <span class="subst">&#123;my_sea_dog.age&#125;</span> years old&quot;</span>)   </span><br><span class="line">my_sea_dog.describe_sea_dog() </span><br><span class="line">my_sea_dog.roll_over()</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">My dog&#96;s name is sea_willie</span><br><span class="line">My dog is 10 years old</span><br><span class="line">sea dog can swim</span><br><span class="line">sea dog can&#39;t roll over!</span><br></pre></td></tr></table></figure>

<h2 id="9-3-4-将实例用作属性"><a href="#9-3-4-将实例用作属性" class="headerlink" title="9.3.4 将实例用作属性"></a>9.3.4 将实例用作属性</h2><p>使用代码模拟实物时，可能会发现自己给类添加的细节越来越多：属性和方法清单以及文件都越来越长。在这种情况下，可能需要将类的一部分提取出来，作为一个单独的类。可以将大型类拆分成多个协同工作得小类。</p>
<h1 id="9-4-导入类"><a href="#9-4-导入类" class="headerlink" title="9.4 导入类"></a>9.4 导入类</h1><p>随着不断给类添加功能，文件可能变得很长，即便妥善地使用了继承亦如此。为遵循Python的总体理念，应让文件尽可能整洁。Python在这方面提供了帮助，允许将类存储在模块中，然后在主程序中导入所需的模块。</p>
<h2 id="9-4-1-导入单个类"><a href="#9-4-1-导入单个类" class="headerlink" title="9.4.1 导入单个类"></a>9.4.1 导入单个类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> ClassName</span><br></pre></td></tr></table></figure>

<h2 id="9-4-2从一个模块中导入多个类"><a href="#9-4-2从一个模块中导入多个类" class="headerlink" title="9.4.2从一个模块中导入多个类"></a>9.4.2从一个模块中导入多个类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> ClassName1, ClassName2, ClassName3</span><br></pre></td></tr></table></figure>

<h2 id="9-4-3-导入整个模块"><a href="#9-4-3-导入整个模块" class="headerlink" title="9.4.3 导入整个模块"></a>9.4.3 导入整个模块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> module_name</span><br></pre></td></tr></table></figure>

<h2 id="9-4-4-导入模块中的所有类"><a href="#9-4-4-导入模块中的所有类" class="headerlink" title="9.4.4 导入模块中的所有类"></a>9.4.4 导入模块中的所有类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>

<h2 id="9-4-5-使用别名"><a href="#9-4-5-使用别名" class="headerlink" title="9.4.5 使用别名"></a>9.4.5 使用别名</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> ClassName <span class="keyword">as</span> CN</span><br></pre></td></tr></table></figure>



<h1 id="9-5-Python标准库"><a href="#9-5-Python标准库" class="headerlink" title="9.5 Python标准库"></a>9.5 Python标准库</h1><p>Python标准库是一组模块，安装的Python都包含它。</p>
<p>模块random演示</p>
<p>在这个模块中，一个有趣的函数是randint()。它将两个整数作为参数，并随机返回一个位于这两个整数之间（含）的整数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    print(randint(<span class="number">1</span>, <span class="number">6</span>))</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">6</span><br><span class="line">5</span><br><span class="line">2</span><br><span class="line">6</span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<p>另一个有用的函数是choic()。它将一个列表或元组作为参数，并随机返回其中的一个元素：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> choice</span><br><span class="line">numbers = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    print(choice(numbers))</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">5</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">1</span><br><span class="line">1</span><br></pre></td></tr></table></figure>



<h1 id="9-6-类编码风格"><a href="#9-6-类编码风格" class="headerlink" title="9.6 类编码风格"></a>9.6 类编码风格</h1><p>类名应采用驼峰命名法，即将类名中的每个单词的首字母都大写，而不使用下划线。实例命名和模块命名都采用小写格式，并在单词之间加上下划线。</p>
<p>在类中，可使用一个空行来分隔方法；而在模块中，可使用两个空行来分隔类。</p>
<p>需要同时导入标准库中的模块和编写的模块时，先编写导入标准库模块的语句，再添加一个空行，然后编写导入自己编写的模块语句。在包含多条import语句的程序中，这种做法让人更容易明白程序使用的各个模块都来自何处。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/10/Python_Notes/%E7%AC%AC%E5%85%AB%E7%AB%A0-%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/10/Python_Notes/%E7%AC%AC%E5%85%AB%E7%AB%A0-%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">第八章 函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-10 20:24:46 / 修改时间：20:24:03" itemprop="dateCreated datePublished" datetime="2021-06-10T20:24:46+08:00">2021-06-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">Python基础</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/10/Python_Notes/%E7%AC%AC%E5%85%AB%E7%AB%A0-%E5%87%BD%E6%95%B0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/10/Python_Notes/%E7%AC%AC%E5%85%AB%E7%AB%A0-%E5%87%BD%E6%95%B0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="8-1-定义函数"><a href="#8-1-定义函数" class="headerlink" title="8.1 定义函数"></a>8.1 定义函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greet_user</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示简单的问候语。&quot;&quot;&quot;</span></span><br><span class="line">    print(<span class="string">&quot;Hello!&quot;</span>)</span><br><span class="line"></span><br><span class="line">greet_user()</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello!</span><br></pre></td></tr></table></figure>

<p>使用关键字def来告诉Python，要定义一个函数。这是函数定义，向Python指出了函数名，还可能在圆括号内指出函数为完成任务需要什么样的信息。</p>
<p>用三对引号括起来的文本称为<strong>文档字符串（docstring）</strong>的注释，描述了函数是做什么的。</p>
<h2 id="8-1-1-向函数传递信息"><a href="#8-1-1-向函数传递信息" class="headerlink" title="8.1.1 向函数传递信息"></a>8.1.1 向函数传递信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greet_user</span>(<span class="params">username</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示简单的问候语&quot;&quot;&quot;</span></span><br><span class="line">    print(<span class="string">f&quot;Hello,<span class="subst">&#123;username&#125;</span>!&quot;</span>)</span><br><span class="line"></span><br><span class="line">greet_user(<span class="string">&quot;zhangsan&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello,zhangsan!</span><br></pre></td></tr></table></figure>

<h2 id="8-1-2-实参和形参"><a href="#8-1-2-实参和形参" class="headerlink" title="8.1.2 实参和形参"></a>8.1.2 实参和形参</h2><p>在函数greet_user()的定义中，变量username是一个<strong>形参（parameter）</strong>，即函数完成工作所需的信息。在代码greet_user(“zhangsan”)中，值“zhangsan”是一个<strong>实参（argument）</strong>，即调用函数时传递给函数的信息。</p>
<h1 id="8-2-传递实参"><a href="#8-2-传递实参" class="headerlink" title="8.2 传递实参"></a>8.2 传递实参</h1><p>向函数传递实参的方式很多：</p>
<ul>
<li>使用<strong>位置实参</strong>，这要求实参的顺序与形参的顺序相同；</li>
<li>使用<strong>关键字实参</strong>，其中每个实参都由变量名和值组成；</li>
<li>使用<strong>列表</strong>和<strong>字典</strong>。</li>
</ul>
<h2 id="8-2-1-位置实参"><a href="#8-2-1-位置实参" class="headerlink" title="8.2.1 位置实参"></a>8.2.1 位置实参</h2><p>调用函数时，Python必须将函数调用中的每个实参都关联到函数定义中的一个形参。为此，最简单的关联方式就是基于实参的顺序。这种关联方式称为<strong>位置实参</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describe_person</span>(<span class="params">person_name, person_age</span>):</span></span><br><span class="line">    print(<span class="string">f&quot;my name is <span class="subst">&#123;person_name&#125;</span>,i am <span class="subst">&#123;person_age&#125;</span> yesrs old~&quot;</span>)</span><br><span class="line">  </span><br><span class="line">describe_person(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">24</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my name is zhangsan,i am 24 yesrs old~</span><br></pre></td></tr></table></figure>

<ol>
<li><p>多次调用函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describe_person</span>(<span class="params">person_name, person_age</span>):</span></span><br><span class="line">    print(<span class="string">f&quot;my name is <span class="subst">&#123;person_name&#125;</span>,i am <span class="subst">&#123;person_age&#125;</span> yesrs old~&quot;</span>)</span><br><span class="line">  </span><br><span class="line">describe_person(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">24</span>)</span><br><span class="line">describe_person(<span class="string">&quot;lisi&quot;</span>, <span class="number">30</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my name is zhangsan,i am 24 yesrs old~</span><br><span class="line">my name is lisi,i am 30 yesrs old~</span><br></pre></td></tr></table></figure></li>
<li><p>位置实参的顺序很重要</p>
<p>使用位置实参来调用函数时，如果实参的顺序不正确，结果可能出乎意料：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describe_person</span>(<span class="params">person_name, person_sex</span>):</span></span><br><span class="line">    print(<span class="string">f&quot;my name is <span class="subst">&#123;person_name&#125;</span>,i am <span class="subst">&#123;person_sex&#125;</span>&quot;</span>)</span><br><span class="line">  </span><br><span class="line">describe_person(<span class="string">&quot;male&quot;</span>, <span class="string">&quot;zhangsan&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my name is male,i am zhangsan</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="8-2-2-关键字实参"><a href="#8-2-2-关键字实参" class="headerlink" title="8.2.2 关键字实参"></a>8.2.2 关键字实参</h2><p><strong>关键字实参</strong>是传递给函数的名称值对。因为直接在实参中将名称和值关联起来，所以向函数传递实参时不会混淆。关键字实参让你必须考虑函数调用中的实参顺序，还清楚地指出了函数调用中各个值的用途。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describe_person</span>(<span class="params">person_name, person_sex</span>):</span></span><br><span class="line">    print(<span class="string">f&quot;my name is <span class="subst">&#123;person_name&#125;</span>,i am <span class="subst">&#123;person_sex&#125;</span>&quot;</span>)</span><br><span class="line">  </span><br><span class="line">describe_person(person_name = <span class="string">&#x27;zhangsan&#x27;</span>, person_sex = <span class="string">&#x27;male&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my name is zhangsan,i am male</span><br></pre></td></tr></table></figure>

<p>注意：使用关键字实参时，务必准确指定函数定义中的形参名。</p>
<h2 id="8-2-3-默认值"><a href="#8-2-3-默认值" class="headerlink" title="8.2.3 默认值"></a>8.2.3 默认值</h2><p>编写函数时，可给每个形参指定<strong>默认值</strong>。在调用函数中给形参提供了实参时，Python将使用指定的实参数；否则，将使用形参的默认值。因此形参指定默认值后，可在函数调用中省略相应的实参。使用默认值可简化函数调用，还可清楚第指出函数的典型用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describe_person</span>(<span class="params">person_name, person_sex = <span class="string">&#x27;male&#x27;</span></span>):</span></span><br><span class="line">    print(<span class="string">f&quot;my name is <span class="subst">&#123;person_name&#125;</span>,i am <span class="subst">&#123;person_sex&#125;</span>&quot;</span>)</span><br><span class="line">  </span><br><span class="line">describe_person(person_name = <span class="string">&#x27;zhangsan&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my name is zhangsan,i am male</span><br></pre></td></tr></table></figure>

<p>使用默认值时，必须先在形参列表中列出没有默认值的形参，再列出有默认值的形参。这让Python依然能够正确地解读位置实参。</p>
<h2 id="8-2-4-等效的函数调用"><a href="#8-2-4-等效的函数调用" class="headerlink" title="8.2.4 等效的函数调用"></a>8.2.4 等效的函数调用</h2><p>鉴于可混合使用位置实参，关键字实参和默认值，通常有多种等效的函数调用方式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describe_person</span>(<span class="params">person_name, person_sex = <span class="string">&#x27;male&#x27;</span></span>):</span></span><br><span class="line">    print(<span class="string">f&quot;my name is <span class="subst">&#123;person_name&#125;</span>,i am <span class="subst">&#123;person_sex&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">describe_person(<span class="string">&quot;zhangsan&quot;</span>)</span><br><span class="line">describe_person(person_name = <span class="string">&quot;lisi&quot;</span>)</span><br><span class="line">describe_person(<span class="string">&quot;wangwu&quot;</span>,<span class="string">&quot;female&quot;</span>)</span><br><span class="line">describe_person(person_name = <span class="string">&quot;zhaoliu&quot;</span>, person_sex = <span class="string">&#x27;female&#x27;</span>)</span><br><span class="line">describe_person(person_sex = <span class="string">&#x27;female&#x27;</span>, person_name = <span class="string">&quot;liuqi&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my name is zhangsan,i am male</span><br><span class="line">my name is lisi,i am male</span><br><span class="line">my name is wangwu,i am female</span><br><span class="line">my name is zhaoliu,i am female</span><br><span class="line">my name is liuqi,i am female</span><br></pre></td></tr></table></figure>



<h1 id="8-3-返回值"><a href="#8-3-返回值" class="headerlink" title="8.3 返回值"></a>8.3 返回值</h1><p>函数并非总是直接显示输出，它还可以处理一些数据，并返回一个或一组值。函数返回的值称为<strong>返回值</strong>。</p>
<h2 id="8-3-1-返回简单值"><a href="#8-3-1-返回简单值" class="headerlink" title="8.3.1 返回简单值"></a>8.3.1 返回简单值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_formatted_name</span>(<span class="params">first_name, last_name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回整洁的姓名&quot;&quot;&quot;</span></span><br><span class="line">    full_name = <span class="string">f&quot;<span class="subst">&#123;first_name&#125;</span> <span class="subst">&#123;last_name&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">return</span> full_name.title()</span><br><span class="line"></span><br><span class="line">full_person_name = get_formatted_name(<span class="string">&#x27;zhang&#x27;</span>, <span class="string">&#x27;san&#x27;</span>)</span><br><span class="line">print(full_person_name)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Zhang San</span><br></pre></td></tr></table></figure>

<h2 id="8-3-3-返回字典"><a href="#8-3-3-返回字典" class="headerlink" title="8.3.3 返回字典"></a>8.3.3 返回字典</h2><p>函数可返回任何类型的值，包括列表和字典等较复杂的数据结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_person</span>(<span class="params">first_name, last_name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回一个字典，其中包含有关一个人的信息。&quot;&quot;&quot;</span></span><br><span class="line">    person = &#123;<span class="string">&#x27;first&#x27;</span> : first_name, <span class="string">&#x27;last&#x27;</span> : last_name&#125;</span><br><span class="line">    <span class="keyword">return</span> person</span><br><span class="line"></span><br><span class="line">full_person = build_person(<span class="string">&#x27;zhang&#x27;</span>, <span class="string">&#x27;san&#x27;</span>)</span><br><span class="line">print(full_person)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;first&#39;: &#39;zhang&#39;, &#39;last&#39;: &#39;san&#39;&#125;</span><br></pre></td></tr></table></figure>

<p>在条件测试中，None相当于False。</p>
<h1 id="8-4-传递列表"><a href="#8-4-传递列表" class="headerlink" title="8.4 传递列表"></a>8.4 传递列表</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greet_users</span>(<span class="params">names</span>):</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> names:</span><br><span class="line">        print(<span class="string">f&quot;Hello <span class="subst">&#123;name&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">user_names = [<span class="string">&#x27;zhangsan&#x27;</span>, <span class="string">&#x27;lisi&#x27;</span>, <span class="string">&#x27;wangwu&#x27;</span>]</span><br><span class="line">greet_users(user_names)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Hello zhangsan</span><br><span class="line">Hello lisi</span><br><span class="line">Hello wangwu</span><br></pre></td></tr></table></figure>

<h2 id="8-4-1-在函数中修改列表"><a href="#8-4-1-在函数中修改列表" class="headerlink" title="8.4.1 在函数中修改列表"></a>8.4.1 在函数中修改列表</h2><p>将列表传递给函数后，函数就可对其进行修改。在函数中对这个列表所做的任何修改都是永久性的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greet_users</span>(<span class="params">names</span>):</span></span><br><span class="line">   names.append(<span class="string">&#x27;zhaoliu&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">user_names = [<span class="string">&#x27;zhangsan&#x27;</span>, <span class="string">&#x27;lisi&#x27;</span>, <span class="string">&#x27;wangwu&#x27;</span>]</span><br><span class="line">print(user_names)</span><br><span class="line">greet_users(user_names)</span><br><span class="line">print(user_names)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#39;zhangsan&#39;, &#39;lisi&#39;, &#39;wangwu&#39;]</span><br><span class="line">[&#39;zhangsan&#39;, &#39;lisi&#39;, &#39;wangwu&#39;, &#39;zhaoliu&#39;]</span><br></pre></td></tr></table></figure>

<h2 id="8-4-2-禁止函数修改列表"><a href="#8-4-2-禁止函数修改列表" class="headerlink" title="8.4.2 禁止函数修改列表"></a>8.4.2 禁止函数修改列表</h2><p>如果想在函数中操作列表而不影响原列表，可将列表的副本传递给函数，可以这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">function_name(list_name[:])</span><br></pre></td></tr></table></figure>

<p>切片表示法[:]创建列表的副本。</p>
<p>虽然向函数传递列表的副本可保留原始列表的内容，但除非有充分的理由，否则还是应该将原始列表传递给函数。这是因为让函数使用现成的列表可以避免花时间和内存创建副本，从而提高效率，在处理大型列表时尤其如此。</p>
<h1 id="8-5-传递任意数量的实参"><a href="#8-5-传递任意数量的实参" class="headerlink" title="8.5 传递任意数量的实参"></a>8.5 传递任意数量的实参</h1><p>有时候，预先不知道函数需要接受多少个实参，好在Python允许函数从调用语句中收集任意数量的实参。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greet_users</span>(<span class="params">*names</span>):</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> names:</span><br><span class="line">        print(<span class="string">f&quot;Hello <span class="subst">&#123;name&#125;</span>&quot;</span>)</span><br><span class="line">    print(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">greet_users(<span class="string">&#x27;zhangsan&#x27;</span>)</span><br><span class="line">greet_users(<span class="string">&#x27;zhangsan&#x27;</span>, <span class="string">&#x27;lisi&#x27;</span>)</span><br><span class="line">greet_users(<span class="string">&#x27;zhangsan&#x27;</span>, <span class="string">&#x27;lisi&#x27;</span>, <span class="string">&#x27;wangwu&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Hello zhangsan</span><br><span class="line"></span><br><span class="line">Hello zhangsan</span><br><span class="line">Hello lisi</span><br><span class="line"></span><br><span class="line">Hello zhangsan</span><br><span class="line">Hello lisi</span><br><span class="line">Hello wangwu</span><br></pre></td></tr></table></figure>

<p>形参名*names中的星号让Python创建一个名为names的空元组，并将收到的所有值都封装到这个元组中。</p>
<h2 id="8-5-1-结合使用位置实参和任意数量实参"><a href="#8-5-1-结合使用位置实参和任意数量实参" class="headerlink" title="8.5.1 结合使用位置实参和任意数量实参"></a>8.5.1 结合使用位置实参和任意数量实参</h2><p>如果要让函数接受不同类型的实参，必须在函数定义中将接纳任意数量实参的形参放在最后。Python先匹配位置实参和关键字实参，再将余下的实参都收集到最后一个形参中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_pizza</span>(<span class="params">size, *toppings</span>):</span></span><br><span class="line">    print(<span class="string">f&quot;\nMaking a <span class="subst">&#123;size&#125;</span>-inch pizza with the following topping:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> topping <span class="keyword">in</span> toppings:</span><br><span class="line">        print(<span class="string">f&quot;-<span class="subst">&#123;topping&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">make_pizza(<span class="number">16</span>, <span class="string">&#x27;pepperoni&#x27;</span>)</span><br><span class="line">make_pizza(<span class="number">12</span>, <span class="string">&#x27;mushrooms&#x27;</span>, <span class="string">&#x27;green peppers&#x27;</span>, <span class="string">&#x27;extra cheese&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为;l</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Making a 16-inch pizza with the following topping:</span><br><span class="line">-pepperoni</span><br><span class="line"></span><br><span class="line">Making a 12-inch pizza with the following topping:</span><br><span class="line">-mushrooms</span><br><span class="line">-green peppers</span><br><span class="line">-extra cheese</span><br></pre></td></tr></table></figure>

<p>经常i看到通用形参名*args，它也收集任意数量的位置实参。</p>
<h2 id="8-5-2-使用任意数量的关键字实参"><a href="#8-5-2-使用任意数量的关键字实参" class="headerlink" title="8.5.2 使用任意数量的关键字实参"></a>8.5.2 使用任意数量的关键字实参</h2><p>有时候，需要接受任意数量的实参，但预先不知道传递给函数的会是什么样的信息。在这种情况下，可将函数编写成能够接受任意数量的键值对——调用语句提供了多少就接受多少。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_profile</span>(<span class="params">first, last, **user_info</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个字典，其中包含我们知道的有关用户的一切。&quot;&quot;&quot;</span></span><br><span class="line">    user_info[<span class="string">&#x27;first_name&#x27;</span>] = first</span><br><span class="line">    user_info[<span class="string">&#x27;last_name&#x27;</span>] = last</span><br><span class="line">    <span class="keyword">return</span> user_info</span><br><span class="line"></span><br><span class="line">user_profile = build_profile(<span class="string">&#x27;zhang&#x27;</span>, <span class="string">&#x27;san&#x27;</span>, location=<span class="string">&#x27;chongqing&#x27;</span>, field=<span class="string">&#x27;computer science&#x27;</span>)</span><br><span class="line">print(user_profile)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;location&#39;: &#39;chongqing&#39;, &#39;field&#39;: &#39;computer science&#39;, &#39;first_name&#39;: &#39;zhang&#39;, &#39;last_name&#39;: &#39;san&#39;&#125;</span><br></pre></td></tr></table></figure>

<p>形参**user_info中的两个星号让Python创建一个名为user_info的空字典，并将接收到的所有名称值对都放到这个字典中。</p>
<p>经常会看到形参名**kwargs，它用于收集任意数量的关键字实参。</p>
<h1 id="8-6-将函数存储在模块中"><a href="#8-6-将函数存储在模块中" class="headerlink" title="8.6 将函数存储在模块中"></a>8.6 将函数存储在模块中</h1><p>使用函数的优点之一是可将代码块与主程序分离。通过给函数指定描述性名称，可让主程序容易理解得多。还可以更进一步，将函数存储在称为<strong>模块</strong>的独立文件中，再将模块<strong>导入</strong>到主程序中。</p>
<h2 id="8-6-1-导入整个模块"><a href="#8-6-1-导入整个模块" class="headerlink" title="8.6.1  导入整个模块"></a>8.6.1  导入整个模块</h2><p>模块是扩展名为.py的文件，包含要导入到程序中的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> module_name</span><br></pre></td></tr></table></figure>

<p>只需要编写一条import语句并在其中指定模块名，就可在程序中使用该模块中的所有函数。如果使用这种import语句导入了名为module_name.py的整个模块，就可以使用下面的语法来使用其中的任何一个函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">module_name.function_name()</span><br></pre></td></tr></table></figure>

<h2 id="8-6-2-导入特定的函数"><a href="#8-6-2-导入特定的函数" class="headerlink" title="8.6.2 导入特定的函数"></a>8.6.2 导入特定的函数</h2><p>还可以导入模块中的特定函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> function_name</span><br></pre></td></tr></table></figure>

<p>通过用逗号分隔函数名，可根据需要从模块中导入任意数量的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> function_0, function_1, function_2 </span><br></pre></td></tr></table></figure>

<p>由于在import语句中显示地导入了函数，调用时只需指定其名称即可。</p>
<h2 id="8-6-3-使用as给函数指定别名"><a href="#8-6-3-使用as给函数指定别名" class="headerlink" title="8.6.3 使用as给函数指定别名"></a>8.6.3 使用as给函数指定别名</h2><p>如果要导入函数的名称可能与程序中现有的名称冲突，或者函数的名称太长，可指定简短而独一无二的别名：函数的另一个名称，类似于外号。要给函数取这种特殊外号，需要在导入它时指定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> function_name <span class="keyword">as</span> fn</span><br></pre></td></tr></table></figure>

<h2 id="8-6-4-使用as给模块指定别名"><a href="#8-6-4-使用as给模块指定别名" class="headerlink" title="8.6.4 使用as给模块指定别名"></a>8.6.4 使用as给模块指定别名</h2><p>还可以给模块指定别名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> module_name <span class="keyword">as</span> mn</span><br></pre></td></tr></table></figure>

<h2 id="8-6-5-导入模块中的所有函数"><a href="#8-6-5-导入模块中的所有函数" class="headerlink" title="8.6.5 导入模块中的所有函数"></a>8.6.5 导入模块中的所有函数</h2><p>使用星号（*）运算符可让Python导入模块中的所有函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from module_name import *</span><br></pre></td></tr></table></figure>



<h1 id="8-7-函数编写指南"><a href="#8-7-函数编写指南" class="headerlink" title="8.7 函数编写指南"></a>8.7 函数编写指南</h1><p>应给函数指定描述性名称，且只在其中使用小写字母和下划线。</p>
<p>给形参指定默认值时，等号两边不要有空格。对于函数调用中的关键字实参，也应遵循这种约定。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/10/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/10/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">第六章 逻辑斯蒂回归与最大熵模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-10 20:24:36" itemprop="dateCreated datePublished" datetime="2021-06-10T20:24:36+08:00">2021-06-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-09 20:09:16" itemprop="dateModified" datetime="2021-06-09T20:09:16+08:00">2021-06-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/10/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/10/Statistical_Learning_Methods_Notes/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>逻辑斯蒂回归（logistic regression）</strong>是统计学习中的经典分类方法。</p>
<p>最大熵是概率模型学习的一个准则，将其推广到分类问题得到<strong>最大熵模型（maximum entropy model）</strong>。</p>
<p>逻辑斯蒂回归模型与最大熵模型都属于对数线性模型。</p>
<h1 id="6-1-逻辑斯蒂回归模型"><a href="#6-1-逻辑斯蒂回归模型" class="headerlink" title="6.1 逻辑斯蒂回归模型"></a>6.1 逻辑斯蒂回归模型</h1><h2 id="6-1-1-逻辑斯蒂分布"><a href="#6-1-1-逻辑斯蒂分布" class="headerlink" title="6.1.1 逻辑斯蒂分布"></a>6.1.1 逻辑斯蒂分布</h2><p>首先介绍<strong>逻辑斯蒂分布（logistic distribution）</strong>。</p>
<p><strong>定义 6.1（逻辑斯蒂分布）</strong>  设$X$是连续随机变量，$X$服从逻辑斯蒂分布是指$X$具有下列分布函数和密度函数：<br>$$<br>F(x) = P(X \leq x) = \frac{1}{1 \ + \ e^{-(x - \mu)/\gamma}} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.1)<br>$$</p>
<p>$$<br>f(x) = F’(x) = \frac{e^{-(x - \mu)/\gamma}}{\gamma(1 \ + \ e^{-(x - \mu)/\gamma})^2} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.2)<br>$$</p>
<p>式中，$\mu$为位置参数，$\gamma &gt; 0$为形状参数。</p>
<p>分布函数属于逻辑斯蒂函数，其图形是一条$S$形曲线（sigmoid curve）。该曲线以点$(\mu,\frac{1}{2})$为中心对称，即满足<br>$$<br>F(-x \ + \ \mu) - \frac{1}{2} = -F(x \ + \ \mu) + \frac{1}{2}<br>$$<br>形状参数$\gamma$的值越小，曲线在中心附近增长得越快。</p>
<h2 id="6-1-2-二项逻辑斯蒂回归模型"><a href="#6-1-2-二项逻辑斯蒂回归模型" class="headerlink" title="6.1.2 二项逻辑斯蒂回归模型"></a>6.1.2 二项逻辑斯蒂回归模型</h2><p><strong>二项逻辑斯蒂回归模型（binomial logistic regression model）</strong>是一种分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的逻辑斯蒂分布。</p>
<p><strong>定义 6.2（逻辑斯蒂回归模型）</strong>  二项逻辑斯蒂回归模型是如下的条件概率分布：<br>$$<br>P(Y=1|x) = \frac{\exp(\omega·x + b)}{1 \ + \ \exp(\omega·x + b)} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.3)<br>$$</p>
<p>$$<br>P(Y=0|x) = \frac{1}{1 \ + \ \exp(\omega·x + b)} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.4)<br>$$</p>
<p>这里，$x \in R^n$是输入，$Y \in {0,1}$是输出，$\omega \in R^n $和$b \in R$是参数，$\omega$称为权值向量，$b$称为偏置，$\omega·x$为$\omega$和$x$的内积。</p>
<p>对于给定的输入实例$x$，逻辑斯蒂回归比较两个条件概率值的大小，将实例$x$分到概率值较大的那一类。</p>
<p>有时为了方便，将权值向量和输入向量加以扩充，仍记作$\omega,x$，即$\omega = (\omega^{(1)},\omega^{(2)},…,\omega^{(n)},b)^T,\ x = (x^{(1)},x^{(2)},…,x^{(n)},1)$。这时，逻辑斯蒂回归模型如下：<br>$$<br>P(Y=1|x) = \frac{\exp(\omega·x)}{1 \ + \ \exp(\omega·x)} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.5)<br>$$</p>
<p>$$<br>P(Y=0|x) = \frac{1}{1 \ + \ \exp(\omega·x)} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.6)<br>$$</p>
<hr>
<p>一个事件的<strong>几率（odds）</strong>是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率是$\frac{p}{1-p}$，该事件的<strong>对数几率（log odds）</strong>或<strong>logit函数</strong>是<br>$$<br>logit(p) = \log\frac{p}{1-p}<br>$$<br>对于逻辑斯蒂回归而言，由式$(6.5)$与式$(6.6)$得<br>$$<br>\log\frac{P(Y=1|x)}{1\ - \ P(Y=1|x)} = \omega ·x<br>$$<br>这就是说，在逻辑斯蒂回归模型中，输出$Y=1$的对数几率是输入$x$的线性函数。或者说，输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯蒂回归模型。</p>
<h2 id="6-1-3-模型参数估计"><a href="#6-1-3-模型参数估计" class="headerlink" title="6.1.3 模型参数估计"></a>6.1.3 模型参数估计</h2><p>逻辑斯蒂回归模型学习时，对于给定的训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中，$x_i \in R^n, y_i \in{0,1} $，可以应用极大似然估计法估计模型参数，从而得到逻辑斯蒂回归模型。</p>
<p>设：<br>$$<br>P(Y=1|x) = \pi(x),\ \ \  P(Y=0|x) = 1 - \pi(x)<br>$$<br>似然函数为<br>$$<br>\prod\limits_{i=1}^{N}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}<br>$$<br>对数似然函数为<br>$$<br>L(\omega) = \sum\limits_{i=1}^{N}[y_i \log \pi(x_i)\  + \ (1-y_i)\log(1-\pi(x_i))]<br>$$</p>
<p>$$<br>\ \ \ \ =\sum\limits_{i=1}^{N}[y_i \log \frac{\pi(x_i)}{1-\pi(x_i)}\  + \ \log(1-\pi(x_i))]<br>$$</p>
<p>$$<br>\ \ =\sum\limits_{i=1}^{N}[y_i (\omega·x_i)\  - \ \log(1+\exp(\omega · x_i))]<br>$$</p>
<p>对$L(\omega)$求极大值，得到$\omega$的估计值。</p>
<p>这样，问题就变成了以对数似然函数为目标函数的最优化问题。逻辑斯蒂回归学习中通常采用的方法是梯度下降法及拟牛顿法。</p>
<h2 id="6-1-4-多项逻辑斯蒂回归"><a href="#6-1-4-多项逻辑斯蒂回归" class="headerlink" title="6.1.4 多项逻辑斯蒂回归"></a>6.1.4 多项逻辑斯蒂回归</h2><p>上面介绍的逻辑斯蒂回归模型是二项分类模型，用于二类分类。可以将其推广为<strong>多项逻辑斯蒂回归模型（multi-nominal logistic regression model）</strong>，用于多类分类。</p>
<p>设离散型随机变量$Y$的取值集合是${1,2,…,K}$，那么多项逻辑斯蒂回归模型是<br>$$<br>P(Y=k|x) = \frac{\exp(\omega_k·x)}{1 \ + \ \sum\limits_{k=1}^{K-1}\exp(\omega_k·x)}, \ \ k = 1,2,…,K-1 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.7)<br>$$</p>
<p>$$<br>P(Y=K|x) = \frac{1}{1 \ + \ \sum\limits_{k=1}^{K-1}\exp(\omega_k·x)}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.8)<br>$$</p>
<p>这里，$x \in R^{n+1},\omega_k \in R^{n+1}$。</p>
<p>二项逻辑斯蒂回归的参数估计法也可以推广到多项逻辑斯蒂回归。</p>
<h1 id="6-2-最大熵模型"><a href="#6-2-最大熵模型" class="headerlink" title="6.2 最大熵模型"></a>6.2 最大熵模型</h1><p><strong>最大熵模型（maximum entropy model）</strong>由最大熵原理推导实现。</p>
<h2 id="6-2-1-最大熵原理"><a href="#6-2-1-最大熵原理" class="headerlink" title="6.2.1 最大熵原理"></a>6.2.1 最大熵原理</h2><p>最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。</p>
<p>假设离散随机变量$X$的概率分布是$P(X)$，则其熵是<br>$$<br>H(P) = -\sum\limits_{x}P(x)\log P(x)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.9)<br>$$<br>熵满足下列不等式：<br>$$<br>0 \leq H(P) \leq \log|X|<br>$$<br>式中，$|X|$是$X$的取值个数，当且仅当$X$的分布是均匀分布时右边的等号成立。这就是说，当$X$服从均匀分布时，熵最大。</p>
<p>直观地，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性。“等可能”不容易操作，而熵则是一个可优化的数值指标。</p>
<h2 id="6-2-2-最大熵模型的定义"><a href="#6-2-2-最大熵模型的定义" class="headerlink" title="6.2.2 最大熵模型的定义"></a>6.2.2 最大熵模型的定义</h2><p>假设分类模型是一个条件概率分布$P(Y|X),X \in \chi \subseteq R^n$表示输入，$Y \in \mathcal{Y}$表示输出，$\chi $和$\mathcal{Y}$分别是输入和输出的集合。这个模型表示的是对于给定的输入$X$，以条件概率$P(Y|X)$输出$Y$。</p>
<p>给定一个训练数据集<br>$$<br>T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}<br>$$<br>学习的目标是用最大熵原理选择最好的分类模型。</p>
<hr>
<p>首先考虑模型应该满足的条件。给定训练数据集，可以确定联合分布$P(X,Y)$的经验分布和边缘分布$P(X)$的经验分布，分别以$\tilde{P}(X,Y)$和$\tilde{P}(X)$表示。这里，<br>$$<br>\tilde{P}(X=x,Y=y) = \frac{\upsilon(X=x,Y=y)}{N}<br>$$</p>
<p>$$<br>\tilde{P}(X=x)=\frac{\upsilon(X=x)}{N}<br>$$</p>
<p>其中，$\upsilon(X=x,Y=y)$表示训练数据中样本$(x,y)$出现的频数，$\upsilon(X=x)$表示训练数据中输入$x$出现的频数，$N$表示训练样本容量。</p>
<p>用<strong>特征函数（feature function）</strong>$f(x,y)$描述输入$x$和输出$y$之间的某一个事实。其定义是<br>$$<br>f(x,y) = \lbrace_{0, \ \ \ 否则}^{1,\ \ \ x与y满足某一事实}<br>$$<br>特征函数$f(x,y)$关于经验分布$\tilde{P}(X,Y)$的期望值，用$E_{\tilde{P}}(f)$表示：<br>$$<br>E_{\tilde{P}}(f) = \sum\limits_{x,y}\tilde{P}(x,y)f(x,y)<br>$$<br>特征函数$f(x,y)$关于模型$P(Y|X)$与经验分布$\tilde{P}(X)$的期望值，用$E_P(f)$表示：<br>$$<br>E_P(f)=\sum\limits_{x,y}\tilde{P}(x)P(y|x)f(x,y)<br>$$<br>如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即<br>$$<br>E_P(f)=E_{\tilde{P}}(f)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.10)<br>$$<br>或<br>$$<br>\sum\limits_{x,y}\tilde{P}(x)P(y|x)f(x,y) = \sum\limits_{x,y}\tilde{P}(x,y)f(x,y)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.11)<br>$$<br>将式$(6.10)$或式$(6.11)$作为模型学习的约束条件。假设有$n$个特征函数$f_i(x,y),\ \ i=1,2,…,n$，那么就有$n$个约束条件。</p>
<hr>
<p><strong>定义 6.3（最大熵模型）</strong> 假设满足所有约束条件的模型集合为<br>$$<br>\mathcal{C} \equiv {P\in \mathcal{P}|E_P(f_i)=E_{\tilde{P}}(f),\ \ \ i=1,2,…,n}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.12)<br>$$<br>定义在条件概率分布$P(Y|X)$上的条件熵为<br>$$<br>H(P) = -\sum\limits_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.13)<br>$$<br>则模型集合$\mathcal{C}$中条件熵$H(P)$最大的模型称为最大熵模型。</p>
<h2 id="6-2-3-最大熵模型的学习"><a href="#6-2-3-最大熵模型的学习" class="headerlink" title="6.2.3 最大熵模型的学习"></a>6.2.3 最大熵模型的学习</h2><p>最大熵模型的学习过程就是求解最大熵模型的过程。最大熵模型的学习可以形式化为约束最优化问题。</p>
<p>对于给定的训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$以及特征函数$f_i(x,y),i=1,2,…,n$，最大熵模型的学习等价于约束最优化问题：<br>$$<br>\max\limits_{P \in \mathcal{C}}  H(P) = -\sum\limits_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)<br>$$</p>
<p>$$<br>s.t.\ \ \ E_P(f_i)=E_{\tilde{P}}(f_i),\ \ \ i=1,2,…,n<br>$$</p>
<p>$$<br>\sum\limits_{y} P(y|x) = 1<br>$$</p>
<p>按照最优化问题的习惯，将求最大值问题改写为等价的求最小值问题：<br>$$<br>\min\limits_{P \in \mathcal{C}}  -H(P) = \sum\limits_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.14)<br>$$</p>
<p>$$<br>s.t.\ \ \ E_P(f_i) - E_{\tilde{P}}(f_i) = 0,\ \ \ i=1,2,…,n \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.15)<br>$$</p>
<p>$$<br>\sum\limits_{y} P(y|x) = 1 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  (6.16)<br>$$</p>
<p>求解约束最优化问题$(6.14)<del>(6.16)$，所得出的解，就是最大熵模型学习的解。**具体推导省略。详见《统计学习方法-李航》98</del>100页。**</p>
<p>可以应用最优化算法求对偶函数$\Psi(\omega)$的极大化，得到$\omega^*$，用来表示$P^* \in \mathcal{C}$。这里，$P^*=P_{\omega^*}=P_{\omega^*}(y|x)$是学习到的最优模型（最大熵模型）也就是说，最大熵模型的学习归结为对偶函数$\Psi(\omega)$的极大化。</p>
<h2 id="6-2-4-极大似然估计"><a href="#6-2-4-极大似然估计" class="headerlink" title="6.2.4 极大似然估计"></a>6.2.4 极大似然估计</h2><p>对偶函数极大化等价于最大熵模型的极大似然估计。对偶函数$\Psi(\omega)$等价于对数似然函数$L_{\tilde{P}}(P_\omega)$。<strong>具体推导省略。详见《统计学习方法-李航》102~103页。</strong></p>
<p>这样，最大熵模型的学习问题就转换为具体求解对数似然函数极大化或对偶函数极大化的问题。</p>
<p>可以将最大熵模型写成更一般的形式。<br>$$<br>P_{\omega}(y|x)=\frac{1}{Z_{\omega}(x)}\exp(\sum\limits_{i=1}^n \omega_if_i(x,y))\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.28)<br>$$<br>其中，<br>$$<br>Z_{\omega}(x)=\sum\limits_{y} \exp(\sum\limits_{i=1}^n \omega_if_i(x,y))\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6.29)<br>$$<br>最大熵模型与逻辑斯蒂回归模型由类似的形式，它们又称为<strong>对数线性模型（log linear model）</strong>。模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计。</p>
<h1 id="6-3-模型学习的最优化算法"><a href="#6-3-模型学习的最优化算法" class="headerlink" title="6.3 模型学习的最优化算法"></a>6.3 模型学习的最优化算法</h1><p>逻辑斯蒂回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。从最优化的观点看，这时的目标函数具有很好的性质。它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法和拟牛顿法。牛顿法或拟牛顿法一般收敛速度更快。</p>
<h2 id="6-3-1-改进的迭代尺度法"><a href="#6-3-1-改进的迭代尺度法" class="headerlink" title="6.3.1 改进的迭代尺度法"></a>6.3.1 改进的迭代尺度法</h2><p><strong>改进的迭代尺度法（improved iterative scaling，IIS）</strong>是一种最大熵模型学习的最优化算法。</p>
<h2 id="6-3-2-拟牛顿法"><a href="#6-3-2-拟牛顿法" class="headerlink" title="6.3.2 拟牛顿法"></a>6.3.2 拟牛顿法</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/Python_Notes/%E7%AC%AC%E4%B8%83%E7%AB%A0-%E7%94%A8%E6%88%B7%E8%BE%93%E5%85%A5%E5%92%8Cwhile%E5%BE%AA%E7%8E%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/posthead.jpg">
      <meta itemprop="name" content="Normal People">
      <meta itemprop="description" content="Get busy living or get busy dying">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/08/Python_Notes/%E7%AC%AC%E4%B8%83%E7%AB%A0-%E7%94%A8%E6%88%B7%E8%BE%93%E5%85%A5%E5%92%8Cwhile%E5%BE%AA%E7%8E%AF/" class="post-title-link" itemprop="url">第七章 用户输入和while循环</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-08 16:46:39 / 修改时间：16:46:04" itemprop="dateCreated datePublished" datetime="2021-06-08T16:46:39+08:00">2021-06-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">Python基础</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/08/Python_Notes/%E7%AC%AC%E4%B8%83%E7%AB%A0-%E7%94%A8%E6%88%B7%E8%BE%93%E5%85%A5%E5%92%8Cwhile%E5%BE%AA%E7%8E%AF/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/08/Python_Notes/%E7%AC%AC%E4%B8%83%E7%AB%A0-%E7%94%A8%E6%88%B7%E8%BE%93%E5%85%A5%E5%92%8Cwhile%E5%BE%AA%E7%8E%AF/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="7-1-函数input-的工作原理"><a href="#7-1-函数input-的工作原理" class="headerlink" title="7.1 函数input()的工作原理"></a>7.1 函数input()的工作原理</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">message = <span class="built_in">input</span>(<span class="string">&quot;tell me something:&quot;</span>)</span><br><span class="line">print(message)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tell me something:Hello Python World!   #Hello Python World!为用户在控制台的输入</span><br><span class="line">Hello Python World!</span><br></pre></td></tr></table></figure>

<p>函数input()接受一个参数——要向用户显示的<strong>提示（prompt）</strong>或说明，让用户知道该如何做。</p>
<p>程序运行到input()函数时，程序等待用户输入，并在用户按回车键后继续运行。</p>
<h2 id="7-1-1-编写清晰的程序"><a href="#7-1-1-编写清晰的程序" class="headerlink" title="7.1.1 编写清晰的程序"></a>7.1.1 编写清晰的程序</h2><p>有时候，提示可能超过一行。例如，你可能需要指出获取特定输入的原因。在这种情况下，可将提示赋给一个变量，再将该变量传递给函数input()。这样，即便提示超过一行，input()语句也会非常清晰。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;If you tell us who you are, we can personalize the message you see.&quot;</span></span><br><span class="line">prompt += <span class="string">&quot;\nWhat is your first name?&quot;</span></span><br><span class="line">name = <span class="built_in">input</span>(prompt)</span><br><span class="line">print(<span class="string">f&quot;hello, <span class="subst">&#123;name&#125;</span>!&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">If you tell us who you are, we can personalize the message you see.</span><br><span class="line">What is your first name?zhang</span><br><span class="line">hello, zhang!</span><br></pre></td></tr></table></figure>

<h2 id="7-1-2-使用int-来获取数值输入"><a href="#7-1-2-使用int-来获取数值输入" class="headerlink" title="7.1.2 使用int()来获取数值输入"></a>7.1.2 使用int()来获取数值输入</h2><p>使用函数input()时，Python将用户输入解读为字符串。为解决这个问题，可使用函数int()，它让Python将输入视为数值。函数int()将数的字符串转换为数值表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">age = <span class="built_in">input</span>(<span class="string">&quot;How old are you?&quot;</span>)</span><br><span class="line">age = <span class="built_in">int</span>(age)</span><br><span class="line"><span class="keyword">if</span>(age &gt;= <span class="number">18</span>):</span><br><span class="line">	print(<span class="string">&quot;you are a men!&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">How old are you?24</span><br><span class="line">you are a men!</span><br></pre></td></tr></table></figure>

<p>还有函数float()可将字符串转为小数。</p>
<h2 id="7-1-3-求模运算符"><a href="#7-1-3-求模运算符" class="headerlink" title="7.1.3 求模运算符"></a>7.1.3 求模运算符</h2><p>处理数值信息时，求模运算符（%）是个很有用的工具，它将两个数相除并返回余数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="number">4</span> % <span class="number">3</span>)</span><br><span class="line">print(<span class="number">5</span> % <span class="number">3</span>)</span><br><span class="line">print(<span class="number">6</span> % <span class="number">3</span>)</span><br><span class="line">print(<span class="number">7</span> % <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">0</span><br><span class="line">1</span><br></pre></td></tr></table></figure>



<h1 id="7-2-while循环简介"><a href="#7-2-while循环简介" class="headerlink" title="7.2 while循环简介"></a>7.2 while循环简介</h1><p>for循环用于针对集合中的每个元素都执行一个代码块，而while循环则不断运行，直到指定的条件不满足为止。</p>
<h2 id="7-2-1-使用while循环"><a href="#7-2-1-使用while循环" class="headerlink" title="7.2.1 使用while循环"></a>7.2.1 使用while循环</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cur_number = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> cur_number &lt; <span class="number">6</span>:</span><br><span class="line">	print(cur_number)</span><br><span class="line">	cur_number += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<h2 id="7-2-2-让用户选择何时退出"><a href="#7-2-2-让用户选择何时退出" class="headerlink" title="7.2.2 让用户选择何时退出"></a>7.2.2 让用户选择何时退出</h2><p>可以使用while循环让程序在用户愿意时不断运行，可以定义一个<strong>退出值</strong>，只要用户输入的不是这个值，程序就将接着运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">message = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">while</span> message != <span class="string">&#x27;quit&#x27;</span>:</span><br><span class="line">	message = <span class="built_in">input</span>(<span class="string">&quot;please input something:&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> message != <span class="string">&#x27;quit&#x27;</span>:</span><br><span class="line">		print(<span class="string">f&quot;your input the message is :<span class="subst">&#123;message&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">please input something:hello</span><br><span class="line">your input the message is :hello</span><br><span class="line"></span><br><span class="line">please input something:python</span><br><span class="line">your input the message is :python</span><br><span class="line"></span><br><span class="line">please input something:world</span><br><span class="line">your input the message is :world</span><br><span class="line"></span><br><span class="line">please input something:quit</span><br></pre></td></tr></table></figure>

<h2 id="7-2-3-使用标志"><a href="#7-2-3-使用标志" class="headerlink" title="7.2.3 使用标志"></a>7.2.3 使用标志</h2><p>在要求很多条件都满足才继续运行的程序中，可定义一个变量，用于判断整个程序是否处于活动状态。这个变量称为<strong>标志（flag）</strong>，充当程序的交通信号灯。可以让程序在标志为True时继续运行，并在任何事件导致的值为False时让程序停止运行。这样，在while语句中就只需检查一个条件：标志的当前值是否为True。然后将所有其他测试（是否发生了应将标志设为False的事件）都放在其他地方，从而让程序更整洁。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">message = <span class="string">&quot;&quot;</span></span><br><span class="line">flag = <span class="literal">True</span></span><br><span class="line"><span class="keyword">while</span> flag:</span><br><span class="line">	message = <span class="built_in">input</span>(<span class="string">&quot;please input something:&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> message != <span class="string">&#x27;quit&#x27;</span>:</span><br><span class="line">		print(<span class="string">f&quot;your input the message is :<span class="subst">&#123;message&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        flag = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">please input something:hello </span><br><span class="line">your input the message is :hello </span><br><span class="line"></span><br><span class="line">please input something:python</span><br><span class="line">your input the message is :python</span><br><span class="line"></span><br><span class="line">please input something:world</span><br><span class="line">your input the message is :world</span><br><span class="line"></span><br><span class="line">please input something:quit</span><br></pre></td></tr></table></figure>

<h2 id="7-2-4-使用break退出循环"><a href="#7-2-4-使用break退出循环" class="headerlink" title="7.2.4 使用break退出循环"></a>7.2.4 使用break退出循环</h2><p>break语句用于控制程序流程，可用来控制哪些代码行将执行，哪些代码行不执行，从而让程序按你的要求执行你要执行的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">message = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">	message = <span class="built_in">input</span>(<span class="string">&quot;please input something:&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> message == <span class="string">&#x27;quit&#x27;</span>:</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">f&quot;your input the message is :<span class="subst">&#123;message&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">please input something:hello </span><br><span class="line">your input the message is :hello </span><br><span class="line"></span><br><span class="line">please input something:python</span><br><span class="line">your input the message is :python</span><br><span class="line"></span><br><span class="line">please input something:world</span><br><span class="line">your input the message is :world</span><br><span class="line"></span><br><span class="line">please input something:quit</span><br></pre></td></tr></table></figure>

<h2 id="7-2-5-在循环中使用continue"><a href="#7-2-5-在循环中使用continue" class="headerlink" title="7.2.5 在循环中使用continue"></a>7.2.5 在循环中使用continue</h2><p>要返回循环开头，并根据条件测试结果决定是否继续执行循环，可使用continue语句，它不像break语句那样不再执行余下的代码并退出整个循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cur_number = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> cur_number &lt; <span class="number">10</span>:</span><br><span class="line">	cur_number += <span class="number">1</span></span><br><span class="line">	<span class="keyword">if</span>(cur_number % <span class="number">2</span> == <span class="number">0</span>):</span><br><span class="line">		<span class="keyword">continue</span></span><br><span class="line">	print(cur_number)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3</span><br><span class="line">5</span><br><span class="line">7</span><br><span class="line">9</span><br></pre></td></tr></table></figure>

<h2 id="7-2-6-避免无限循环"><a href="#7-2-6-避免无限循环" class="headerlink" title="7.2.6 避免无限循环"></a>7.2.6 避免无限循环</h2><p>如果程序陷入无限循环，可按Ctrl+C，也可关闭显示程序输出的终端窗口</p>
<h1 id="7-3使用while循环处理列表和字典"><a href="#7-3使用while循环处理列表和字典" class="headerlink" title="7.3使用while循环处理列表和字典"></a>7.3使用while循环处理列表和字典</h1><p>for循环是一种遍历列表的有效方式，但不应在for循环中修改列表，否则将导致Python难以跟踪其中的元素。要在遍历列表的同时对其进行修改，可使用while循环。</p>
<h2 id="7-3-1-在列表之间移动元素"><a href="#7-3-1-在列表之间移动元素" class="headerlink" title="7.3.1 在列表之间移动元素"></a>7.3.1 在列表之间移动元素</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">unconfirmed_users = [<span class="string">&#x27;alice&#x27;</span>, <span class="string">&#x27;brian&#x27;</span>, <span class="string">&#x27;candace&#x27;</span>]</span><br><span class="line">confirmed_users = []</span><br><span class="line">print(unconfirmed_users)</span><br><span class="line">print(confirmed_users)</span><br><span class="line"><span class="keyword">while</span> unconfirmed_users:</span><br><span class="line">	cur_user = unconfirmed_users.pop()</span><br><span class="line">	print(<span class="string">f&quot;verifying user:<span class="subst">&#123;cur_user.title()&#125;</span>&quot;</span>)</span><br><span class="line">	confirmed_users.append(cur_user)</span><br><span class="line">print(unconfirmed_users)</span><br><span class="line">print(confirmed_users)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[&#39;alice&#39;, &#39;brian&#39;, &#39;candace&#39;]</span><br><span class="line">[]</span><br><span class="line">verifying user:Candace</span><br><span class="line">verifying user:Brian</span><br><span class="line">verifying user:Alice</span><br><span class="line">[]</span><br><span class="line">[&#39;candace&#39;, &#39;brian&#39;, &#39;alice&#39;]</span><br></pre></td></tr></table></figure>

<h2 id="7-3-2删除为特定值的所有列表元素"><a href="#7-3-2删除为特定值的所有列表元素" class="headerlink" title="7.3.2删除为特定值的所有列表元素"></a>7.3.2删除为特定值的所有列表元素</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pets = [<span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;goldfish&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;rabbit&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]</span><br><span class="line">print(pets)</span><br><span class="line"><span class="keyword">while</span> <span class="string">&#x27;cat&#x27;</span> <span class="keyword">in</span> pets:</span><br><span class="line">	pets.remove(<span class="string">&#x27;cat&#x27;</span>)</span><br><span class="line">print(pets)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#39;dog&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;goldfish&#39;, &#39;cat&#39;, &#39;rabbit&#39;, &#39;cat&#39;]</span><br><span class="line">[&#39;dog&#39;, &#39;dog&#39;, &#39;goldfish&#39;, &#39;rabbit&#39;]</span><br></pre></td></tr></table></figure>

<h2 id="7-3-3-使用用户输入来填充字典"><a href="#7-3-3-使用用户输入来填充字典" class="headerlink" title="7.3.3 使用用户输入来填充字典"></a>7.3.3 使用用户输入来填充字典</h2><p>可使用while循环提示用户输入任意多的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">responses = &#123;&#125;</span><br><span class="line">polling_active = <span class="literal">True</span><span class="comment">#设置一个标志，指出调查是否继续</span></span><br><span class="line"><span class="keyword">while</span> polling_active:</span><br><span class="line">    name = <span class="built_in">input</span>(<span class="string">&quot;\nWhat is your name?&quot;</span>)</span><br><span class="line">    response = <span class="built_in">input</span>(<span class="string">&quot;which mountain would you like to climb someday?&quot;</span>)</span><br><span class="line">    responses[name] = response</span><br><span class="line">    </span><br><span class="line">    repeat = <span class="built_in">input</span>(<span class="string">&quot;would you like to let another person respond?(yes/no)&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> repeat == <span class="string">&#x27;no&#x27;</span>:</span><br><span class="line">        polling_active = <span class="literal">False</span></span><br><span class="line">print(<span class="string">&quot;\n---poll results---&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> name, response <span class="keyword">in</span> responses.items():</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> would like to climb <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">What is your name?zhangsan</span><br><span class="line"></span><br><span class="line">which mountain would you like to climb someday?Deanli</span><br><span class="line"></span><br><span class="line">would you like to let another person respond?(yes&#x2F;no)yes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">What is your name?lisi</span><br><span class="line"></span><br><span class="line">which mountain would you like to climb someday?Devil&#96;s Thumb</span><br><span class="line"></span><br><span class="line">would you like to let another person respond?(yes&#x2F;no)no</span><br><span class="line"></span><br><span class="line">---poll results---</span><br><span class="line">zhangsan would like to climb Deanli</span><br><span class="line">lisi would like to climb Devil&#96;s Thumb</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/default-index/page/2/">2</a><a class="page-number" href="/default-index/page/3/">3</a><a class="extend next" rel="next" href="/default-index/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Normal People"
      src="/images/posthead.jpg">
  <p class="site-author-name" itemprop="name">Normal People</p>
  <div class="site-description" itemprop="description">Get busy living or get busy dying</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
        
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/TheNormalPeople" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;TheNormalPeople" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1272481411@qq.com" title="E-Mail → mailto:1272481411@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/5938927274" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;5938927274" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/cy19970506" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;cy19970506" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Normal People</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'y8XFURQNCoQsprRTou9DiEJu-gzGzoHsz',
      appKey     : 'QfVpjKDtJQdJrnJNnWUbVvjH',
      placeholder: "留下邮箱,有空时间回复您！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
